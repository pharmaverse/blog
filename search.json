[
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#introduction",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#introduction",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Introduction",
    "text": "Introduction\nIn the dynamic world of clinical research, innovation and collaboration are key drivers of success. The NEST and   {admiral}  teams exemplify this through their groundbreaking packages. By leveraging open-source tools and fostering a community-driven approach, they have significantly advanced data integration and reporting methodologies in the clinical research setting. This story celebrates one of the first occasions in which the NEST and   {admiral}  were able to join forces and collaborate."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#who-are-the-nest-team",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#who-are-the-nest-team",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Who are the NEST Team?",
    "text": "Who are the NEST Team?\nThe NEST team, an acronym for Next-Generation Exploratory and Standardized Tools, has pioneered a collection of open-sourced R packages designed to expedite insight generation under clinical research settings. Originating at Roche, NEST has attracted a diverse array of collaborators from academia, the pharmaceutical industry, and clinical research institutes, largely due to efforts like pharmaverse. Their mission is to accelerate clinical reporting and welcome contributions from the broader scientific community."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#who-are-the-admiral-team",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#who-are-the-admiral-team",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Who are the admiral team?",
    "text": "Who are the admiral team?\nFocused on a complementary goal, the   {admiral}  team is dedicated to providing an open-source, modularized toolbox for creating ADaM datasets in R. Their approach is transparent and collaborative, empowering users to co-create and refine a harmonized methodology for ADaM development across the pharmaceutical industry. The   {admiral}  team designs their tools to be user-friendly and versatile, capable of addressing a wide range of data requirements."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#setting-the-stage-for-collaboration",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#setting-the-stage-for-collaboration",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Setting the Stage for Collaboration",
    "text": "Setting the Stage for Collaboration\nOver a year ago, an opportunity for collaboration was identified within NEST and   {admiral} . Indeed, the NEST team was reliant on simulated data for integration testing and powering of its TLG Catalog, which often lacked realism and failed to cover edge cases or expose software limitations. Through conversations within the pharmaverse, it was realized that some of these challenges could be addressed by switching to using the more realistic test data offered by the   {pharmaverseadam}  package (maintained by the   {admiral}  team). The source of the data within   {pharmaverseadam}  is the is real SDTM data published through the CDISC Test Data pilot, converted into ADaM by running the   {admiral}  template programs on it.\nThis potential pivot would beneficial for both teams. For one thing, it would allow the NEST team to achieve more realistic and comprehensive testing, thus enhancing the robustness of their development work. Simultaneously, it would mean that the   {admiral}  team would receive feedback from NEST about the robustness of both the   {pharmaverseadam}  datasets and the templates used to generate them."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#key-achievements",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#key-achievements",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Key Achievements",
    "text": "Key Achievements\nThe following months were a period of collaboration for the two teams as this switch was enacted. Numerous achievements were identified:\n\nRealistic Data Integration: Transitioning from simulated to more realistic data provided the NEST team with more accurate and relevant testing conditions. This change was crucial in identifying and rectifying potential software limitations.\nScope and Dependency Management: Both teams agreed to avoid creating strong interdependencies that could extend release cycles. NEST packages maintained minimal data for documentation purposes, while   {admiral}  preserved extensive datasets separately.\nCI Integration and Automation: On the NEST side, new CI integration tests ensured that template updates were automatically verified against stored   {pharmaverseadam}  datasets. This maintained consistency and allowed developers to identify intended changes promptly.\nStrategic Pipelines: A pipeline was established in   {pharmaverseadam}  so that any updates to the   {pharmaversesdtm}  source datasets and/or the   {admiral}  template scripts can trigger an update to   {pharmaverseadam} ."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#recent-developments",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#recent-developments",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Recent Developments",
    "text": "Recent Developments\nThe collaboration bore fruit as the teams uncovered critical insights and improvements. For instance, using the   {pharmaverseadam}  data in the {scda.test} package helped the NEST team identify and correct issues in their table template development, specifically in calculating denominator values. The realistic test data also revealed minor inconsistencies in the derivation of ECG data within the   {admiral}  templates, which were promptly addressed, enhancing data quality."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#conclusion",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#conclusion",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Conclusion",
    "text": "Conclusion\nThe collaboration between the NEST and   {admiral}  teams showcases the power of open-source initiatives and community-driven efforts such as the pharmaverse in advancing clinical research. By integrating realistic data and refining their testing processes, they have significantly enhanced the robustness and reliability of their tools. This partnership not only accelerates insight generation but also cultivates a culture of collaboration and innovation, benefiting the broader pharmaverse community.\nThe success of this collaboration highlights the profound impact of shared goals and collective innovation, paving the way for future advancements in clinical research methodologies and outcomes."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#acknowledgements",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#acknowledgements",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank the following people for their support during this project:\n\nDaphne Grasselly for establishing the automated CI refresh pipeline in   {pharmaverseadam} , and Kangjie Zhang for leading the   {pharmaverseadam}  team in 2024.\nBen Straub, Stefan Bundfuss, Zelos Zhu, Ross Farrugia and Jeff Dickinson for their support from the   {admiral}  side.\nLeena Khatri and Emily de la Rua for support from the NEST side."
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#last-updated",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#last-updated",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:44.214835"
  },
  {
    "objectID": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#details",
    "href": "posts/2025-01-15_nest_and_pharmaverseadam/nest_and_pharmaverseadam.html#details",
    "title": "A collaborative triumph: Re-using test data between the NEST and admiral teams",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html",
    "href": "posts/2023-08-08_study_day/study_day.html",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "",
    "text": "Creating --DY variables for your ADaMs is super easy using derive_vars_dy() from the admiral package.\nLet’s build some dummy data with 4 subjects, a start date/time for treatment (TRTSDTM), an analysis start date/time variable (ASTDTM) and an analysis end date variable (AENDT).\nlibrary(admiral)\nlibrary(lubridate)\nlibrary(dplyr)\n\nadam &lt;- tribble(\n  ~USUBJID, ~TRTSDTM, ~ASTDTM, ~AENDT,\n  \"001\", \"2014-01-17T23:59:59\", \"2014-01-18T13:09:O9\", \"2014-01-20\",\n  \"002\", \"2014-02-25T23:59:59\", \"2014-03-18T14:09:O9\", \"2014-03-24\",\n  \"003\", \"2014-02-12T23:59:59\", \"2014-02-18T11:03:O9\", \"2014-04-17\",\n  \"004\", \"2014-03-17T23:59:59\", \"2014-03-19T13:09:O9\", \"2014-05-04\"\n) %&gt;%\n  mutate(\n    TRTSDTM = as_datetime(TRTSDTM),\n    ASTDTM = as_datetime(ASTDTM),\n    AENDT = ymd(AENDT)\n  )\nOkay! Next we run our dataset through derive_vars_dy(), specifying:\nderive_vars_dy(\n  adam,\n  reference_date = TRTSDTM,\n  source_vars = exprs(ASTDTM, AENDT)\n)\n\n# A tibble: 4 × 6\n  USUBJID TRTSDTM             ASTDTM              AENDT      ASTDY AENDY\n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 001     2014-01-17 23:59:59 2014-01-18 13:09:09 2014-01-20     2     4\n2 002     2014-02-25 23:59:59 2014-03-18 14:09:09 2014-03-24    22    28\n3 003     2014-02-12 23:59:59 2014-02-18 11:03:09 2014-04-17     7    65\n4 004     2014-03-17 23:59:59 2014-03-19 13:09:09 2014-05-04     3    49\nThat’s it! We got both our ASTDY and AENDY variables in only a few short lines of code!\nWhat if I want my variables to have a different naming convention?\nEasy! In the source_vars argument if you want your variables to be called DEMOADY and DEMOEDY just do DEMOADY = ASTDTM and DEMOEDY = AENDT and derive_vars_dy() will do the rest!\nderive_vars_dy(\n  adam,\n  reference_date = TRTSDTM,\n  source_vars = exprs(DEMOADY = ASTDTM, DEMOEDY = AENDT)\n)\n\n# A tibble: 4 × 6\n  USUBJID TRTSDTM             ASTDTM              AENDT      DEMOADY DEMOEDY\n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 001     2014-01-17 23:59:59 2014-01-18 13:09:09 2014-01-20       2       4\n2 002     2014-02-25 23:59:59 2014-03-18 14:09:09 2014-03-24      22      28\n3 003     2014-02-12 23:59:59 2014-02-18 11:03:09 2014-04-17       7      65\n4 004     2014-03-17 23:59:59 2014-03-19 13:09:09 2014-05-04       3      49\nIf you want to get --DT or --DTM variables using admiral then check out derive_vars_dt() and derive_vars_dtm(). If things are messy in your data, e.g. partial dates, both functions have great imputation abilities, which we will cover in an upcoming blog post!"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html#last-updated",
    "href": "posts/2023-08-08_study_day/study_day.html#last-updated",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:39.321596"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html#details",
    "href": "posts/2023-08-08_study_day/study_day.html#details",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html",
    "href": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html",
    "title": "Introducing a new Coursera course for hands-on clinical data science using R.",
    "section": "",
    "text": "Shifting to R in an industry traditionally reliant on SAS is no small feat. It requires not just new tools, but also significant upskilling for programmers. Roche-Genentech is addressing this need by offering three publicly available Coursera courses aimed at enhancing the skills of current data scientists within the pharmaceutical sector and introducing our work to those outside the industry. In this post, we are excited to announce the release of our second Coursera course, and we will discuss how R open-source software is being embraced across the industry and particularly within Roche-Genentech. Additionally, we will provide an overview of the three Coursera courses and explain why such initiatives are essential for a successful transition to open-source tools and a new mindset.\nIn recent years, the pharmaceutical industry has witnessed a significant shift towards adopting open-source software and programming languages for various applications. Among them, R has emerged as a game-changer, revolutionizing the way we approach Health Authority electronic submissions and pharmaceutical analysis.\nAs the industry embraces new ways of working, R has gained immense popularity due to its flexibility, versatility, and extensive range of open-source packages and tools. Open source tools specifically created for the clinical data scientist are collected under and shared within the pharmaverse, enabling data scientists and researchers to leverage the power of R for a wide array of applications, including clinical trial studies, data set creation, and analytical reports.\nWithin Roche-Genentech, a dedicated working group of Data Scientists recognized the potential of R in a pharmaceutical regulatory setting. Leveraging their expertise and pharmaverse knowledge, they developed a series of trainings on Coursera. These trainings provide hands-on demonstrations and examples, guiding industry professionals on how to effectively utilize R in running a complete clinical trial study. Specifically, the first training, Making Data Science Work for Clinical Reporting, offers a broad introduction to the work as a data scientist within the pharmaceutical industry. This should be of particular interest to aspiring clinical data scientists. The second training – which we are releasing this week – dives deep into the workflow of a clinical data scientist and introduces the tools we use on a daily basis. Looking into the future, there will be a project capstone training course that integrates the concepts from the preceding two courses, culminating in comprehensive learning, providing a strong foundation of knowledge for aspiring data scientists using R in the Pharma industry.\nHere is an outline of what participants will gain from the training we are releasing this week.\nHands-On Clinical Reporting Using R\nIn essence, participants will learn how to navigate and manipulate datasets using R, ensuring data integrity and accuracy. Additionally, R’s powerful statistical capabilities enable the creation of insightful Tables, Listings, and Figures/Graphs (TLF/Gs), which are vital components of regulatory submissions. Even further, the Coursera training offered by Roche-Genentech will empower industry professionals to unlock the full potential of R Shiny in their analytical reports. This feature enables researchers to present complex data in a user-friendly and interactive manner, enhancing the understanding and interpretation of results.\nA stepping stone towards the future: For those in the industry who are embracing these new ways of working, the Coursera training provided by Roche-Genentech serves as a valuable stepping stone into the world of R open-source software. The training equips participants with the necessary skills to harness the power of R, facilitating efficient and effective pharmaceutical analysis.\nIn conclusion, the adoption of R open-source software and programming language is transforming the pharmaceutical industry, enabling professionals to streamline their processes, enhance data analysis, and improve regulatory submissions. Roche-Genentech’s commitment to sharing knowledge and expertise through the Coursera trainings demonstrates their dedication to advancing the pharmaverse As the industry continues to embrace this new direction, the future of pharmaceutical analysis looks increasingly promising with R at its core.\nYour journey to revolutionizing your pharmaceutical analysis knowledge awaits!\nHuge thanks to Adrian Chan who led the effort of building this course and the instructors Jana Stoilova, Joel Laxamana, M.S., Leena Khatri, Tatiana Alonso Amor & Stefan Thoma."
  },
  {
    "objectID": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html#last-updated",
    "href": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html#last-updated",
    "title": "Introducing a new Coursera course for hands-on clinical data science using R.",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:35.773976"
  },
  {
    "objectID": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html#details",
    "href": "posts/2024-07-03_introducing_a_ne.../introducing_a_new_coursera_course_for_hands_on_clinical_data_science_using__r..html#details",
    "title": "Introducing a new Coursera course for hands-on clinical data science using R.",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "",
    "text": "There is significant momentum in driving the adoption of R packages in the life sciences industries, in particular, the R Consortium Submissions Working Group is dedicated to promoting the use of R for regulatory submissions to the FDA.\nThe R Consortium Submissions Working Group successfully completed an R-based submission in November 2021 through the eCTD portal (R Submissions Pilot 1). This Pilot was completed on March 10, 2022 after a successful statistical review and evaluation by the FDA staff.\nMoving forward, the Pilot 2 aimed to include a Shiny application that the FDA staff could deploy on their own servers. The R Consortium recently announced that on September 27, 2023, the R Submissions Working Group successfully completed the follow-up to the Pilot 2 R Shiny-based submission and received a response letter from FDA CDER. This marks the first publicly available submission package that includes a Shiny component. The full FDA response letter can be found here.\nThe Shiny application that was sent for the Pilot 2 had the goal to display the 4 Tables, Listings and Figures (TLFs) that were sent for the Pilot 1 with basic filtering functionality.\nThe submission package adhered to the eCTD folder structure and contained 5 deliverables. Among the deliverables was the proprietary R package {pilot2wrappers}, which enables the execution of the Shiny application.\nThe FDA staff were expected to receive the electronic submission packages in the eCTD format, install and load open-source R packages used as dependencies in the included Shiny application, reconstruct and load the submitted Shiny application, and communicate potential improvements in writing.\nIn the following stage, the R Consortium’s R Submission Working Group launched Pilot 4, aiming to investigate innovative technologies like Linux containers and web assembly. These technologies are being explored to package a Shiny application into a self-contained unit, streamlining the transfer and execution processes for the application.\nIn this post, our aim is to outline how we used the Rhino framework to reproduce the Shiny application that was successfully submitted to the FDA for the Pilot 2 project. Additionally, we detail the challenges identified during the process and how we were able to successfully address them by using an open-source package."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#reproducing-the-r-submission-pilot-2-shiny-app-using-rhino",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#reproducing-the-r-submission-pilot-2-shiny-app-using-rhino",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Reproducing the R Submission Pilot 2 Shiny App using Rhino",
    "text": "Reproducing the R Submission Pilot 2 Shiny App using Rhino\nWhile the original Shiny application submitted to the FDA was wrapped using {Golem}, we replicated the application using our in-house developed framework Rhino. The main motivation was to provide an example of an R Submission that is not an R package and to identify and solve any issues that may arise from this approach.\nOur demo application (FDA-pilot-app) is accessible on our website, alongside other Shiny and Rhinoverse demonstration apps.\n\nThe code for FDA-pilot-app is open-source. You can create your own Rhino-based application by following our tutorial and viewing our workshop, which is available on YouTube."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#brief-introduction-to-rhino",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#brief-introduction-to-rhino",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Brief Introduction to Rhino",
    "text": "Brief Introduction to Rhino\n\nThe Rhino framework was developed by Appsilon to create enterprise-level Shiny applications, consistently and efficiently. This framework allows developers to apply the best software engineering practices, modularize code, test it thoroughly, enhance UI aesthetics, and monitor user adoption effectively.\nRhino provides support in 3 main areas:\n\nClear Code: scalable architecture, modularization based on Box and Shiny modules.\nQuality: comprehensive testing such as unit tests, E2E tests with Cypress and Shinytest2, logging, monitoring and linting.\nAutomation: project startup, CI with GitHub Actions, dependencies management with {renv}, configuration management with config, Sass and JavaScript bundling with ES6 support via Node.js.\n\nRhino is an ideal fit for highly regulated environments such as regulatory submissions or other drug development processes.\n\nFDA-pilot-app structure\nThe structure of this application is available on the github repository. The structure of this Shiny app is the following.\n\nClick here to expand the FDA-pilot-app structure\n\n.\n├── app\n│   ├── view\n│   │   └── demographic_table.R\n|   |   └── km_plot.R\n|   |   └── primary_table.R\n|   |   └── efficacy_table.R\n|   |   └── completion_table.R\n│   ├── logic\n│   │   └── adam_data.R\n│   │   └── eff_modles.R\n│   │   └── formatters.R\n│   │   └── helpers.R\n│   │   └── kmplot_helpers.R\n│   │   └── Tplyr_helpers.R\n│   ├── data\n│   │   └── adam\n│   │       └── adadas.xpt\n│   │       └── adlbc.xpt\n│   │       └── adsl.xpt\n│   │       └── adtte.xpt\n│   ├── docs\n│   │   └── about.md\n│   ├── js\n│   │   └── index.js\n│   ├── static\n│   │   └── favicon.ico\n│   ├── styles\n│   │   └── main.scss\n│   └── app.R\n├── tests\n│   ├── cypress\n│   │   └── integration\n│   │       └── app.spec.js\n│   ├── testthat\n│   │\n│   └── cypress.json\n├── app.R\n├── rhino_submission.Rproj\n├── dependencies.R\n├── renv.lock\n├── rhino.yml\n└── README.md"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#efficient-submissions-to-the-fda",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#efficient-submissions-to-the-fda",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Efficient Submissions to the FDA",
    "text": "Efficient Submissions to the FDA\n\n\n\n\n\nTo comply with the Electronic Submission File Formats and Specifications for the eCTD submission, the programming code should carry a “.txt” extension. In the R Submissions Pilot 3 the group did not use {pkglite} as the FDA clarified that “.zip” and “.r” files are acceptable for submission. In this case, we utilized the {pkglite} R package to efficiently pack and unpack the FDA-pilot-app. This approach would facilitate the FDA reviewers in setting up the submission on their systems.\nThis package allows packing R packages to “.txt” files, which are supported for the submission of proprietary packages to the FDA via the eCTD gateway. \n\nPacking the FDA-pilot-app into a .txt file\nThe code below can be used to pack the Shiny application into a .txt file:\n\napp_name &lt;- \"rhinosubmission\"\nrenv_spec &lt;- pkglite::file_spec(\n  \"renv\",\n  pattern = \"^settings\\\\.dcf$|^activate\\\\.R$\",\n  format = \"text\", recursive = FALSE\n)\ntests_spec &lt;- pkglite::file_tests()\napp_spec &lt;- pkglite::file_auto(\"app\")\nroot_spec &lt;- pkglite::file_spec(\n  path = \".\",\n  pattern = \"^\\\\.Rprofile$|^rhino\\\\.yml$|^renv\\\\.lock$|^dependencies\\\\.R$|^config\\\\.yml$|^app\\\\.R$|^README\\\\.md$|\\\\.Rproj$\",\n  all_files = TRUE,\n  recursive = FALSE\n)\nwrite(paste0(\"Package: \", app_name), \"DESCRIPTION\", append = TRUE)\npkglite::collate(\n  getwd(),\n  renv_spec,\n  tests_spec,\n  app_spec,\n  root_spec\n) |&gt; pkglite::pack()\nfile.remove(\"DESCRIPTION\")\n\n\n\nUnpacking the FDA-pilot-app\nThe packed “.txt” file can be unpacked into a Shiny app by using {pkglite} as follows:\n\npkglite::unpack(\"rhinosubmission.txt\")"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#lessons-learned",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#lessons-learned",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nOur initial objective was to prove that it would be possible to submit a Shiny application using Rhino through the eCTD gateway. During the rewriting process we identified that this could be done by integrating the open-source {pkglite} package. By following this approach, we concluded that it would be possible to submit a Shiny application through the eCTD gateway. This was also achieved through the successful submission of a package that included a Shiny component in Pilot 2.\nHaving rewritten the R Submissions Pilot 2 Shiny application using Rhino holds major implications for the adoption of our framework within the life sciences. Apart from being a strong, opinionated framework that improves reproducibility and reliability for Shiny development, using Rhino for regulatory submissions could improve the flexibility and speed in the clinical reporting pipeline. This would accelerate the adoption of R/Shiny for submissions to the FDA or other regulatory agencies."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#last-updated",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#last-updated",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:29.825517"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#details",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#details",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "",
    "text": "This was a big year for open-source work in clinical submissions in general. We saw Roche speak about shifting to an open-source backbone for clinical trials. Novo Nordisk spoke publicly of an R based submission to the FDA. These are true marks of progress being made in R becoming a first class language for clinical reporting.\nBack in August, Nicholas Eugenio released a blog post on the history of pharmaverse. It’s funny to think about the fact that only 3 years ago, the idea of cross organization collaboration on R packages and building a community around this in the clinical world was just a conversation between friends. Since then, we have a community of more than 1200 people on Slack, 350 on LinkedIn, interest from over 150 organizations, and over 30 packages. If 2020 through 2022 was the birth of pharmaverse, 2023 was finding our identity as a community. For the council, 2024 will be about continuing to mature and find more ways that we can continue to support the community."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#was-a-big-year",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#was-a-big-year",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "",
    "text": "This was a big year for open-source work in clinical submissions in general. We saw Roche speak about shifting to an open-source backbone for clinical trials. Novo Nordisk spoke publicly of an R based submission to the FDA. These are true marks of progress being made in R becoming a first class language for clinical reporting.\nBack in August, Nicholas Eugenio released a blog post on the history of pharmaverse. It’s funny to think about the fact that only 3 years ago, the idea of cross organization collaboration on R packages and building a community around this in the clinical world was just a conversation between friends. Since then, we have a community of more than 1200 people on Slack, 350 on LinkedIn, interest from over 150 organizations, and over 30 packages. If 2020 through 2022 was the birth of pharmaverse, 2023 was finding our identity as a community. For the council, 2024 will be about continuing to mature and find more ways that we can continue to support the community."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#what-we-accomplished",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#what-we-accomplished",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "What We Accomplished",
    "text": "What We Accomplished\nOne of our biggest moves in 2023 was to form our partnership with PHUSE. This latched us into an existing community with shared values and a platform that helps us continue to build the pharmaverse community. At the PHUSE EU Connect we had our first opportunity to host a pharmaverse meetup and bring together pharmaverse contributors in person. Additionally, there was an excellent panel session highlighting our use and adoption of open-source across industry, including the pharmaceutical, commercial, and software perspectives. As we move forward, we’ll continue to use this platform to find ways we can host events and encourage collaboration within the pharmaverse community.\nThis year our community was also able to launch new platforms to share updates and knowledge throughout the industry. The pharmaverse examples webpage was launched to show pharmaverse packages in action, and the pharmaverse blog (which I’m using right here!) provides a platform to share updates and community news."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#where-next",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#where-next",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Where Next?",
    "text": "Where Next?\nBack in October, I had the opportunity to do an interview with Michael Rimler for the PHUSE video series Open Source Technologies in Clinical Data Analytics. The last question he asked me was what I expect the state of data analytics in life science to be in 2 to 3 years. My response was that when that time comes, I hope I couldn’t have predicted where we would be - because back in 2020 I could never have predicted where we are now. The progress we’ve made is unbelievable, and the pharmaverse community has played a huge role in getting us where we are today. For the pharmaverse community, I hope to see that progress continue as we move into next year. As a council, our goal is to continue to mature this community. How can we support and foster collaboration between our organizations? How can we leverage this platform to drive the industry forward?\nFor you as an individual, there’s always an opportunity to get involved - and you don’t have to be a package developer to contribute. You can join a working group, write examples, or author a blog post. Furthermore, you can get started with the pharmaverse packages, provide feedback via issues, and advocate for their use within your own organization. The pharmaverse community doesn’t exist without you, and we’re happy to have you all here to help us build this together."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#p.s.",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#p.s.",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "P.S.",
    "text": "P.S.\nAt PHUSE US Connect 2024 this coming February, be on the lookout for one of the keynote presentations from Michael Rimler and Ross Farrugia! We hope to see you there!\nHere’s to a 2024 full of progress and collaboration!"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#last-updated",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#last-updated",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:26.033436"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#details",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#details",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html",
    "title": "Blanks and NAs",
    "section": "",
    "text": "Reading in SAS-based datasets (.sas7bdat or xpt) into R has users calling the R package haven. A typical call might invoke read_sas() or read_xpt() to bring in your source data to construct your ADaMs or SDTMs.\nUnfortunately, while using haven the character blanks (missing data) found in a typical SAS-based dataset are left as blanks. These blanks will typically prove problematic while using functions like is.na in combination with dplyr::filter() to subset data. Check out Bayer’s SAS2R catalog: handling-of-missing-values for more discussion on missing values and NAs.\nIn the admiral package, we have built a simple function called convert_blanks_to_na() to help us quickly remedy this problem. You can supply an entire dataframe to this function and it will convert any character blanks to NA_character_"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#loading-packages-and-making-dummy-data",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#loading-packages-and-making-dummy-data",
    "title": "Blanks and NAs",
    "section": "Loading Packages and Making Dummy Data",
    "text": "Loading Packages and Making Dummy Data\n\nlibrary(admiral)\nlibrary(tibble)\nlibrary(dplyr)\n\ndf &lt;- tribble(\n  ~USUBJID, ~RFICDTC,\n  \"01\", \"2000-01-01\",\n  \"02\", \"2001-01-01\",\n  \"03\", \"\", # Here we have a character blank\n  \"04\", \"2001-01--\",\n  \"05\", \"2001---01\",\n  \"05\", \"\", # Here we have a character blank\n)\n\ndf\n\n# A tibble: 6 × 2\n  USUBJID RFICDTC     \n  &lt;chr&gt;   &lt;chr&gt;       \n1 01      \"2000-01-01\"\n2 02      \"2001-01-01\"\n3 03      \"\"          \n4 04      \"2001-01--\" \n5 05      \"2001---01\" \n6 05      \"\""
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#a-simple-conversion",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#a-simple-conversion",
    "title": "Blanks and NAs",
    "section": "A simple conversion",
    "text": "A simple conversion\n\ndf_na &lt;- convert_blanks_to_na(df)\n\ndf_na\n\n# A tibble: 6 × 2\n  USUBJID RFICDTC   \n  &lt;chr&gt;   &lt;chr&gt;     \n1 01      2000-01-01\n2 02      2001-01-01\n3 03      &lt;NA&gt;      \n4 04      2001-01-- \n5 05      2001---01 \n6 05      &lt;NA&gt;      \n\n\n\ndf_na %&gt;% filter(is.na(RFICDTC))\n\n# A tibble: 2 × 2\n  USUBJID RFICDTC\n  &lt;chr&gt;   &lt;chr&gt;  \n1 03      &lt;NA&gt;   \n2 05      &lt;NA&gt;"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#thats-it",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#thats-it",
    "title": "Blanks and NAs",
    "section": "That’s it!",
    "text": "That’s it!\nA simple call to this function can make your derivation life so much easier while working in R if working with SAS-based datasets. In admiral, we make use of this function at the start of all ADaM templates for common ADaM datasets. You can use the function use_ad_template() to get the full R script for the below ADaMs.\n\nlist_all_templates()\n\nExisting ADaM templates in package 'admiral':\n• ADAE\n• ADCM\n• ADEG\n• ADEX\n• ADLB\n• ADLBHY\n• ADMH\n• ADPC\n• ADPP\n• ADPPK\n• ADSL\n• ADVS"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#last-updated",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#last-updated",
    "title": "Blanks and NAs",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:20.509798"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#details",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#details",
    "title": "Blanks and NAs",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html",
    "href": "posts/2023-10-30_floating_point/floating_point.html",
    "title": "Floating point",
    "section": "",
    "text": "{admiral} recently ran into some trouble when dealing with floating point values, captured by this thread on GitHub. This post gives a brief overview on floating point values, recaps the discussion on GitHub, and explains how {admiral} deals with floating point values."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#floating-point-values",
    "href": "posts/2023-10-30_floating_point/floating_point.html#floating-point-values",
    "title": "Floating point",
    "section": "Floating point values",
    "text": "Floating point values\nFloating point values are numeric objects representing numbers between integers, e.g. 0.5, 2.3, 3.1415, etc. However, floating point numbers are not stored like integers, and most floating point numbers are approximations to the number they represent. To see what value a floating point number is actually stored as, we can use the format() function where we can increase the number of digits shown:\n\nformat(1.4, digits = 22)\n\n[1] \"1.399999999999999911182\"\n\n\nThese very small numerical differences impact the result of mathematical operations:\n\n0.1 + 0.2 == 0.3\n\n[1] FALSE\n\n\nIf we look at the actually stored values, this makes sense:\n\n0.1 %&gt;% format(digits = 22)\n\n[1] \"0.1000000000000000055511\"\n\n0.2 %&gt;% format(digits = 22)\n\n[1] \"0.2000000000000000111022\"\n\n(0.1 + 0.2) %&gt;% format(digits = 22)\n\n[1] \"0.3000000000000000444089\"\n\n0.3 %&gt;% format(digits = 22)\n\n[1] \"0.2999999999999999888978\"\n\n\nThe bottom line is: Avoid using exact comparators such as == and &gt;= when comparing floating point values.\n\n\n\n\n\n\nExact floating point values\n\n\n\nFloating point values are stored in binary format. While most floating point values are approximations, there are some exceptions which can be exactly represented, namely if they can be written down as \\(\\frac{x}{2^y}\\), where x and y are integers. For example, 0.5 is stored as \\(\\frac{1}{2}\\), 0.25 is stored as \\(\\frac{1}{4}\\), 0.125 is stored as \\(\\frac{1}{8}\\), etc.\n\n# simple examples\n0.5 %&gt;% format(digits = 22)\n\n[1] \"0.5\"\n\n0.25 %&gt;% format(digits = 22)\n\n[1] \"0.25\"\n\n0.125 %&gt;% format(digits = 22)\n\n[1] \"0.125\"\n\n0.0625 %&gt;% format(digits = 22)\n\n[1] \"0.0625\"\n\n# some weird values for x and y\n(1121 / (2^9)) %&gt;% format(digits = 22)\n\n[1] \"2.189453125\"\n\n\nAll floating point values are stored as \\(\\frac{x}{2^y}\\), where the outcome may be a very close approximation to the value they represent*.\nhttps://en.wikipedia.org/wiki/Floating-point_arithmetic#Representable_numbers,_conversion_and_rounding If you would like to learn more about representable floating point values please read the wikipedia article on floating point values, especially section Representable numbers, conversion and rounding.\n* Based on a recollection of the course associated with this GitHub Repository by Martin Mächler."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#issues-arising",
    "href": "posts/2023-10-30_floating_point/floating_point.html#issues-arising",
    "title": "Floating point",
    "section": "Issues arising",
    "text": "Issues arising\nGordon Miller came across this issue when he was creating DAIDS criteria for adverse events in cancer therapy when using case_when statements to implement the grade.\nWe can have a glimpse here:\n\natoxgr_criteria_daids %&gt;%\n  filter(TERM %in% c(\"Amylase, High\", \"Lipase, High\")) %&gt;%\n  select(TERM, GRADE_CRITERIA_CODE) %&gt;%\n  reactable(defaultPageSize = 4, highlight = TRUE, bordered = TRUE, striped = TRUE, resizable = TRUE)\n\n\n\n\n\nAs you can see, the data-frame contains the column GRADE_CRITERIA_CODE which contains comparisons of floating point values. And there was a discrepancy of what Gordon expected to see, and how R actually computed the comparison initially:\n\nThe test is AVAL &gt;= 1.1*ANRHI should give a value of “1” where AVAL = 110 and ANRHI = 100.\nI tried it separately and I also got 1.1*ANRHI not equal to 110 where ANRHI = 100.\n\nWhere ANRHI is the analysis range upper limit and AVAL is an analysis value.\nWhat happened here? Gordon Miller wanted to compute the analysis range upper limit plus 10% and compare it to the analysis value. He expected the comparison to yield TRUE (or 1 if converted to numeric) as AVAL (110) should be exactly 1.1 * 100. However, he multiplied an integer (100) with a floating point value (1.1). And the result was not exactly 110, as 1.1 is not exactly represented as a floating point value.\n\n(1.1 * 100) %&gt;% format(digits = 22)\n\n[1] \"110.0000000000000142109\"\n\n1.1 * 100 == 110\n\n[1] FALSE\n\n\nOn my machine, the result is actually larger than 110, while on Gordon Miller’s machine the result was smaller than 110. In {admiral}, we strive towards removing platform specific and unexpected behavior, so we had to find a way to solve the floating point issue."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#potential-solutions",
    "href": "posts/2023-10-30_floating_point/floating_point.html#potential-solutions",
    "title": "Floating point",
    "section": "Potential solutions",
    "text": "Potential solutions\nA very crude option would be to round the result of the multiplication to the nearest integer.\n\nround(1.1 * 100) %&gt;% format(digits = 22)\n\n[1] \"110\"\n\n\nHowever, this does not work when the result is not an integer, i.e. the upper limit was 101 instead. We should then compare the analysis value to 101 * 1.1, which should be exactly 111.1. We could try to round to the nearest decimal place, but that value would again be stored as a floating point value:\n\n(101 * 1.1) %&gt;%\n  round(digits = 1) %&gt;%\n  format(digits = 22)\n\n[1] \"111.0999999999999943157\"\n\n\nA workaround would be to multiply both sides of the equation with 10, and then round to the next integer:\n\n(101 * 1.1 * 10) %&gt;%\n  round() %&gt;%\n  format(digits = 22)\n\n[1] \"1111\"\n\n(111.1 * 10) %&gt;%\n  round() %&gt;%\n  format(digits = 22)\n\n[1] \"1111\"\n\n\nThis is very awkward, as you don’t know by how much you need to multiply each time, a very clunky solution.\nAlternatively, we can compare the absolute value of the difference between the analysis value and the upper limit plus 10% to a very small number, e.g. 0.0000001:\n\nAVAL &lt;- 111.1\nANRHI &lt;- 101\n\nabs(AVAL - ANRHI * 1.1) &lt; 0.0000001\n\n[1] TRUE\n\n\nComparing to a very small value is also how the all.equal() function works, which compares two numeric values and returns TRUE if they are equal within a tolerance. By default the tolerance is around \\(1.5 * 10^{-8}\\) but you can set it yourself to a lower value, e.g. machine tolerance .Machine$double.eps - (one of**) the smallest positive floating-point number x such that 1 + x != 1.\n\n1 + .Machine$double.eps == 1\n\n[1] FALSE\n\n# but:\n1 + .Machine$double.eps / 2 == 1\n\n[1] TRUE\n\n# so we can use:\nall.equal(AVAL, ANRHI * 1, 1, tolerance = .Machine$double.eps)\n\n[1] \"Mean absolute difference: 10.1\"\n\n\nThis would still be a little clunky for greater than or equal to comparisons:\n\nall.equal(AVAL, ANRHI * 1.1) | AVAL &gt; ANRHI * 1.1\n\n[1] TRUE\n\n# unfortunately, the all.equal() function does not return a FALSE if they are not the same:\nall.equal(AVAL, ANRHI * 1.1 + 1)\n\n[1] \"Mean relative difference: 0.0090009\"\n\n\nFor some reason, the value it returns is also not correct.\nThere is also a dplyr function called near() which does essentially the same thing as all.equal():\n\nANRHI &lt;- 100\nAVAL &lt;- 110\n(ANRHI * 1.1) %&gt;% format(digits = 22)\n\n[1] \"110.0000000000000142109\"\n\nAVAL &gt; ANRHI * 1.1 | near(AVAL, ANRHI * 1.1)\n\n[1] TRUE\n\n\nGordon Miller suggested to replace the standard comparators with the following functions across {admiral}\n\n\n\n{base}\nimproved\n\n\n\n\nA &gt;= B\nA &gt; B | near(A, B)\n\n\nA &lt;= B\nA &lt; B | near(A, B)\n\n\nA == B\nnear(A, B)\n\n\nA != B\n!near(A, B)\n\n\nA &gt; B\nA &gt; B & !near(A, B)\n\n\nA &lt; B\nA &lt; B & !near(A, B)\n\n\n\nThis would work perfectly fine, but especially for case_when() statements, it would add a lot of code-bloat.\nAlthough a minor issue, it looks like the near() function tests for absolute differences, while the all.equal() function tests for relative differences, as discussed in this thread:\n\n# Very large values:\n# When checking for absolute differences\nnear(\n  ANRHI * 1.1 * 10^6,\n  AVAL * 10^6\n)\n\n[1] FALSE\n\n# When checking for relative differences\nall.equal(\n  ANRHI * 1.1 * 10^6,\n  AVAL * 10^6\n)\n\n[1] TRUE\n\n# As:\n(ANRHI * 1.1 * 10^6) %&gt;% format(digits = 22)\n\n[1] \"110000000.0000000149012\"\n\n(AVAL * 10^6) %&gt;% format(digits = 22)\n\n[1] \"1.1e+08\"\n\n\n\n\n\n{base}\n{fpCompare}\n\n\n\n\nA &gt;= B\nA %&gt;=% B\n\n\nA &lt;= B\nA %&lt;=% B\n\n\nA == B\nA %==% B\n\n\nA != B\nA %!=% B\n\n\nA &gt; B\nA %&gt;&gt;% B\n\n\nA &lt; B\nA %&lt;&lt;% B\n\n\n\nAs an example to how this is implemented, we can have a look at the {fpCompare} source code for one of the operators:\n\n`%&lt;=%` &lt;- function(x, y) {\n  (x &lt; y + getOption(\"fpCompare.tolerance\"))\n}\n\nEven if y is ever so slightly smaller than x, adding the tolerance to y will make the result larger than x, and the comparison will return TRUE.\n\n# we need to set the fpCompare.tolerance first, because we did not load the package:\noptions(fpCompare.tolerance = 1e-8)\n\n(ANRHI * 1.1) %&lt;=% AVAL\n\n[1] TRUE\n\n\nAs long as {admiral} remains open source and free to use, using this package, or even reusing the code itself would be fine. Although this was my preferred option, we did not end up implementing it. Instead, we made use of the signif() function, which rounds a number to a specified number of significant digits. This way, we could use the regular infix operators and simply provide the number of significant digits we want to compare to:\n\nsignif_dig &lt;- 15\n\nsignif(AVAL, signif_dig) == signif(ANRHI * 1.1, signif_dig)\n\n[1] TRUE\n\n# as:\n(ANRHI * 1.1) %&gt;%\n  signif(signif_dig) %&gt;%\n  format(digits = 22)\n\n[1] \"110\"\n\n# and although when printed, the number still looks off:\nANRHI &lt;- 101\n((ANRHI * 1.1) %&gt;% signif(signif_dig)) %&gt;% format(digits = 22)\n\n[1] \"111.0999999999999943157\"\n\n# the comparison works now:\n((ANRHI * 1.1) %&gt;% signif(signif_dig)) == 111.1\n\n[1] TRUE\n\n\nThis is now implemented throughout atoxgr_criteria_daids, atoxgr_criteria_ctcv4, and atoxgr_criteria_ctcv5, and we are working on an issue for the 1.0.0 release of {admiral} to implement this for derive_var_anrind as well.\n\natoxgr_criteria_daids %&gt;%\n  select(TERM, GRADE_CRITERIA_CODE) %&gt;%\n  reactable(defaultPageSize = 4, highlight = TRUE, bordered = TRUE, striped = TRUE, resizable = TRUE)"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#conclusion",
    "href": "posts/2023-10-30_floating_point/floating_point.html#conclusion",
    "title": "Floating point",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe recent challenges faced by {admiral} in dealing with floating point values shed light on the complexities and nuances of working with these numerical representations. Floating point values, as we’ve seen, are approximations of real numbers and can lead to unexpected issues in mathematical operations, especially when using exact comparators like == and &gt;=. The differences between how these values are stored and computed can result in platform-specific discrepancies and unexpected behavior.\nSeveral potential solutions were explored to address this issue, including rounding, using near() or all.equal() functions, or implementing custom infix operators as seen in the fpCompare package. However, the most elegant and practical solution adopted in {admiral} was to use the signif() function to round values to a specified number of significant digits. This approach allows for reliable and consistent comparisons without adding unnecessary complexity to the code base.\nReaders and developers should be vigilant when working with floating point values in their own code or when utilizing {admiral} for their projects. Keep in mind that some floating point values can look like integers at first glance as in the above example of 1.1*100. The experience with floating point issues in {admiral} serves as a valuable reminder of the potential pitfalls associated with numerical precision in programming. It’s crucial to exercise caution when performing comparisons with floating point numbers as small discrepancies can have significant downstream implications. When writing your own comparisons consider the following best practices:\n\nAvoid Exact Comparisons: As highlighted earlier, using exact comparators like == or &gt;= when dealing with floating point values can lead to unexpected results. Instead, opt for methods that take into account a tolerance or margin of error, such as the near() function or the signif() approach discussed in this context.\nPlatform Independence: Be aware that floating point representations may differ across various platforms or environments. Always test your code on multiple platforms to ensure consistency in results.\nDocumentation and Comments: When writing code that potentially involves floating point comparisons, it’s advisable to include clear documentation and comments that explain the reasoning behind your approach. This will help others understand and maintain the code effectively.\nTesting and Validation: Implement thorough testing and validation procedures to verify the correctness of your code, particularly when it relies on floating point comparisons. This should include specific tests that would flag floating point issues on any machine or platform.\n\nBy heeding these precautions and understanding the intricacies of floating point representations, you can mitigate the risk of encountering unexpected behavior in your code. Whether you’re working with {admiral} or any other software, a cautious and informed approach to handling floating point values is essential for maintaining code accuracy and reliability.\n** This is a number of the smallest magnitude for which a difference is still detected. I.e. .Machine$double.eps / 1.8 is still detectable, while .Machine$double.eps / 2 is not detectable any longer (at least on my machine):\n\n# eps / 1.8 is still detectable:\n.Machine$double.eps / 1.8 + 1 == 1\n\n[1] FALSE\n\n.Machine$double.eps / 2 + 1 == 1\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#last-updated",
    "href": "posts/2023-10-30_floating_point/floating_point.html#last-updated",
    "title": "Floating point",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:16.479388"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#details",
    "href": "posts/2023-10-30_floating_point/floating_point.html#details",
    "title": "Floating point",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html",
    "href": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html",
    "title": "Writing my first custom CICD action for the pharmaverseblog",
    "section": "",
    "text": "Each pharmaverseblog post is tagged with one or more categories that describe the topics discussed within it. For instance, this post is tagged Technical. When making a new blog post, users are invited to select the tags from a curated list designed to split the posts according to categories that balance specificity and generality. Here is the list we currently use:\n\nc(\"Metadata\", \"SDTM\", \"ADaM\", \"TLG\", \"Shiny\", \"Community\", \"Conferences\", \"Submissions\", \"Technical\")\n\nUsers can add to this list, however we have observed that more often than not, if users do diverge, it is due to a typo. This has unfortunate effects within our pharmaverseblog, chiefly that our front page glossary of posts is now split:\n\n\n\n\n\nWithin the pharmaverseblog editor team, we wondered whether there was a simple way to police these tags a bit more, perhaps in an automated manner. Enter CICD checks!\n\n\n\n\n\n\nWhat is CICD?\n\n\n\nCICD stands for Continuous Integration, Continuous Deployment and is a catch-all term for automated code pipelines that ensure that new code added to an existing codebase seamlessly assimilates with the rest of the codebase without introducing unexpected behavior.\nOften, when working in R projects hosted on GitHub, one encounters CICD in the form of checks that are triggered when making a pull request to the main branch (see image below for an example). These can check all sorts of aspects, ranging from correct spelling in your documentation all the way to executing your function unit tests and checking they all pass.\n\n\n\n\n\n\n\nThe pharmaverseblog already had three active CICD pipelines, which run for spelling, style and links. For style and links, we did not write the code for these checks ourselves, rather we just activated open-source checks for our code repository. For spelling, this is a custom pipeline written by one of our blog editors, Stefan Thoma. So, what if we could write another custom CICD pipeline to check that new blog posts use tags from our selected list?"
  },
  {
    "objectID": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#constructing-the-check",
    "href": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#constructing-the-check",
    "title": "Writing my first custom CICD action for the pharmaverseblog",
    "section": "Constructing the check",
    "text": "Constructing the check\nI decided that my strategy would be to write the main body of my CICD check as an R script. As I am relatively confident in R, the main challenge for me would be to then figure out how to automatically execute that script in a CICD pipeline.\nAfter some playing around, I settled with the following 30-line script:\n\n# Get list of blog posts ----\nposts &lt;- list.files(\"posts\", recursive = TRUE, pattern = \"*.qmd\")\n\n# Get vector of allowed tags ----\nsource(\"R/allowed_tags.R\")\n\n# Function to extract tags from a post and check them against the allowed list ----\ncheck_post_tags &lt;- function(post, allowed_post_tags = allowed_tags) {\n  post_tags &lt;- rmarkdown::yaml_front_matter(file.path(\"posts\", post))$categories\n  cross_check &lt;- post_tags %in% allowed_post_tags\n  problematic_tags &lt;- post_tags[!cross_check]\n\n  if (!all(cross_check)) {\n    cli::format_message(\"The tag(s) {.val {problematic_tags}} in the post {.val {post}} are not from the allowed list of tags.\")\n  }\n}\n\n# Apply check_post_tags to all blog posts and find problem posts ----\ncheck_results &lt;- lapply(posts, check_post_tags)\nerror_messages &lt;- unlist(Filter(Negate(is.null), check_results))\n\n# Construct error message if one or more posts have problematic tags ----\nif (length(error_messages) &gt; 0) {\n  error_messages &lt;- c(error_messages, \"Please select from the following tags: {.val {allowed_tags}}, or contact one of the maintainers.\")\n  names(error_messages) &lt;- rep(\"x\", length(error_messages) - 1)\n\n  concatenated_error_messages &lt;- cli::cli_bullets(error_messages)\n\n  cli::cli_abort(concatenated_error_messages)\n}\n\nThe script works as follows:\n\nGet a full list of blog posts. These are all the .qmd files within the posts folder of the pharmaverseblog repo.\nSpecify a vector of “allowed tags” in the allowed_tags.R script.\nSpecify a function that, given a post:\n\nExtracts categories from the yaml header of the .qmd file.\nCross-checks the tags with the allowed ones.\nFor problematic tags, uses the {cli} package to construct a nicely-formatted error message.\n\nLoop check_post_tags() over all blog posts using a simple lapply() call and extract all error messages.\nIf there are any error messages, use {cli} again to construct a concatenated error message.\n\nThe final error message looks something like the below:\n✖ The tag(s) \"ADaMs\" in the post\n  \"2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.qmd\"\n  are not from the allowed list of tags.\nPlease select from the following tags: \"Metadata\", \"SDTM\", \"ADaM\", \"TLG\", \"Shiny\",\n\"Community\", \"Conferences\", \"Submissions\", and \"Technical\", or contact one of the maintainers."
  },
  {
    "objectID": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#creating-a-pipeline-for-the-check",
    "href": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#creating-a-pipeline-for-the-check",
    "title": "Writing my first custom CICD action for the pharmaverseblog",
    "section": "Creating a pipeline for the check",
    "text": "Creating a pipeline for the check\nWhen it came to creating a pipeline for the check, if you had asked me to do this a few months ago, I wouldn’t have known my left from my right. Luckily I had recently attended a great CICD workshop at useR 2024 in Salzburg, led by Daphne Grasselly, Franciszek Walkowiak and Pawel Rucki. You can find the repository from their workshop here - it was invaluable to orient me in the right direction. With a (very naive) google search, I also found this video, which shows how to execute an R script automatically whenever a pull request is made to the main branch of a repo.\nThrough some trial and error, I was able to coalesce the above resources into quite a short yaml file that set up my CICD pipeline. Within the pharmaverseblog repository, the CICD pipelines live under .github/workflows. There, I added the following new workflow, in the form of a yaml file, called check_post_tags.yml:\nname: Check post tags\n\non:\n  pull_request:\n    branches:\n      - 'main'\n\njobs:\n  Check-post-tags:\n    runs-on: ubuntu-latest\n    container:\n      image: \"rocker/tidyverse:4.2.1\"\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run check_post_tags\n        run: source(\"R/check_post_tags.R\")\n        shell: Rscript {0}\nIt’s deceptively simple to read:\n\nWe execute the workflow upon any pull request to main.\nWhen there is a pull request to main, we can load the rocker/tidyverse:4.2.1 docker image which has all the {tidyverse} packages pre-installed.\nThen we need to checkout the pharmaverseblog repo and run the check_post_tags.R script.\n\nThat’s it! If there are any problematic posts, the script will throw an error and the check will fail like so:\n\n\n\n\n\nClicking on the “Details” option will return the error message I constructed previously.\n\n\n\n\n\nOtherwise, no error will be thrown, the check will pass, and the post is good to go (provided the other checks pass)!"
  },
  {
    "objectID": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#last-updated",
    "href": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#last-updated",
    "title": "Writing my first custom CICD action for the pharmaverseblog",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:10.586283"
  },
  {
    "objectID": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#details",
    "href": "posts/2024-09-11_writing_my_first.../writing_my_first_custom_ci_cd_action_for_the_pharmaverseblog.html#details",
    "title": "Writing my first custom CICD action for the pharmaverseblog",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "",
    "text": "The importance of reliable and accurate software in the pharmaceutical industry cannot be overstated. As you grapple with the complexities of developing Shiny applications that not only meet requirements but also remain free of defects, the conversation inevitably turns to the topic of validation. This is where the Rhino framework comes into play. It provides a validation environment through a set of tools that not only ensure the thorough testing of your Shiny Application but also enforce good software practices for producing quality code."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#what-is-validation",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#what-is-validation",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "What is Validation?",
    "text": "What is Validation?\nAt its core, validation is about generating objective evidence that the software consistently fulfills the users’ needs and requirements. It is closely intertwined with concepts such as verification, testing, quality assurance, and quality control. However, it is not a one-time activity. Validation is an ongoing aspect of the entire development process for assessing whether an application meets requirements and is free of defects. (phuse, 2021)\nThe foundational aspect of validation lies in the clarity and precision of the requirements set upfront. Projects often encounter challenges not due to technical constraints but unclear requirements. This clear definition of needs is the first and most critical step in the validation process, setting the stage for effective and meaningful software evaluation.\nTesting in some form is indispensable in any software engineering project. However, writing automated tests costs time and effort. Therefore, the approach to testing can vary widely depending on the project’s scope, complexity, and purpose.\nFor instance, manual testing, which involves checking the software by hand for bugs or other issues, might suffice for a Proof of Concept (PoC) application. On the other hand, in applications performing critical functions, such as those involved in clinical trial data analysis, the cost of failure could be extremely high, necessitating more rigorous and comprehensive automated testing.\nThe key is to strike a balance, ensuring the software is thoroughly validated without exhausting resources."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#how-rhino-helps-in-validating-r-shiny-apps",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#how-rhino-helps-in-validating-r-shiny-apps",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "How Rhino Helps in Validating R Shiny Apps",
    "text": "How Rhino Helps in Validating R Shiny Apps\nRhino automates the steps for creating the validation environment so you can spend your time and effort on writing the actual tests.\n\n\n\nValidation with Rhino\n\n\nWhen you first run rhino::init() inside your project’s root directory, Rhino introduces a suite of tools that will help you validate your Shiny application and a robust file structure to modularize your code into meaningful parts. With Rhino initialized, validating your Shiny application becomes an integral and seamless part of your development workflow.\n\nTesting\nRhino comes with testthat installed and introduces the tests/ directory in your project. As part of the validation process, you want to test your requirements. With Rhino, all you need to do is create your test-*.R files in tests/testthat directory and run rhino::test_r().\nAlthough shinytest2 tests can also be added with minimal effort (How to use shinytest2), one of the distinct features of Rhino is the ability to write end-to-end tests with Cypress out of the box. To learn more about how to write Cypress tests via Rhino, you can read more on Write end-to-end tests with Cypress tutorial in the documentation.\n\nContinuous Integration via GitHub\nAs discussed in the “What is validation?” section, validation is a continuous process. To accomplish this, Rhino uses a GitHub Actions workflow. This file enables the automatic running of tests and linting checks every time the code is pushed to the Github repository.\nYou can further configure your repository to require these checks to pass before a pull request can be merged, ensuring only validated code makes it into your project.\n\n\n\nStandardization of Good Practices\nRhino also provides tools for good software development practices. These standardization tools don’t directly address requirements or defects, but they raise the overall code quality.\n\nrhino::lint_* Functions\nRhino provides linters for R (rhino::lint_r), Javascript (rhino::lint_js), and Sass (rhino::lint_sass) code. These linters ensure that the code style is consistent. This consistency might not be visible to the end-users, but it significantly enhances code readability and maintainability, which are crucial for reducing bugs and errors in the application. The cleaner and more readable the code, the easier it is for you to spot and rectify issues.\n\n\nModularization with box\nModularization allows developers to compartmentalize different aspects of the application, making each part easier to understand, develop, and validate individually.\nRhino leverages the box package for effective modularization. This approach to structuring the app makes the codebase more manageable and navigable and significantly enhances the ease of testing and maintenance.\n\n\nReproducibility with renv\nrenv is currently the best tool for ensuring reproducibility and consistency across different development environments. Each Rhino project comes with renv activated. It further automatizes managing dependencies with rhino::pkg_install() and rhino::pkg_remove() functions to install, update or remove a package with just one call. Check out this explanation on Renv configuration to learn more!"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#wider-picture",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#wider-picture",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Wider picture",
    "text": "Wider picture\n\nBare Bones Approach\nA minimal Shiny app can be as simple as a single file. This ‘bare bones’ approach is the most straightforward, requiring minimal setup. However, it places the workload on the developer to establish everything from scratch.\nThis method may be suitable for quick prototypes or small-scale projects but it lacks the robustness needed for complex, large-scale applications, especially in a field as critical as pharmaceuticals.\n\n\nOther Shiny Frameworks\nFrameworks like Golem and Leprechaun adopt R package structure for Shiny apps. This method allows developers to leverage standard R package testing tools like R CMD check.\nWhile this approach brings the benefits of R’s package development ecosystem, it does not offer extra state-of-the-art tools such as linters, Cypress tests, and GitHub Actions that Rhino provides out of the box. Leaving the burden of manually setting them up on the developers.\nRead our {rhino} vs {golem} vs {leprechaun}: Which R/Shiny Library is Right for You? blog post to learn more about the differences.\n\n\nValidation Theater\nTest coverage is often cited as a key metric for assessing the reliability of an application. While high test coverage is beneficial, it is not a definitive guarantee of software quality. The quality of the tests themselves matters significantly more. A common pitfall is the overreliance on achieving 100% test coverage, which may give a false sense of security about the application’s robustness.\nAdditionally, R CMD checks provide reassurance with their green check marks. However, many of these automated validations are not directly applicable or relevant to Shiny apps. This is one of the reasons why Rhino does not use these checks. While they are useful, they do not encompass the full spectrum of what needs to be tested in a Shiny application.\nThe key takeaway is that no automated check can substitute for a thorough and well-thought-out development process. The quality of a Shiny application hinges not just on the tests it passes but on the entirety of its development lifecycle, from initial design to final deployment."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#summing-up-rhino-for-r-shiny-app-validation",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#summing-up-rhino-for-r-shiny-app-validation",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Summing up Rhino for R Shiny App Validation",
    "text": "Summing up Rhino for R Shiny App Validation\nRhino adopts a software engineering perspective, focusing primarily on the technical facets of validation. It provides a comprehensive suite of tools that streamline the process of creating Shiny applications that are not only functionally sound but also robust and maintainable.\nHowever, it’s crucial to recognize that Rhino’s contribution is a piece of a much larger puzzle. Validation, as emphasized throughout this post, is not a standalone activity but a continuous process that intertwines with every phase of software development.\nFrom the initial requirement gathering to the final deployment, each step plays a vital role in shaping the overall quality and effectiveness of the application.\nIf you’d like to learn more about Rhino, especially within the context of building Shiny apps for regulatory submissions you can check out this blogpost, Reproducible and Reliable Shiny Apps for Regulatory Submissions or visit Appsilon’s website."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#references",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#references",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "References",
    "text": "References\nphuse. (2021). R Package Validation Framework. R Package Validation Framework. https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Data+Visualisation+%26+Open+Source+Technology/WP059.pdf\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#last-updated",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#last-updated",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:47.751026"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#details",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#details",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "",
    "text": "In pharmaceutical research, high-performance computing (HPC) plays a pivotal role in driving advancements in drug discovery and development. From analyzing vast genomic datasets to simulating drug interactions across diverse populations, HPC enables researchers to tackle complex computational tasks at high speeds. As pharmaceutical research becomes increasingly data-driven, the need for powerful computational tools has grown, allowing for more accurate predictions, faster testing, and more efficient processes. However, with the growing complexity and scale of these computations, ensuring reproducibility of results becomes a significant challenge.\nIn this blog post, we will explore common reproducibility challenges in drug development and simulations, using the {mirai} package as a backend solution to manage parallelization."
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#harnessing-hpc-for-drug-development",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#harnessing-hpc-for-drug-development",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "",
    "text": "In pharmaceutical research, high-performance computing (HPC) plays a pivotal role in driving advancements in drug discovery and development. From analyzing vast genomic datasets to simulating drug interactions across diverse populations, HPC enables researchers to tackle complex computational tasks at high speeds. As pharmaceutical research becomes increasingly data-driven, the need for powerful computational tools has grown, allowing for more accurate predictions, faster testing, and more efficient processes. However, with the growing complexity and scale of these computations, ensuring reproducibility of results becomes a significant challenge.\nIn this blog post, we will explore common reproducibility challenges in drug development and simulations, using the {mirai} package as a backend solution to manage parallelization."
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#the-problem-reproducibility-in-parallel-processing",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#the-problem-reproducibility-in-parallel-processing",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "The Problem: Reproducibility in Parallel Processing",
    "text": "The Problem: Reproducibility in Parallel Processing\nImagine a research team working on a cutting-edge drug development project. To process and analyze vast amounts of data efficiently, they leverage parallel processing, distributing tasks across multiple processors. This approach significantly accelerates their work, enabling them to handle large datasets and complex computations in a fraction of the time.\nHowever, the team soon encounters an issue. Each time they rerun the same processing tasks with identical input parameters, the results differ slightly. This raises a major concern: the results are not reproducible. In industries like pharmaceuticals, where accuracy and consistency are critical, reproducibility is not just important—it’s a regulatory requirement.\nFor example, in large-scale Monte Carlo simulations, small differences can arise not only from changes in execution order across processors but also from inconsistencies between workers or difficulties in maintaining synchronized random number generation (RNG) streams. Furthermore, the more complex the environment—with multiple components such as distributed workers, different hardware, or varying system configurations—the harder it becomes to reprovision the exact same environment and repeat the computations exactly. As these variations accumulate, ensuring consistent and reproducible results becomes a significant challenge in data-driven research.\n\nTracking Operations in Parallel Computing\nLet’s explore a simple scenario where parallelization creates confusion in tracking operations due to the asynchronous nature of task execution and logging. For this, we will also use the {tidylog} package, which tracks and logs {dplyr} operations, providing insight into how the computations are executed across multiple workers.\nWe’ll create our workflow in a script and run it using the {logrx} package from Pharmaverse. The workflow will be written as an expression using base::substitute(), which will help generate the complete script. In our example, we’ll start four daemons. A daemon is a background process that runs in the background continuously and handles specific computing tasks.\n\nmirai_workflow &lt;- substitute({\n  library(\"mirai\")\n  library(\"dplyr\")\n\n  log_file &lt;- tempfile()\n\n  # start parallel workers\n  mirai::daemons(4)\n\n  # load libraries on each worker and set up logging to a file\n  mirai::everywhere(\n    {\n      library(\"dplyr\")\n      library(\"tidylog\")\n\n      # Define function to log messages to the log file\n      log_to_file &lt;- \\(txt) cat(txt, file = log_file, sep = \"\\n\", append = TRUE)\n      options(\"tidylog.display\" = list(message, log_to_file))\n    },\n    log_file = log_file\n  )\n\n  # perform computations in parallel\n  m &lt;- mirai_map(letters[1:5], \\(x) {\n    mutate(tibble(.rows = 1), \"{x}\" := sample(1:100, 1))\n  })\n\n  # collect results\n  result &lt;- m[] |&gt; bind_cols()\n\n  mirai::daemons(0)\n\n  print(\n    list(\n      logs = readLines(log_file),\n      result = result\n    )\n  )\n})\n\nIn the above code chunk, we set up a parallel processing environment using the {mirai} package. The function mirai_map() is used to apply a mutating function in parallel to a tibble for each element of letters, logging the operations to a file using the {tidylog} package. However, while we can log each operation as it happens, due to the parallel nature of {mirai}, the logging does not occur in a controlled or sequential order. Each daemon executes its task independently, and the order of logging in the file will depend on the completion times of these parallel processes rather than the intended flow of operations.\n\nParallel computations can obscure the traceability of operations\n\nThis lack of control can lead to a situation where the log entries do not reflect the actual sequence in which the {dplyr} commands were expected to be processed. Although the operations themselves are carried out correctly, the asynchronous logging may create challenges in tracing and debugging the process, as entries in the log file could appear out of order, giving an incomplete or misleading representation of the task flow.\nLet’s first save the code to an R script called mirai_workflow.R. This step helps ensure that the execution can be properly tracked and documented:\n\nmirai_workflow |&gt;\n  deparse() |&gt;\n  writeLines(\"mirai_workflow.R\")\n\nNext, we execute the script using logrx::axecute(), which not only runs the workflow but also logs key metadata and outputs for enhanced traceability and reproducibility:\n\nlogrx::axecute(\"mirai_workflow.R\", to_report = \"result\")\n\n\n\n$logs\n[1] \"mutate: new variable 'a' (integer) with one unique value and 0% NA\"\n[2] \"mutate: new variable 'c' (integer) with one unique value and 0% NA\"\n[3] \"mutate: new variable 'b' (integer) with one unique value and 0% NA\"\n[4] \"mutate: new variable 'e' (integer) with one unique value and 0% NA\"\n[5] \"mutate: new variable 'd' (integer) with one unique value and 0% NA\"\n\n$result\n  a  b  c  d  e\n1 8 25 86 43 46\n\n\nUpon examining the log file generated, you’ll notice that the entries are not in the same order as the commands were dispatched. This illustrates the inherent difficulty in maintaining a consistent logging sequence for parallel tasks, especially since the timing of each process completion and log recording is unpredictable.\nAdditionally, it is worth noting that logrx does not capture the logging performed by {tidylog} during the execution of tasks on {mirai} daemons. This is because the daemons run as independent R processes, and the logging messages are not propagated back to the parent process in a straightforward manner. As described in {mirai}’s documentation, daemons are responsible for handling tasks asynchronously, and messages logged within these processes do not automatically integrate into the parent session. Therefore, we access {tidylog} messages indirectly, by reading the dedicated log file (log_file) that each worker writes to during execution.\n\n\nTask Dispatching and RNG Management\nBy default, {mirai} uses an advanced dispatcher to manage task distribution efficiently, scheduling tasks in a First-In-First-Out manner and leveraging {nanonext} primitives for zero-latency, resource-free task management. However, its asynchronous execution can hinder reproducibility, especially with random number generation (RNG) or tasks needing strict order.\nTo enhance reproducibility, {mirai} allows disabling the dispatcher which usually decides the order in which tasks are run. Instead, it connects directly to the workers one by one in a simple order (see round-robin). While less efficient, this approach provides greater control over task execution and is better suited for ensuring reproducibility by initializing L’Ecuyer-CMRG RNG streams.\nIn the following example, we simulate drug efficacy across different patient cohorts using parallel processing with the {mirai} package. We define three cohorts, each with a different mean drug effect and standard deviation, and initialize four daemons to handle the computations.\n\nlibrary(mirai)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Parameters for the simulation\ncohorts &lt;- tribble(\n  ~patient_count, ~mean_effect, ~sd_effect,\n  1000,           0.7,          0.1,\n  1000,           0.65,         0.15,\n  1000,           0.75,         0.05\n)\n\n# Start daemons with consistent RNG streams\nx &lt;- mirai::daemons(\n  n = 4,\n  dispatcher = \"none\", # For mirai versions below 1.3.0, use dispatcher = FALSE\n  seed = 123\n)\n\n# Parallel simulation for each row of the cohorts table\nm &lt;- mirai::mirai_map(cohorts, \\(patient_count, mean_effect, sd_effect) {\n  dplyr::tibble(\n    patient_id = 1:patient_count,\n    efficacy = rnorm(patient_count, mean = mean_effect, sd = sd_effect)\n  )\n})\n\nresults &lt;- m[] |&gt; bind_rows()\n\nx &lt;- mirai::daemons(0, dispatcher = \"none\")\n\nresults %&gt;%\n  group_by(patient_id) %&gt;%\n  summarise(\n    mean_efficacy = mean(efficacy),\n    sd_efficacy = sd(efficacy)\n  )\n\n# A tibble: 1,000 × 3\n   patient_id mean_efficacy sd_efficacy\n        &lt;int&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1          1         0.630      0.113 \n 2          2         0.772      0.0166\n 3          3         0.745      0.130 \n 4          4         0.660      0.249 \n 5          5         0.730      0.0862\n 6          6         0.617      0.120 \n 7          7         0.772      0.150 \n 8          8         0.529      0.172 \n 9          9         0.674      0.118 \n10         10         0.707      0.0517\n# ℹ 990 more rows\n\n\nWe used tribble() to define the simulation parameters and initialize 4 daemons with dispatcher = \"none\" and a fixed seed to ensure consistent random number generation across tasks. The mirai_map() function parallelizes the drug efficacy simulation, and the results are combined using bind_rows() for further analysis.\nDisabling the dispatcher gives more control over task execution, ensuring reproducibility. If you repeat the computation you will notice that it generates consistent results. However, this approach comes at a cost. Disabling the dispatcher may lead to inefficient resource utilization when tasks are unevenly distributed, as some daemons may remain idle. While reproducibility is prioritized, we sacrifice some performance, especially when handling tasks with varying workloads.\nReproducibility becomes trickier when using parallelization frameworks like {parallelMap}, {doFuture}, and {future}, as each handles random number generation (RNG) differently. While set.seed() works for sequential tasks, parallel tasks need careful management of RNG streams, often using specific methods like “L’Ecuyer-CMRG” or functions like clusterSetRNGStream() to keep results consistent. Each framework has its own approach, so it’s important to understand how each one manages RNG to ensure reproducibility.\nEven without random numbers, simple tasks—like adding floating-point numbers—can give different results in parallel processing. This happens because floating-point numbers aren’t exactly represented, and the order of operations can affect the outcome. In parallel environments, where tasks finish in different orders, these small differences can add up, making it harder to reproduce results in large computations."
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#closing-thoughts",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#closing-thoughts",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nWhile we’ve explored the basics of reproducibility in parallel computing with simple examples, the challenges extend beyond random number generation. Issues such as process synchronization, using tools like lock files (see for example {filelock}), become critical in multi-process environments. Floating-point arithmetic adds complexity, particularly when distributed across heterogeneous systems with varying architectures and precision. Managing dependencies also becomes more intricate as tasks grow in complexity, and ensuring error recovery in a controlled manner is vital to avoid crashes or inconsistent results in large-scale operations.\nPowerful tools like {targets} and {crew} can help tackle these advanced challenges. {targets} is a workflow orchestration tool that manages dependencies, automates reproducible pipelines, and ensures consistent results across runs. Meanwhile, {crew} extends this by efficiently managing distributed computing tasks, allowing for seamless scaling, load balancing, and error handling across local processes or cloud environments. Together, these tools simplify the execution of complex high-performance computing (HPC) workflows, providing flexibility and robustness for scaling computations while trying for maintaining control and reproducibility.\nThis blog post has hopefully increased your intuition about the challenges that may arise when incorporating HPC into your work. By understanding these complexities, you’ll be better positioned to make informed decisions about the trade-offs—such as balancing performance and reproducibility—that are most relevant to your specific case. As your computations scale, finding the right balance between efficiency, accuracy, and reproducibility will be crucial for the success of your projects."
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#last-updated",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#last-updated",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:43.938442"
  },
  {
    "objectID": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#details",
    "href": "posts/2024-10-16_the__tensio.../the__tension_of__high-_performance__computing:__reproducibility_vs.__parallelization.html#details",
    "title": "The Tension of High-Performance Computing: Reproducibility vs. Parallelization",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "",
    "text": "Throughout the pharmaverse journey, our focus has always been on achieving equity across pharma, biotech, charity groups and more. We’ve strived to enable any company, no matter how big or small, to prepare clinical trial reporting using cutting edge and freely available solutions in R. The ultimate ambition being that the best treatments have the best chance of reaching patients all over the world. However, in our focus on this mission we took our eyes off other forms of equity – such as the gender diversity of our representatives.\nDiversity is not a new issue in open source – for example there has long been an uneven gender representation across many communities and initiatives in this space. Given the growing influence of pharmaverse we want to strive to use our platform to not further any such imbalance.\nWith respect to gender diversity, the pharmaverse council is currently formed of 7 male members. Our council is and always has been open to any individual put forth by an organization, and anyone who wants to volunteer in the Pharmaverse is welcome. If this perception has differed, then we have to do our part to ensure anyone feels comfortable bringing their contribution forth and engaging in our community.\nRepresentation starts from the top, and as council members complete their 2-year terms, we will advocate within our companies for more diverse candidates to be proactively encouraged to step forward for consideration.\nWe are committed to using our influence to ensure everyone has an equal opportunity to make a difference in our community at all levels, so this discussion has to continue across package development teams and all the various ways people can contribute.\nTo close, we’d like to reflect on the past years by saying that the true strength of what has been built here is not from us on the council, or from the packages or the developers, it really comes from the whole community being built. This is a community of like-minded people that have come together from all walks of life passionate to make a difference and change the siloed trends of our industry. That is what makes the pharmaverse, and we hope that everyone no matter what demographic you belong to feels welcome to join and give your free time to help do more for ALL patients across the world.\nYours faithfully,\npharmaverse council"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#last-updated",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#last-updated",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:39.870425"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#details",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#details",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html",
    "href": "posts/2023-07-09_cardinal/falcon.html",
    "title": "cardinal",
    "section": "",
    "text": "The {cardinal} initiative is an industry collaborative effort under pharmaverse that unites Boehringer Ingelheim, Moderna, Roche, and Sanofi with the aspiration of building and open-sourcing a comprehensive catalog of harmonized tables, listings, and graphs (TLGs) for clinical study reporting. By leveraging existing open-source R packages, {cardinal} aims to simplify the process of output review, comparison, and meta-analyses, while fostering efficient communication among stakeholders in the pharmaceutical industry."
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#what-is-cardinal",
    "href": "posts/2023-07-09_cardinal/falcon.html#what-is-cardinal",
    "title": "cardinal",
    "section": "",
    "text": "The {cardinal} initiative is an industry collaborative effort under pharmaverse that unites Boehringer Ingelheim, Moderna, Roche, and Sanofi with the aspiration of building and open-sourcing a comprehensive catalog of harmonized tables, listings, and graphs (TLGs) for clinical study reporting. By leveraging existing open-source R packages, {cardinal} aims to simplify the process of output review, comparison, and meta-analyses, while fostering efficient communication among stakeholders in the pharmaceutical industry."
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#why-do-we-build-it",
    "href": "posts/2023-07-09_cardinal/falcon.html#why-do-we-build-it",
    "title": "cardinal",
    "section": "Why do we build it?",
    "text": "Why do we build it?\nThe collaborative effort focuses on improving the clarity, consistency, and accessibility of TLGs by addressing variations and redundancies in their creation and use. This harmonized approach allows for streamlined reporting processes and facilitates effective communication of study results within the industry and to regulatory authorities."
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#what-has-been-done-so-far",
    "href": "posts/2023-07-09_cardinal/falcon.html#what-has-been-done-so-far",
    "title": "cardinal",
    "section": "What has been done so far?",
    "text": "What has been done so far?\nDrawing inspiration from the FDA Standard Safety Tables and Figures Integrated Guide, the {cardinal} initiative has successfully developed and open-sourced 11 templates to date. 4 product owners and 11 developers from 4 companies have collaborated to make these templates available and also published them on the official {cardinal} website."
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#next-steps-vision",
    "href": "posts/2023-07-09_cardinal/falcon.html#next-steps-vision",
    "title": "cardinal",
    "section": "Next steps & vision",
    "text": "Next steps & vision\nFuture plans for {cardinal} involve expanding the catalog through continuous collaboration from participating companies and inviting wider industry engagement. The ultimate goal is to promote harmonization of TLGs for clinical reporting across the pharmaceutical industry, leading to greater efficiency, collaboration, and innovation. Even though templates currently come from a published FDA guide, the collaborating companies are open to share and discuss similarities and differences on analysis concepts and output layouts of their own implementations in clinical reporting, for both safety and efficacy analyses.\nIn addition, while currently all templates were built using {rtables}, {tern}, {rlistings} and drew inspiration from the open-sourced TLG-Catalog, moving forward, we plan to support creating the same templates using alternative table engines such as {gt}.\n{cardinal} will be presented at the upcoming PHUSE EU (Standards Implementation stream), where we will share the collaboration journey of {cardinal} so far, providing more details on the current progress, long-term vision, and strategies for this initiative. Attendees will gain insights into the challenges and opportunities of harmonizing clinical reporting through open-source collaboration and learn about the potential benefits and future direction of {cardinal}."
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#last-updated",
    "href": "posts/2023-07-09_cardinal/falcon.html#last-updated",
    "title": "cardinal",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:36.412215"
  },
  {
    "objectID": "posts/2023-07-09_cardinal/falcon.html#details",
    "href": "posts/2023-07-09_cardinal/falcon.html#details",
    "title": "cardinal",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html",
    "href": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html",
    "title": "Council Member updates",
    "section": "",
    "text": "Hi pharmaverse community,\nOn behalf of the council we wanted to update you on the latest council member 2-yearly renewals.\nFirstly, we’d like to thank Ari Siggaard Knoph who has stepped back from his Novo Nordisk council representative role as of Jan 2025. Ari brought so much to the pharmaverse council in his 2 years with us, including amazing technical work to make our site homepage as visually appealing as it is today with our hexwall and contributor network graph, plus impactful presentations at PHUSE and beyond. He showed great passion for open source and as a user himself of many of the pharmaverse packages, he always kept us true to what people working in clinical reporting care most about.\nWith this, we are delighted to welcome Julia Gnatek as Ari’s replacement to represent Novo Nordisk on the council. Julia has been a statistical programmer at Novo Nordisk for over 5 years. She began her career as a statistical programmer and is now a statistical programming project lead within the Rare Disease area. As an R, SAS, and Python user, Julia is dedicated to leveraging her programming skills to drive innovation within the organization. Outside of work, she enjoys participating in data science communities and attending industry conferences to stay updated with the latest trends and advancements.\nPlease join us in thanking Ari for his contributions and welcoming Julia!\nWe also have several other council members terms due to expire as of March 2025, and we’re happy to confirm that Michael Rimler, Mike Stackhouse, Sumesh Kalappurakal and Ross Farrugia have all agreed to renew council membership for another 2 years. As the 4 founding members of pharmaverse we all felt there was still so much more impact this initiative can achieve, and we wanted to continue to commit our time to helping open source prosper for the good of all patients across the world.\nIf you ever want to know more about our council please check our site page, and you can reach all of us using: pharmaverse.council@phuse.global."
  },
  {
    "objectID": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html#last-updated",
    "href": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html#last-updated",
    "title": "Council Member updates",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:32.694372"
  },
  {
    "objectID": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html#details",
    "href": "posts/zzz_DO_NOT_EDIT_council__me.../council__member_updates.html#details",
    "title": "Council Member updates",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html",
    "href": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html",
    "title": "Meet our Diversity Champion – Laura Needleman",
    "section": "",
    "text": "On behalf of the pharmaverse council, we’d like to introduce you to Laura Needleman – an active member of our community who has volunteered to support us as a champion around DE&I. All of our efforts to this day have started from the ground up with passionate people putting their hands up to make a difference for our industry and patients all over the world. Therefore, when Laura identified a challenge and stepped forward to help foster solutions, this resonated hugely with us as a group that are looking to learn and evolve continually to make a positive impact in our scope of open source. Here’s some further introduction and context from Laura in her own words…\n“Over the summer, I approached the pharmaverse council ready to join forces with a Diversity Champion proposal in hand. As I walked the council through my proposal and thoughts, I believe they saw my passion for equal access and representation was burning bright! I was ready to hit the ground running to help the community members feel represented and heard within pharmaverse.\nI plan to work with the council to not only suggest ways to increase more diversity, equity, and inclusion at the top (within the council) and within the community but also as an autistic brain ready to pick apart systems and ask the questions others might not be asking. If there is one thing autistics hit out of the park… it’s disrupting groupthink. Do I hear a record scratch every time I open my mouth and say something? …yes …yes I do…\nJokes aside, one thing that drew my attention to pharmaverse and why I wanted to give my time to further DE&I is because I could see the large-scale movement that was happening from the energy the pharmaverse platform was creating. The increase in R collaboration over the past 4 years in our industry is noticeable and palpable even and I believe that movement is in part because of the systems and communities built off of the pharmaverse foundation. As an R user myself, and as someone in our biometrics clinical reporting community of programmers and statisticians, it’s a grassroots type space where people are coming with their ideas on how to make it better and it grows year by year. Even when that feedback is uncomfortable to hear, such as gender disparity in the council, response and action is taken and that is something I can back and believe in.\nAs far as my professional background in this space, currently I am the US Policy Lead for Astrazeneca’s Neurodiversity Employee Resource Group, TH!NK, which is a role designed to advocate for neurodivergent employees around accommodations and team perceptions as well as contribute to US Policies from a neurodivergent perspective. I am also an autistic public advocate in our space of biometrics clinical trials where I openly share my experiences in our industry as an autistic woman. I have presented topics of Neurodiversity at both PharmaSUG and PHUSE and have an active voice on LinkedIn. In addition, in 2022, I was awarded Cytel’s CEO award for furthering their DE&I mission.\nDE&I work is an intentional process that is done consciously and continuously. My presence within the pharmaverse council is a relationship and not a solution in it of itself. I come to the council as an individual advisor and thus my presence within the council does not represent Astrazeneca joining the council. Please reach out to me if you’d like to chat about DE&I at pharmaverse, in general or on a specific concern! I am listening and I’ll work to elevate voices that need it. Remember, I’m not perfect, I also have a disability, and I am here to do volunteer DE&I work at no cost.\nFeel free to reach out via email at any time – laura.needleman@astrazeneca.com or connect and reach out to me on LinkedIn!”"
  },
  {
    "objectID": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html#last-updated",
    "href": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html#last-updated",
    "title": "Meet our Diversity Champion – Laura Needleman",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:28.800444"
  },
  {
    "objectID": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html#details",
    "href": "posts/2024-09-03_meet_our__divers.../meet_our__diversity__champion_–__laura__needleman.html#details",
    "title": "Meet our Diversity Champion – Laura Needleman",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html",
    "title": "admiral 1.0.0",
    "section": "",
    "text": "admiral 1.0.0 is out on CRAN. This release brings several new features to your tool set for working with ADaMs in R. admiral 1.0.0 also brings needed stability to users who were wishing to adopt admiral, but were a little worried by the fast deprecation and experimentation for pre-v1.0.0 releases.\nThis blog post will discuss our commitment to stability, walk you through the new features available, discuss some of the bug fixes, a push for common APIs across our functions, and showcase the resources available to help you on-board to admiral."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_extreme_event",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_extreme_event",
    "title": "admiral 1.0.0",
    "section": "derive_vars_extreme_event()",
    "text": "derive_vars_extreme_event()\nThis function takes available records from user-defined events by selecting the extreme observations and appending them as a variable(s) to your dataset.   derive_vars_extreme_event()  works similar to   derive_extreme_event() , but instead of adding observations the function will add variable(s).\nLet’s take a peek with a very simple example where we just use ADSL! The documentation for   derive_vars_extreme_event()  has a much richer example with events from other domains that is more aligned to where you would use this function.\nLet us make some dummy ADSL data and load up our packages. The goal here is to add two new variables LSTALVDT and DTHFL based on a list of objects that are used to specify the following:\n\nthe dataset to look at\na set of conditions\nwhat to set the values for the new variables\n\n\nlibrary(tibble)\nlibrary(admiral)\nlibrary(lubridate)\n\nadsl &lt;- tribble(\n  ~STUDYID, ~USUBJID, ~TRTEDT, ~DTHDT,\n  \"PILOT01\", \"01-1130\", ymd(\"2014-08-16\"), ymd(\"2014-09-13\"),\n  \"PILOT01\", \"01-1133\", ymd(\"2013-04-28\"), ymd(\"\"),\n  \"PILOT01\", \"01-1211\", ymd(\"2013-01-12\"), ymd(\"\"),\n  \"PILOT01\", \"09-1081\", ymd(\"2014-04-27\"), ymd(\"\"),\n  \"PILOT01\", \"09-1088\", ymd(\"2014-10-09\"), ymd(\"2014-11-01\"),\n)\n\nIn this example, we only use ADSL as the source dataset, so it is a bit contrived, but much more compact for us. Note the use of the events that is taking in our list of event objects and the different conditions and values we set to create our LSTALVDT and DTHFL variables.\n\nderive_vars_extreme_event(\n  adsl,\n  by_vars = exprs(STUDYID, USUBJID),\n  events = list(\n    event(\n      dataset_name = \"adsl\",\n      condition = !is.na(DTHDT),\n      set_values_to = exprs(LSTALVDT = DTHDT, DTHFL = \"Y\")\n    ),\n    event(\n      dataset_name = \"adsl\",\n      condition = !is.na(TRTEDT),\n      set_values_to = exprs(LSTALVDT = TRTEDT, DTHFL = \"N\")\n    )\n  ),\n  source_datasets = list(adsl = adsl),\n  order = exprs(LSTALVDT),\n  mode = \"last\",\n  new_vars = exprs(LSTALVDT = LSTALVDT, DTHFL = DTHFL)\n)\n\n# A tibble: 5 × 6\n  STUDYID USUBJID TRTEDT     DTHDT      LSTALVDT   DTHFL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;\n1 PILOT01 01-1130 2014-08-16 2014-09-13 2014-09-13 Y    \n2 PILOT01 01-1133 2013-04-28 NA         2013-04-28 N    \n3 PILOT01 01-1211 2013-01-12 NA         2013-01-12 N    \n4 PILOT01 09-1081 2014-04-27 NA         2014-04-27 N    \n5 PILOT01 09-1088 2014-10-09 2014-11-01 2014-11-01 Y    \n\n\nOkay! We used a very small example to showcase how to find extreme observations and appending this information as new variables to our ADSL dataset. Highly recommend checking out the more detailed example in the function documentation to see its true power!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_var_merged_ef_msrc",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_var_merged_ef_msrc",
    "title": "admiral 1.0.0",
    "section": "derive_var_merged_ef_msrc()",
    "text": "derive_var_merged_ef_msrc()\nThis function has some similarity to   derive_vars_extreme_event() , but now we are only looking at adding a single flag variable based on checking conditions across multiple datasets.\nWe develop some simple dummy data for ADSL, CM and PR. Our goal is to flag patients who have CMCAT = \"ANTI-CANCER\" in the CM dataset or have records in the PR dataset. Any participants who meet these conditions will have our new variable CANCTRFL set as \"Y\".\n\nadsl &lt;- tribble(\n  ~USUBJID,\n  \"1\",\n  \"2\",\n  \"3\",\n  \"4\",\n)\n\ncm &lt;- tribble(\n  ~USUBJID, ~CMCAT,        ~CMSEQ,\n  \"1\",      \"ANTI-CANCER\",      1,\n  \"1\",      \"GENERAL\",          2,\n  \"2\",      \"GENERAL\",          1,\n  \"3\",      \"ANTI-CANCER\",      1\n)\n\npr &lt;- tribble(\n  ~USUBJID, ~PRSEQ,\n  \"2\",      1,\n  \"3\",      1\n)\n\nNow we have the argument flag_events that takes a list of objects where we define the conditions and datasets to check in.\n\nderive_var_merged_ef_msrc(\n  adsl,\n  flag_events = list(\n    flag_event(\n      dataset_name = \"cm\",\n      condition = CMCAT == \"ANTI-CANCER\"\n    ),\n    flag_event(\n      dataset_name = \"pr\"\n    )\n  ),\n  source_datasets = list(cm = cm, pr = pr),\n  by_vars = exprs(USUBJID),\n  new_var = CANCTRFL\n)\n\n# A tibble: 4 × 2\n  USUBJID CANCTRFL\n  &lt;chr&gt;   &lt;chr&gt;   \n1 1       Y       \n2 2       Y       \n3 3       Y       \n4 4       &lt;NA&gt;    \n\n\nLet’s go! We searched over multiple datasets, CM and PR, with multiple conditions and appended a new variable CANCTRFL to ADSL setting to \"Y\" if those conditions were met."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_computed",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_computed",
    "title": "admiral 1.0.0",
    "section": "derive_vars_computed()",
    "text": "derive_vars_computed()\nThis function is very similar to   derive_param_computed() , but instead of adding observations we are going to add variable(s). Very handy when wanting to add some additional variables to ADSL, e.g. baseline variables.\nLet’s make some dummy data for an ADSL and ADVS. Our goal is to derive a BMIBL variable pulled from ADVS and append to ADSL.\n\nadsl &lt;- tribble(\n  ~STUDYID,   ~USUBJID, ~AGE,   ~AGEU,\n  \"PILOT01\", \"01-1302\",   61, \"YEARS\",\n  \"PILOT01\", \"17-1344\",   64, \"YEARS\"\n)\n\nadvs &lt;- tribble(\n  ~STUDYID, ~USUBJID, ~PARAMCD, ~PARAM, ~VISIT, ~AVAL, ~AVALU, ~ABLFL,\n  \"PILOT01\", \"01-1302\", \"HEIGHT\", \"Height (cm)\", \"SCREENING\", 177.8, \"cm\", \"Y\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"SCREENING\", 81.19, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"BASELINE\", 82.1, \"kg\", \"Y\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 2\", 81.19, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 4\", 82.56, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 6\", 80.74, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"HEIGHT\", \"Height (cm)\", \"SCREENING\", 163.5, \"cm\", \"Y\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"SCREENING\", 58.06, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"BASELINE\", 58.06, \"kg\", \"Y\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 2\", 58.97, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 4\", 57.97, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 6\", 58.97, \"kg\", \"N\"\n)\n\nTake a look at how we use new_vars and filter_add. We use a function inside of new_vars to help us calculate the BMI while using the filter_add argument to only look at baseline records for the calculation.\n\nderive_vars_computed(\n  dataset = adsl,\n  dataset_add = advs,\n  by_vars = exprs(STUDYID, USUBJID),\n  parameters = c(\"WEIGHT\"),\n  constant_by_vars = exprs(STUDYID, USUBJID),\n  constant_parameters = c(\"HEIGHT\"),\n  new_vars = exprs(BMIBL = compute_bmi(height = AVAL.HEIGHT, weight = AVAL.WEIGHT)),\n  filter_add = ABLFL == \"Y\"\n)\n\n# A tibble: 2 × 5\n  STUDYID USUBJID   AGE AGEU  BMIBL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 PILOT01 01-1302    61 YEARS  26.0\n2 PILOT01 17-1344    64 YEARS  21.7\n\n\nAlright! Simple enough. We just took records from ADVSto help us calculate the BMI at baseline using this function and appended our new variable to ADSL."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#argument-alignment",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#argument-alignment",
    "title": "admiral 1.0.0",
    "section": "Argument Alignment",
    "text": "Argument Alignment\nA huge push was made for 1.0.0 to help align our arguments across all of &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions. What does this mean? We identified arguments in functions where the argument did the same things but was slightly named differently. For 1.0.0, we really want users to have a solid API for &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions.\nLet’s take a peak at the function   consolidate_metadata()  to even better understand our goal here.\nconsolidate_metadata(\n  datasets,\n  key_vars,\n  source_var = SOURCE,\n  check_vars = \"warning\",\n  check_keys,\n  check_type = \"error\"\n)\nIn previous versions of   {admiral}  the   consolidate_metadata()  function had the argument check_keys, which helps to check uniqueness. Other functions had a similar argument, but were called check_unique. Therefore, to better align our common API for   {admiral}  functions we decided to rename the check_keys argument to check_unique. You can follow the discussion around this renaming effort in this GitHub Issue.\n\n\n\n\n\n\n\n\n\nThe argument has a deprecated tag in the function documentation and will issue a warning to users. There was quite a bit of renaming of arguments for 1.0.0 so there are quite a few of these tags in our documentation. In subsequent releases, these arguments will be removed. Please see the changelog if you would like to explore other functions that had arguments renamed. The issues are linked to each rename so you can follow along with the discussions!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#bug-fixes",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#bug-fixes",
    "title": "admiral 1.0.0",
    "section": "Bug Fixes",
    "text": "Bug Fixes\nWe love fixing bugs and take them incredibly seriously - especially when identified by members from the community.\nIf you find a pesky bug, please fill out a Bug Report on our Issues Tab.\nEach bug fixed by our development team is documented in our changelog with the Issue linked.\n\n\n\n\n\n\n\n\n\nFor example, if you click through the issue for derive_extreme_event() that identified a problem where the condition was ignored if the mode argument was used, you can see the Bug Report along with a reproducible example. You can also see the Pull Request for the exact code changes that are addressing this bug linked in the Issue! Way cool!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiraldiscovery",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiraldiscovery",
    "title": "admiral 1.0.0",
    "section": "admiraldiscovery",
    "text": "admiraldiscovery\nThis is a dedicated website that lists out in a tabular format standard ADaM datasets and their common variables with corresponding &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions that could be used to create the variables. Very handy when you just want to get some starter code on deriving EOSDT or TRTSDT!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiral-cheat-sheet",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiral-cheat-sheet",
    "title": "admiral 1.0.0",
    "section": "admiral Cheat Sheet",
    "text": "admiral Cheat Sheet\nInspired by other R package cheat sheets! We try and surface commonly needed functions for doing ADaM derivations with simple tables to show how the data is transforming."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#way-back-machine",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#way-back-machine",
    "title": "admiral 1.0.0",
    "section": "Way Back Machine",
    "text": "Way Back Machine\nStudies can last a long time. Adopting R as your primary analysis language for your study can introduce certain risks around package dependencies. Fixing those dependencies to certain package versions can help mitigate those risks. Unfortunately, as package websites are updated those helpful documents, examples and vignettes can be lost as the version changes. Do not lose heart &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; users. If you decided to fix to a certain version of &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt;, we have you covered with our Way Back Machine that allows you to change the website documentation back to the version you are using."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#last-updated",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#last-updated",
    "title": "admiral 1.0.0",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:25.449383"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#details",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#details",
    "title": "admiral 1.0.0",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-08-20_teal_submission_.../teal_submission_for__posit__shiny__contest_2024.html#last-updated",
    "href": "posts/2024-08-20_teal_submission_.../teal_submission_for__posit__shiny__contest_2024.html#last-updated",
    "title": "teal and Posit Shiny Contest 2024",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:20.433661"
  },
  {
    "objectID": "posts/2024-08-20_teal_submission_.../teal_submission_for__posit__shiny__contest_2024.html#details",
    "href": "posts/2024-08-20_teal_submission_.../teal_submission_for__posit__shiny__contest_2024.html#details",
    "title": "teal and Posit Shiny Contest 2024",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html",
    "title": "PK Examples",
    "section": "",
    "text": "A new pharmaverse examples website has some exciting new features to explore.\nOne of these is the ability to launch Posit Cloud to explore the example code and make your own modifications. This interactive Posit Cloud environment is preconfigured with all required package installations. Click here: Launch Posit Cloud to explore the examples code.\nThis sample code here is based on the Population PK Analysis Data (ADPPK) model which follows the recently released CDISC Implementation Guide.\nPopulation PK models generally make use of nonlinear mixed effects models that require numeric variables. The data used in the models will include both dosing and concentration records, relative time variables, and numeric covariate variables. For more details see the {admiral} vignette.\n\n\nFirst we will load the packages required for our project. We will use {admiral} for the creation of analysis data. {admiral} requires {dplyr}, {lubridate} and {stringr}. We will use {metacore} and {metatools} to store and manipulate metadata from our specifications. We will use {xportr} to perform checks on the final data and export to a transport file.\nThe source SDTM data will come from the CDISC pilot study data stored in {pharmaversesdtm} and the ADaM ADSL data will come from {pharmaverseadam}.\n\n# Load Packages\nlibrary(admiral)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(metacore)\nlibrary(metatools)\nlibrary(xportr)\nlibrary(readr)\nlibrary(pharmaversesdtm)\nlibrary(pharmaverseadam)\n\n\n\n\nWe have saved our specifications in an Excel file and will load them into {metacore} with the metacore::spec_to_metacore() function.\n\n# ---- Load Specs for Metacore ----\nmetacore &lt;- spec_to_metacore(\"pk_spec.xlsx\") %&gt;%\n  select_dataset(\"ADPPK\")\n\n\n\n\nWe will load our SDTM data from {pharmaversesdtm}. The main components of the Population PK will be exposure data from EX and pharmacokinetic concentration data from PC. Here we will use ADSL from {pharmaverseadam} for baseline characteristics and we will derive additional baselines from vital signs VS and laboratory data LB.\n\n# ---- Load source datasets ----\n# Load PC, EX, VS, LB and ADSL\ndata(\"pc\")\ndata(\"ex\")\ndata(\"vs\")\ndata(\"lb\")\ndata(\"adsl\")\n\nex &lt;- convert_blanks_to_na(ex)\npc &lt;- convert_blanks_to_na(pc)\nvs &lt;- convert_blanks_to_na(vs)\nlb &lt;- convert_blanks_to_na(lb)\n\n\n\n\nIn this step we will create our numeric covariates using the metatools::create_var_from_codelist() function.\n\n#---- Derive Covariates ----\n# Include numeric values for STUDYIDN, USUBJIDN, SEXN, RACEN etc.\n\ncovar &lt;- adsl %&gt;%\n  create_var_from_codelist(metacore, input_var = STUDYID, out_var = STUDYIDN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SEX, out_var = SEXN) %&gt;%\n  create_var_from_codelist(metacore, input_var = RACE, out_var = RACEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ETHNIC, out_var = AETHNIC) %&gt;%\n  create_var_from_codelist(metacore, input_var = AETHNIC, out_var = AETHNICN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORT) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORTC) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYN) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYL) %&gt;%\n  mutate(\n    STUDYIDN = as.numeric(word(USUBJID, 1, sep = fixed(\"-\"))),\n    SITEIDN = as.numeric(word(USUBJID, 2, sep = fixed(\"-\"))),\n    USUBJIDN = as.numeric(word(USUBJID, 3, sep = fixed(\"-\"))),\n    SUBJIDN = as.numeric(SUBJID),\n    ROUTE = unique(ex$EXROUTE),\n    FORM = unique(ex$EXDOSFRM),\n    REGION1 = COUNTRY,\n    REGION1N = COUNTRYN,\n    SUBJTYPC = \"Volunteer\",\n  ) %&gt;%\n  create_var_from_codelist(metacore, input_var = FORM, out_var = FORMN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ROUTE, out_var = ROUTEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SUBJTYPC, out_var = SUBJTYP)\n\n\n\nNext we add additional baselines from vital signs and laboratory data. Several common variables are computed using some of the built in functions in {admiral}.\n\nlabsbl &lt;- lb %&gt;%\n  filter(LBBLFL == \"Y\" & LBTESTCD %in% c(\"CREAT\", \"ALT\", \"AST\", \"BILI\")) %&gt;%\n  mutate(LBTESTCDB = paste0(LBTESTCD, \"BL\")) %&gt;%\n  select(STUDYID, USUBJID, LBTESTCDB, LBSTRESN)\n\ncovar_vslb &lt;- covar %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"HEIGHT\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(HTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"WEIGHT\" & VSBLFL == \"Y\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(WTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_transposed(\n    dataset_merge = labsbl,\n    by_vars = exprs(STUDYID, USUBJID),\n    key_var = LBTESTCDB,\n    value_var = LBSTRESN\n  ) %&gt;%\n  mutate(\n    BMIBL = compute_bmi(height = HTBL, weight = WTBL),\n    BSABL = compute_bsa(\n      height = HTBL,\n      weight = HTBL,\n      method = \"Mosteller\"\n    ),\n    CRCLBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CRCL\"\n    ),\n    EGFRBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CKD-EPI\"\n    )\n  ) %&gt;%\n  rename(TBILBL = BILIBL)\n\nThis covariate section of the code will be combined with the dosing and observation records from EX and PC.\nThe rest of the code can be seen on the pharmaverse examples website or in the Posit Cloud environment.\nHappy exploring!"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#first-load-packages",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#first-load-packages",
    "title": "PK Examples",
    "section": "",
    "text": "First we will load the packages required for our project. We will use {admiral} for the creation of analysis data. {admiral} requires {dplyr}, {lubridate} and {stringr}. We will use {metacore} and {metatools} to store and manipulate metadata from our specifications. We will use {xportr} to perform checks on the final data and export to a transport file.\nThe source SDTM data will come from the CDISC pilot study data stored in {pharmaversesdtm} and the ADaM ADSL data will come from {pharmaverseadam}.\n\n# Load Packages\nlibrary(admiral)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(metacore)\nlibrary(metatools)\nlibrary(xportr)\nlibrary(readr)\nlibrary(pharmaversesdtm)\nlibrary(pharmaverseadam)"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#next-load-specifications-for-metacore",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#next-load-specifications-for-metacore",
    "title": "PK Examples",
    "section": "",
    "text": "We have saved our specifications in an Excel file and will load them into {metacore} with the metacore::spec_to_metacore() function.\n\n# ---- Load Specs for Metacore ----\nmetacore &lt;- spec_to_metacore(\"pk_spec.xlsx\") %&gt;%\n  select_dataset(\"ADPPK\")"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#load-source-datasets",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#load-source-datasets",
    "title": "PK Examples",
    "section": "",
    "text": "We will load our SDTM data from {pharmaversesdtm}. The main components of the Population PK will be exposure data from EX and pharmacokinetic concentration data from PC. Here we will use ADSL from {pharmaverseadam} for baseline characteristics and we will derive additional baselines from vital signs VS and laboratory data LB.\n\n# ---- Load source datasets ----\n# Load PC, EX, VS, LB and ADSL\ndata(\"pc\")\ndata(\"ex\")\ndata(\"vs\")\ndata(\"lb\")\ndata(\"adsl\")\n\nex &lt;- convert_blanks_to_na(ex)\npc &lt;- convert_blanks_to_na(pc)\nvs &lt;- convert_blanks_to_na(vs)\nlb &lt;- convert_blanks_to_na(lb)"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#derive-covariates-using-metatools",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#derive-covariates-using-metatools",
    "title": "PK Examples",
    "section": "",
    "text": "In this step we will create our numeric covariates using the metatools::create_var_from_codelist() function.\n\n#---- Derive Covariates ----\n# Include numeric values for STUDYIDN, USUBJIDN, SEXN, RACEN etc.\n\ncovar &lt;- adsl %&gt;%\n  create_var_from_codelist(metacore, input_var = STUDYID, out_var = STUDYIDN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SEX, out_var = SEXN) %&gt;%\n  create_var_from_codelist(metacore, input_var = RACE, out_var = RACEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ETHNIC, out_var = AETHNIC) %&gt;%\n  create_var_from_codelist(metacore, input_var = AETHNIC, out_var = AETHNICN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORT) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORTC) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYN) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYL) %&gt;%\n  mutate(\n    STUDYIDN = as.numeric(word(USUBJID, 1, sep = fixed(\"-\"))),\n    SITEIDN = as.numeric(word(USUBJID, 2, sep = fixed(\"-\"))),\n    USUBJIDN = as.numeric(word(USUBJID, 3, sep = fixed(\"-\"))),\n    SUBJIDN = as.numeric(SUBJID),\n    ROUTE = unique(ex$EXROUTE),\n    FORM = unique(ex$EXDOSFRM),\n    REGION1 = COUNTRY,\n    REGION1N = COUNTRYN,\n    SUBJTYPC = \"Volunteer\",\n  ) %&gt;%\n  create_var_from_codelist(metacore, input_var = FORM, out_var = FORMN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ROUTE, out_var = ROUTEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SUBJTYPC, out_var = SUBJTYP)\n\n\n\nNext we add additional baselines from vital signs and laboratory data. Several common variables are computed using some of the built in functions in {admiral}.\n\nlabsbl &lt;- lb %&gt;%\n  filter(LBBLFL == \"Y\" & LBTESTCD %in% c(\"CREAT\", \"ALT\", \"AST\", \"BILI\")) %&gt;%\n  mutate(LBTESTCDB = paste0(LBTESTCD, \"BL\")) %&gt;%\n  select(STUDYID, USUBJID, LBTESTCDB, LBSTRESN)\n\ncovar_vslb &lt;- covar %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"HEIGHT\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(HTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"WEIGHT\" & VSBLFL == \"Y\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(WTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_transposed(\n    dataset_merge = labsbl,\n    by_vars = exprs(STUDYID, USUBJID),\n    key_var = LBTESTCDB,\n    value_var = LBSTRESN\n  ) %&gt;%\n  mutate(\n    BMIBL = compute_bmi(height = HTBL, weight = WTBL),\n    BSABL = compute_bsa(\n      height = HTBL,\n      weight = HTBL,\n      method = \"Mosteller\"\n    ),\n    CRCLBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CRCL\"\n    ),\n    EGFRBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CKD-EPI\"\n    )\n  ) %&gt;%\n  rename(TBILBL = BILIBL)\n\nThis covariate section of the code will be combined with the dosing and observation records from EX and PC.\nThe rest of the code can be seen on the pharmaverse examples website or in the Posit Cloud environment.\nHappy exploring!"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#last-updated",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#last-updated",
    "title": "PK Examples",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:15.364218"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#details",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#details",
    "title": "PK Examples",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html",
    "href": "posts/2023-11-27_higher_order/higher_order.html",
    "title": "Believe in a higher order!",
    "section": "",
    "text": "Picture the following scenario:\nYou, a budding {admiral} programmer, are finding your groove chaining together modular code blocks to derive variables and parameters in a drive to construct your favorite ADaM dataset, ADAE. Suddenly you notice that one of the flags you are deriving should only use records on or after study day 1. In a moment of mild annoyance, you get to work modifying what was originally a simple call to derive_var_extreme_flag() by first subsetting ADAE to records where AESTDY &gt; 1, then deriving the flag only for the subsetted ADAE, and finally binding the two portions of ADAE back together before continuing on with your program. Miffed by this interruption, you think to yourself: “I wish there was a neater, faster way to do this in stride, that didn’t break my code modularity…”\nIf the above could never be you, then you’ll probably be alright never reading this blog post. However, if you want to learn more about the tools that {admiral} provides to make your life easier in cases like this one, then you are in the right place, since this blog post will highlight how higher order functions can solve such issues.\nA higher order function is a function that takes another function as input. By introducing these higher order functions, {admiral} intends to give the user greater power over derivations, whilst trying to negate the need for both adding additional {admiral} functions/arguments, and the user needing many separate steps.\nThe functions covered in this post are:\n\nrestrict_derivation(): Allows the user to execute a single derivation on a subset of the input dataset.\ncall_derivation(): Allows the user to call a single derivation multiple times with some arguments being fixed across iterations and others varying.\nslice_derivation(): Allows the user to split the input dataset into slices (subsets) and for each slice a single derivation is called separately. Some or all arguments of the derivation may vary depending on the slice.\n\n\n\nThe examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nFor example purpose, the ADSL dataset - which is included in {admiral} - and the SDTM datasets from {pharmaversesdtm} are used.\n\ndata(\"admiral_adsl\")\ndata(\"ae\")\ndata(\"vs\")\nadsl &lt;- admiral_adsl\nae &lt;- convert_blanks_to_na(ae)\nvs &lt;- convert_blanks_to_na(vs)\n\nThe following code creates a minimally viable ADAE dataset to be used where needed in the following examples.\n\nadae &lt;- ae %&gt;%\n  left_join(adsl, by = c(\"STUDYID\", \"USUBJID\")) %&gt;%\n  derive_vars_dt(\n    new_vars_prefix = \"AST\",\n    dtc = AESTDTC,\n    highest_imputation = \"M\"\n  ) %&gt;%\n  mutate(\n    TRTEMFL = if_else(ASTDT &gt;= TRTSDT, \"Y\", NA_character_),\n    TEMP_AESEVN = as.integer(factor(AESEV, levels = c(\"SEVERE\", \"MODERATE\", \"MILD\")))\n  )"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#required-packages",
    "href": "posts/2023-11-27_higher_order/higher_order.html#required-packages",
    "title": "Believe in a higher order!",
    "section": "",
    "text": "The examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nFor example purpose, the ADSL dataset - which is included in {admiral} - and the SDTM datasets from {pharmaversesdtm} are used.\n\ndata(\"admiral_adsl\")\ndata(\"ae\")\ndata(\"vs\")\nadsl &lt;- admiral_adsl\nae &lt;- convert_blanks_to_na(ae)\nvs &lt;- convert_blanks_to_na(vs)\n\nThe following code creates a minimally viable ADAE dataset to be used where needed in the following examples.\n\nadae &lt;- ae %&gt;%\n  left_join(adsl, by = c(\"STUDYID\", \"USUBJID\")) %&gt;%\n  derive_vars_dt(\n    new_vars_prefix = \"AST\",\n    dtc = AESTDTC,\n    highest_imputation = \"M\"\n  ) %&gt;%\n  mutate(\n    TRTEMFL = if_else(ASTDT &gt;= TRTSDT, \"Y\", NA_character_),\n    TEMP_AESEVN = as.integer(factor(AESEV, levels = c(\"SEVERE\", \"MODERATE\", \"MILD\")))\n  )"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#last-updated",
    "href": "posts/2023-11-27_higher_order/higher_order.html#last-updated",
    "title": "Believe in a higher order!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:10.398444"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#details",
    "href": "posts/2023-11-27_higher_order/higher_order.html#details",
    "title": "Believe in a higher order!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "",
    "text": "We are excited to announce that Appsilon and Sanofi will take up the remaining two open council seats on the pharmaverse council!\nAppsilon will on the council be represented by Damian Rodziewicz and Sanofi by Andre Couturier."
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#last-updated",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#last-updated",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:05.492273"
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#details",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#details",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html",
    "title": "xportr 0.4.0",
    "section": "",
    "text": "In the pharmaceuticals and healthcare industries, it is crucial to maintain a standard structure for data exchange and regulatory submissions, enter xpt datasets! xpt datasets are binary files that are typically created by SAS software, they contain structured data, including variables, labels, and metadata. In order to develop xpt formatted files in R, let’s introducing you to xportr."
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#last-updated",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#last-updated",
    "title": "xportr 0.4.0",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:00.769596"
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#details",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#details",
    "title": "xportr 0.4.0",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "",
    "text": "The use of R programming in clinical trials has not always been the most popular and obvious in the past. Despite experiencing significant growth in recent years, the adoption of R programming in clinical trials is not as widespread and evident as anticipated. Practical implementation faces obstacles due to various factors, including occasional misunderstandings, particularly in the context of validation, and a notable lack of awareness regarding its capabilities. However, despite these challenges, R is steadily establishing a growing presence within the pharmaceutical industry."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#introduction",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#introduction",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "",
    "text": "The use of R programming in clinical trials has not always been the most popular and obvious in the past. Despite experiencing significant growth in recent years, the adoption of R programming in clinical trials is not as widespread and evident as anticipated. Practical implementation faces obstacles due to various factors, including occasional misunderstandings, particularly in the context of validation, and a notable lack of awareness regarding its capabilities. However, despite these challenges, R is steadily establishing a growing presence within the pharmaceutical industry."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#opportunities-for-r-programming-in-clinical-trials",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#opportunities-for-r-programming-in-clinical-trials",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Opportunities for R Programming in Clinical Trials",
    "text": "Opportunities for R Programming in Clinical Trials\nAlthough R is versatile and applicable in various settings, it is commonly associated with scientific computing and statistics. In the context of clinical trials, where researchers aim to understand and enhance drug development and testing processes, R has become a prominent tool for analyzing the collected data. While SAS® has been a longstanding programming language for clinical trials, the industry has been exploring alternative options. There is a quest for sustainable technology and tools that can effectively address industry challenges.\nTo drive innovation, there is a need to move away from traditional, inefficient processes and tools toward solutions that are efficient, simple, easy to implement, reliable, and cost-effective. Collaboration among industry stakeholders is crucial to develop a robust technology ecosystem and establish consensus on validation and regulatory benchmarks. Equally vital is preparing the workforce with the necessary skillsets to meet future demands."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#current-usage-trends-of-r",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#current-usage-trends-of-r",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Current Usage Trends of R",
    "text": "Current Usage Trends of R\nAnalyzing the current trends of R in the pharmaceutical industry reveals that its usage still has room for growth when related to Pharma Regulatory Submissions. However, R finds extensive use in public health projects, healthcare economics, exploratory and scientific analysis, trend identification, generating plots/graphs, specific statistical analysis, and machine learning. R continues to advance steadily in clinical trials, however lacks widespread usage within the clinical space.\nThis is an area that we see gradually evolving thanks to a number of across-industry efforts such as pharmaverse."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#sas-or-r-programming-which-is-better",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#sas-or-r-programming-which-is-better",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "SAS® or R Programming: Which is Better?",
    "text": "SAS® or R Programming: Which is Better?\n\n\n\nSAS® or R?\n\n\nThe ongoing debate in the programming community revolves around whether to replace SAS® with R, use both, or explore other alternatives like Python. Instead of adopting an either-or scenario, leveraging the strengths of each programming language for specific Data Science problems is recommended, recognizing that one size does not fit all. Despite the challenges early adopters of R have faced in regulatory compliance, there have been notable successes that highlight the benefits and potential of using R in regulated industries. Early adopters of R have faced challenges, with regulatory compliance for R packages being a common issue.\nFor R to be considered for tasks related to regulatory submission, a rigorous risk assessment of R packages, feasibility analysis, and the establishment of processes for R usage through pilot projects with necessary documentation becomes imperative. We see great progress in this area through efforts such as the R Consortium R Submissions WG."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#benefits-of-using-r-programming",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#benefits-of-using-r-programming",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Benefits of Using R Programming",
    "text": "Benefits of Using R Programming\nR, as a language and environment for statistical computing and graphics, possesses characteristics that make it a potentially powerful tool for Data Analysis. With approximately 2 million users worldwide and three decades of legacy, R stands out as open-source software receiving substantial support from the community. Its availability under the GNU General Public License and extensive documentation contribute to its strength. R is versatile, running on various platforms, offering a wide array of statistical and graphical techniques, and its ease of producing publication-quality plots enhances its appeal.\nThe pharmaceutical industry has witnessed the emergence of various R packages tailored for Clinical Trial reporting. Examples include {rtables} for creating tables for reporting clinical trials, {admiral} for CDISC ADaM, {pkglite} to support eSubmission, and many others. Pharmaverse packages cater to different aspects of clinical trial data analysis, showcasing the versatility of R in this domain.\nThis article talks more about use of R in clinical trials and how this will be used by taking advantages of open source of R. The FDA emphasizes the need for fully documenting software packages used for statistical analysis in submissions. The use of R poses specific challenges related to validation, given its free and open-source nature. To address this, the R Validation Hub has released guidance documents focusing in this space.\nGiven that the cost of the R package is non-chargeable, it can also serve as a potential tool for API integration. For instance, in signal detection, R packages can prove to be valuable tools due to the intricate derivation process for EBGM in the Bayesian approach, which aims to mitigate false positive signals resulting from multiple comparisons. The computation adjusts the observed-to-expected reporting ratio for temporal trends and confounding variables such as age and sex. While both methods can estimate this, the accessibility of R as free software enables easy integration into any system as an API or for macro estimation purposes without any copyrights issue. As always though consult the license of any package being used to be sure your usage is in compliance."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#identifying-the-limitations-in-using-r-programming",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#identifying-the-limitations-in-using-r-programming",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Identifying the Limitations in Using R Programming",
    "text": "Identifying the Limitations in Using R Programming\nIt is crucial to note that software cost is essential to any company, including Pharma and Biotechs. While R and RStudio® are free and SAS® requires an annual license, using R instead of SAS® may not always lower costs. The cost of software is only one part of the equation. To be used in a highly regulated industry such as pharmaceuticals, software validation, maintenance, and support are critical, and their costs need to be considered. Although R is free and open source, it comes with a learning curve, and in short term the industry might face a shortage of experienced pharma R programmers compared to those familiar with SAS®."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#leveraging-the-right-tools-to-capture-value",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#leveraging-the-right-tools-to-capture-value",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Leveraging the Right Tools to Capture Value",
    "text": "Leveraging the Right Tools to Capture Value\nCapturing the value of R programming starts with a clear vision for its use and a systematic approach to identifying and prioritizing the needs in the industry. Clinical Data Science is evolving rapidly, and the industry actively seeks alternative solutions to unlock valuable insights from diverse datasets. Recognizing the need for innovation, collaboration, and efficient tools is crucial. Rather than viewing SAS®, R, and Python as mutually exclusive, leveraging the strengths of each for appropriate Data Science problems provides a nuanced and effective approach.\nEnsuring data quality, scientific integrity, and regulatory compliance through risk assessment frameworks, validation, and documentation are imperative in this dynamic landscape. Pharmaverse is also actively steering the pharmaceutical industry’s path by pioneering connections and advocating for R, thus exemplifying the broader trend of industries acknowledging the value and potential of open-source tools in tackling complex challenges.\n\n\n\nLeveraging the Right Tools"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#last-updated",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#last-updated",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:56.881311"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#details",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#details",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html",
    "title": "teal is now available on CRAN 🎉",
    "section": "",
    "text": "We’re thrilled to announce that teal v0.15.0 has been released on CRAN!\nThis marks a significant milestone in our journey, and we’re incredibly excited about the possibilities teal brings to the R community, particularly within clinical trial settings.\nOne of the most notable changes in this release is the introduction of teal_data class. This addition enhances how data is handled within the teal framework, paving the way for custom data modules tailored to the needs of our R users, both inside and outside the clinical trial space. With teal_data, users can expect improved efficiency and flexibility in managing their data, opening doors to innovative approaches in data analysis and visualization.\nWhile we’re enthusiastic about the advancements teal v0.15.0 brings, we have to introduce breaking changes to this version.\nBut worry not, we’ve got you covered!\nTo ease the transition, we’ve provided comprehensive guidance on migrating your applications from version 0.14.0 to 0.15.0. Check out our migration guide here, and feel free to ask any questions you may have in the discussion thread.\nAs we roll out teal v0.15.0, we’re also working diligently on releasing teal modules packages to CRAN to fully support this version. While we’re still in the process, we encourage you to dive into the latest teal release and start exploring its capabilities. To get started, make sure to install the development versions of teal.transform, teal.reporter, and any other modules you’re using.\nTo simplify the process, you can execute the following code to verify that you have the correct teal and teal modules versions:\nRest assured, we’re committed to completing the release of the teal modules as swiftly as possible to provide users with an uninterrupted experience.\nAs always, thank you for your continued support and enthusiasm for teal. We can’t wait to see the incredible ways in which teal empowers you to revolutionize your data exploration in R.\nFor further details about the release, please refer to this link.\nFeel free to explore the teal website here to learn more about the latest features."
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#last-updated",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#last-updated",
    "title": "teal is now available on CRAN 🎉",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:51.46258"
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#details",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#details",
    "title": "teal is now available on CRAN 🎉",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html",
    "href": "posts/2023-07-24_rounding/rounding.html",
    "title": "Rounding",
    "section": "",
    "text": "Both SAS and base R have the function round(), which rounds the input to the specified number of decimal places. However, they use different approaches when rounding off a 5:\n\nSAS round() rounds away from zero. So, 0.5 would be rounded to 1 whereas -1.5 to -2.\nbase R round() rounds to the nearest even. Therefore round(0.5) is 0 and round(-1.5) is -2. Note from the base R round documentation:\n\n\n\nthis is dependent on OS services and on representation error (since e.g. 0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).\n\n\n\n\nOther common methods include:\n\nRounding half up, where, as the name suggests, a 5 always gets rounded up. In base R, there are no functions that implement this method natively, however there are functions available in other R packages (e.g., janitor, tidytlg).\nRounding towards zero, which is the opposite of rounding away from zero.\n\nAs the list above shows, there are many options to choose from when looking to round a number. In the table below, you can find examples of some of these applied to the number 1.45.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nExample: 1.45\n1.5\n(round to 1 decimal place)\n1.4\n(round to 1 decimal place)\n2\n1\n1\n\n\n\nHere are the corresponding ways to implement these methods in SAS and R.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nSAS\nround()\nrounde()\nceil()\nfloor()\nint()\n\n\nR\n\njanitor::round_half_up()\n\n\ntidytlg::roundSAS()\n\n\n\n\nbase::round()\n\n\nbase::ceiling()\n\n\nbase::floor()\n\n\nbase::trunc()\n\n\n\n\nThis table is summarized from links below, where more detailed discussions can be found -\n\nTwo SAS blogs about round-to-even and rounding-up-rounding-down\nR documentation: Base R Round, janitor::round_half_up(), tidytlg::roundSAS()\nCAMIS (Comparing Analysis Method Implementations in Software): A cross-industry initiative to document discrepant results between software. Rounding is one of the comparisons, and there are much more on this page!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#rounding-methods",
    "href": "posts/2023-07-24_rounding/rounding.html#rounding-methods",
    "title": "Rounding",
    "section": "",
    "text": "Both SAS and base R have the function round(), which rounds the input to the specified number of decimal places. However, they use different approaches when rounding off a 5:\n\nSAS round() rounds away from zero. So, 0.5 would be rounded to 1 whereas -1.5 to -2.\nbase R round() rounds to the nearest even. Therefore round(0.5) is 0 and round(-1.5) is -2. Note from the base R round documentation:\n\n\n\nthis is dependent on OS services and on representation error (since e.g. 0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).\n\n\n\n\nOther common methods include:\n\nRounding half up, where, as the name suggests, a 5 always gets rounded up. In base R, there are no functions that implement this method natively, however there are functions available in other R packages (e.g., janitor, tidytlg).\nRounding towards zero, which is the opposite of rounding away from zero.\n\nAs the list above shows, there are many options to choose from when looking to round a number. In the table below, you can find examples of some of these applied to the number 1.45.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nExample: 1.45\n1.5\n(round to 1 decimal place)\n1.4\n(round to 1 decimal place)\n2\n1\n1\n\n\n\nHere are the corresponding ways to implement these methods in SAS and R.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nSAS\nround()\nrounde()\nceil()\nfloor()\nint()\n\n\nR\n\njanitor::round_half_up()\n\n\ntidytlg::roundSAS()\n\n\n\n\nbase::round()\n\n\nbase::ceiling()\n\n\nbase::floor()\n\n\nbase::trunc()\n\n\n\n\nThis table is summarized from links below, where more detailed discussions can be found -\n\nTwo SAS blogs about round-to-even and rounding-up-rounding-down\nR documentation: Base R Round, janitor::round_half_up(), tidytlg::roundSAS()\nCAMIS (Comparing Analysis Method Implementations in Software): A cross-industry initiative to document discrepant results between software. Rounding is one of the comparisons, and there are much more on this page!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#round-half-up-in-r",
    "href": "posts/2023-07-24_rounding/rounding.html#round-half-up-in-r",
    "title": "Rounding",
    "section": "Round half up in R",
    "text": "Round half up in R\nThe motivation for having a ‘round half up’ function is clear: it’s a widely used rounding method, but there are no such options available in base R.\nThere are multiple forums that have discussed this topic, and quite a few functions already available. But which ones to choose? Are they safe options?\nThe first time I needed to round half up in R, I chose the function from a PHUSE paper and applied it to my study. It works fine for a while until I encountered the following precision issue when double programming in R for TLGs made in SAS.\n\nNumerical precision issue\nExample of rounding half up for 2436.845, with 2 decimal places:\n\n# a function that rounds half up\n# exact copy from: https://www.lexjansen.com/phuse-us/2020/ct/CT05.pdf\nut_round &lt;- function(x, n = 0) {\n  # x is the value to be rounded\n  # n is the precision of the rounding\n  scale &lt;- 10^n\n  y &lt;- trunc(x * scale + sign(x) * 0.5) / scale\n  # Return the rounded number\n  return(y)\n}\n# round half up for 2436.845, with 2 decimal places\nut_round(2436.845, 2)\n\n[1] 2436.84\n\n\nThe expected result is 2436.85, but the output rounds it down. Thanks to the community effort, there are already discussions and resolution available in a StackOverflow post -\n\nThere are numerical precision issues, e.g., round2(2436.845, 2) returns 2436.84. Changing z + 0.5 to z + 0.5 + sqrt(.Machine$double.eps) seems to work for me. – Gregor Thomas Jun 24, 2020 at 2:16\n\n\n.Machine$double.eps is a built-in constant in R that represents the smallest positive floating-point number that can be represented on the system (reference: Machine Characteristics)\nThe expression + sqrt(.Machine$double.eps) is used to add a very small value to mitigate floating-point precision issues.\nFor more information about computational precision and floating-point, see the following links -\n\nR: Why doesn’t R think these numbers are equal?\nSAS: Numerical Accuracy in SAS Software\n\n\nAfter the fix:\n\n# revised rounds half up\nut_round1 &lt;- function(x, n = 0) {\n  # x is the value to be rounded\n  # n is the precision of the rounding\n  scale &lt;- 10^n\n  y &lt;- trunc(x * scale + sign(x) * 0.5 + sqrt(.Machine$double.eps)) / scale\n  # Return the rounded number\n  return(y)\n}\n# round half up for 2436.845, with 2 decimal places\nut_round1(2436.845, 2)\n\n[1] 2436.85\n\n\n\n\nWe are not alone\nThe same issue occurred in the following functions/options as well, and has been raised by users:\n\njanitor::round_half_up(): issue was raised and fixed in v2.1.0\nTplyr: options(tplyr.IBMRounding = TRUE), issue was raised\nscrutiny::round_up_from()/round_up(): issue was raised and fixed\n... and many others!\n\n\n\nWhich ones to use?\nThe following functions have the precision issue mentioned above fixed, they all share the same logic from this StackOverflow post.\n\njanitor::round_half_up() version &gt;= 2.1.0\ntidytlg::roundSAS()\n\nthis function has two more arguments that can convert the result to character and allow a character string to indicate missing values\n\nscrutiny::round_up_from()/round_up() version &gt;= 0.2.5\n\nround_up_from() has a threshold argument for rounding up, which adds flexibility for rounding up\nround_up() rounds up from 5, which is a special case of round_up_from()\n\n\n\n\nAre they safe options?\nThose “round half up” functions do not offer the same level of precision and accuracy as the base R round function.\nFor example, let’s consider a value a that is slightly less than 1.5. If we choose round half up approach to round a to 0 decimal places, an output of 1 is expected. However, those functions yield a result of 2 because 1.5 - a is less than sqrt(.Machine$double.eps).\n\na &lt;- 1.5 - 0.5 * sqrt(.Machine$double.eps)\nut_round1(a, 0)\n\n[1] 2\n\njanitor::round_half_up(a, digits = 0)\n\n[1] 2\n\n\nThis behavior aligns the floating point number comparison functions all.equal() and dplyr::near() with default tolerance .Machine$double.eps^0.5, where 1.5 and a are treated as equal.\n\nall.equal(a, 1.5)\n\n[1] TRUE\n\ndplyr::near(a, 1.5)\n\n[1] TRUE\n\n\nWe can get the expected results from base R round as it provides greater accuracy.\n\nround(a)\n\n[1] 1\n\n\nHere is an example when base R round reaches the precision limit:\n\n# b is slightly less than 1.5\nb &lt;- 1.5 - 0.5 * .Machine$double.eps\n# 1 is expected but the result is 2\nround(b)\n\n[1] 2\n\n\nThe precision and accuracy requirements can vary depending on the application. Therefore, it is essential to be aware each function’s performance in your specific context before making a choice."
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#conclusion",
    "href": "posts/2023-07-24_rounding/rounding.html#conclusion",
    "title": "Rounding",
    "section": "Conclusion",
    "text": "Conclusion\n\nWith the differences in default behaviour across languages, you could consider your QC strategy and whether an acceptable level of fuzz in the electronic comparisons could be allowed for cases such as rounding when making comparisons between 2 codes written in different languages as long as this is documented. Alternatively you could document the exact rounding approach to be used in the SAP and then match this regardless of programming language used. - Ross Farrugia\n\nThanks Ross Farrugia, Ben Straub, Edoardo Mancini and Liming for reviewing this blog post and providing valuable feedback!\nIf you spot an issue or have different opinions, please don’t hesitate to raise them through pharmaverse/blog!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#last-updated",
    "href": "posts/2023-07-24_rounding/rounding.html#last-updated",
    "title": "Rounding",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:47.970326"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#details",
    "href": "posts/2023-07-24_rounding/rounding.html#details",
    "title": "Rounding",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html",
    "href": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html",
    "title": "New {admiral} extension packages: {admiralpeds} & more coming soon!",
    "section": "",
    "text": "📢 In the pharmaverse, we are not just adopting new tools, but also fostering a culture of collaboration, innovation and a commitment to continuous learning. We are thus thrilled to update you on the latest developments from our {admiral} family team: {admiralpeds} 0.1 is coming soon on CRAN!\n{admiralpeds} is a pediatrics extension package for {admiral}. Its first release is planned by July 2024."
  },
  {
    "objectID": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html#last-updated",
    "href": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html#last-updated",
    "title": "New {admiral} extension packages: {admiralpeds} & more coming soon!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:43.412482"
  },
  {
    "objectID": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html#details",
    "href": "posts/2024-06-17_new_admiral_ex.../new_admiral_extension_packages_admiralpeds_coming_soon.html#details",
    "title": "New {admiral} extension packages: {admiralpeds} & more coming soon!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-12-02_a_nca/a_nca.html#last-updated",
    "href": "posts/2024-12-02_a_nca/a_nca.html#last-updated",
    "title": "Introducing the aNCA Package for Automated Non-Compartmental Analysis",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:40.368476"
  },
  {
    "objectID": "posts/2024-12-02_a_nca/a_nca.html#details",
    "href": "posts/2024-12-02_a_nca/a_nca.html#details",
    "title": "Introducing the aNCA Package for Automated Non-Compartmental Analysis",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "",
    "text": "Date and time is collected in SDTM as character values using the extended ISO 8601 format yyyy-dd-mmThh:mm:ss. This universal format allows missing parts date or time - e.g. the string\"2019-10\" represents a date where the day and the time are unknown. In contrast, ADaM timing variables like ADTM (Analysis Datetime) or ADY (Analysis Relative Day) are numeric variables, which can be derived only if the date or datetime is complete.\nMost ADaM programmers have, at one point or another, encountered situations where missing dates, unexpected formats or confusing imputation functions rendered derivations of timing variables frustrating and time consuming. {admiral} aims to mitigate this (where possible!) by providing functions which automatically derive date/datetime variables for you, and fill in missing date or time parts according to well-defined imputation rules.\nIn this article, we first examine the arsenal of functions provided by{admiral} to aid in datetime imputation and timing variable derivation. We then observe everything in action through a number of selected typical examples."
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-a-partial-date-portion",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-a-partial-date-portion",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputing a Partial Date Portion",
    "text": "Imputing a Partial Date Portion\nIt is easy impute dates to the first day/month if they are partial just by using the highest_imputation argument:\n\nlibrary(admiral)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(dplyr, warn.conflicts = FALSE)\n\ndates &lt;- c(\n  \"2019-07-18T15:25:40\",\n  \"2019-07-18T15:25\",\n  \"2019-07-18T15\",\n  \"2019-07-18\",\n  \"2019-02\",\n  \"2019\",\n  \"2019\",\n  \"2019---07\",\n  \"\"\n)\n\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\"\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-01\"\n[6] \"2019-01-01\" \"2019-01-01\" \"2019-01-01\" NA          \n\n\nA simple modification using date_imputation = \"mid\" or date_imputation = \"last\" or enables the imputation to be made using the middle or last day/month instead:\n\n# Impute to last day/month if date is partial\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\",\n  date_imputation = \"last\",\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-28\"\n[6] \"2019-12-31\" \"2019-12-31\" \"2019-12-31\" NA          \n\n# Impute to mid day/month if date is partial\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\",\n  date_imputation = \"mid\"\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-15\"\n[6] \"2019-06-30\" \"2019-06-30\" \"2019-06-30\" NA          \n\n\nBut what if there exist minimum dates that the imputed date cannot exceed? Here, the min_date argument comes to the rescue:\n\nimpute_dtc_dt(\n  \"2020-12\",\n  min_dates = list(\n    ymd(\"2020-12-06\"),\n    ymd(\"2020-11-11\")\n  ),\n  highest_imputation = \"M\"\n)\n\n[1] \"2020-12-06\""
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#computing-date-imputation-flags",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#computing-date-imputation-flags",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Computing Date Imputation Flags",
    "text": "Computing Date Imputation Flags\nWhen it comes to carrying out an imputation, the twin task is to flag the type of imputation that was executed. Here, functions like compute_dtf() make this straightforward. For this function, all that needs to be done is to pass a date character date to the dtc argument, and the resulting imputed date to the dt argument. This will then return the right date imputation flag - see the examples below for some possible behaviors:\n\ncompute_dtf(dtc = \"2019-07\", dt = as.Date(\"2019-07-18\"))\n\n[1] \"D\"\n\ncompute_dtf(dtc = \"2019\", dt = as.Date(\"2019-07-18\"))\n\n[1] \"M\"\n\ncompute_dtf(dtc = \"--06-01T00:00\", dt = as.Date(\"2022-06-01\"))\n\n[1] \"Y\""
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-datetime-and-date-variable-and-imputation-flag-variables",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-datetime-and-date-variable-and-imputation-flag-variables",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Creating an Imputed Datetime and Date Variable and Imputation Flag Variables",
    "text": "Creating an Imputed Datetime and Date Variable and Imputation Flag Variables\nAs described previously, derive_vars_dtm() derives an imputed datetime variable and the corresponding date and time imputation flags. The imputed date variable can then be derived by using derive_vars_dtm_to_dt(). It is not necessary and advisable to perform the imputation for the date variable if it was already done for the datetime variable. CDISC considers the datetime and the date variable as two representations of the same date. Thus the imputation must be the same and the imputation flags are valid for both the datetime and the date variable.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\",\n    time_imputation = \"first\"\n  ) %&gt;%\n  derive_vars_dtm_to_dt(exprs(ASTDTM))\n\n\n\n\n\n\nAESTDTC\nASTDTM\nASTDTF\nASTTMF\nASTDT\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09 12:34:56\nNA\nNA\n2019-08-09\n\n\n2019-04-12\n2019-04-12 00:00:00\nNA\nH\n2019-04-12\n\n\n2010-09\n2010-09-01 00:00:00\nD\nH\n2010-09-01\n\n\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-date-variable-and-imputation-flag-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-date-variable-and-imputation-flag-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Creating an Imputed Date Variable and Imputation Flag Variable",
    "text": "Creating an Imputed Date Variable and Imputation Flag Variable\nIf an imputed date variable without a corresponding datetime variable is required, it can be derived by the derive_vars_dt() function.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dt(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\"\n  )\n\n\n\n\n\n\nAESTDTC\nASTDT\nASTDTF\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09\nNA\n\n\n2019-04-12\n2019-04-12\nNA\n\n\n2010-09\n2010-09-01\nD\n\n\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-time-without-imputing-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-time-without-imputing-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputing Time Without Imputing Date",
    "text": "Imputing Time Without Imputing Date\nIf the time should be imputed but not the date, the highest_imputation argument should be set to \"h\". This results in NA if the date is partial. As no date is imputed the date imputation flag is not created.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"h\",\n    time_imputation = \"first\"\n  )\n\n\n\n\n\n\nAESTDTC\nASTDTM\nASTTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09 12:34:56\nNA\n\n\n2019-04-12\n2019-04-12 00:00:00\nH\n\n\n2010-09\nNA\nNA\n\n\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-before-a-particular-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-before-a-particular-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Avoiding Imputed Dates Before a Particular Date",
    "text": "Avoiding Imputed Dates Before a Particular Date\nUsually an adverse event start date is imputed as the earliest date of all possible dates when filling the missing parts. The result may be a date before treatment start date. This is not desirable because the adverse event would not be considered as treatment emergent and excluded from the adverse event summaries. This can be avoided by specifying the treatment start date variable (TRTSDTM) for the min_dates argument.\nImportantly, TRTSDTM is used as imputed date only if the non missing date and time parts of AESTDTC coincide with those of TRTSDTM. Therefore 2019-10 is not imputed as 2019-11-11 12:34:56. This ensures that collected information is not changed by the imputation.\n\nae &lt;- tribble(\n  ~AESTDTC,              ~TRTSDTM,\n  \"2019-08-09T12:34:56\", ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-10\",             ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-11\",             ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-12-04\",          ymd_hms(\"2019-11-11T12:34:56\")\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\",\n    time_imputation = \"first\",\n    min_dates = exprs(TRTSDTM)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAESTDTC\nTRTSDTM\nASTDTM\nASTDTF\nASTTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-11-11 12:34:56\n2019-08-09 12:34:56\nNA\nNA\n\n\n2019-10\n2019-11-11 12:34:56\n2019-10-01 00:00:00\nD\nH\n\n\n2019-11\n2019-11-11 12:34:56\n2019-11-11 12:34:56\nD\nH\n\n\n2019-12-04\n2019-11-11 12:34:56\n2019-12-04 00:00:00\nNA\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-after-a-particular-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-after-a-particular-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Avoiding Imputed Dates After a Particular Date",
    "text": "Avoiding Imputed Dates After a Particular Date\nIf a date is imputed as the latest date of all possible dates when filling the missing parts, it should not result in dates after data cut off or death. This can be achieved by specifying the dates for the max_dates argument.\nImportantly, non missing date parts are not changed. Thus 2019-12-04 is imputed as 2019-12-04 23:59:59 although it is after the data cut off date. It may make sense to replace it by the data cut off date but this is not part of the imputation. It should be done in a separate data cleaning or data cut off step.\n\nae &lt;- tribble(\n  ~AEENDTC,              ~DTHDT,            ~DCUTDT,\n  \"2019-08-09T12:34:56\", ymd(\"2019-11-11\"), ymd(\"2019-12-02\"),\n  \"2019-11\",             ymd(\"2019-11-11\"), ymd(\"2019-12-02\"),\n  \"2019-12\",             NA,                ymd(\"2019-12-02\"),\n  \"2019-12-04\",          NA,                ymd(\"2019-12-02\")\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AEENDTC,\n    new_vars_prefix = \"AEN\",\n    highest_imputation = \"M\",\n    date_imputation = \"last\",\n    time_imputation = \"last\",\n    max_dates = exprs(DTHDT, DCUTDT)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAEENDTC\nDTHDT\nDCUTDT\nAENDTM\nAENDTF\nAENTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-11-11\n2019-12-02\n2019-08-09 12:34:56\nNA\nNA\n\n\n2019-11\n2019-11-11\n2019-12-02\n2019-11-11 23:59:59\nD\nH\n\n\n2019-12\nNA\n2019-12-02\n2019-12-02 23:59:59\nD\nH\n\n\n2019-12-04\nNA\n2019-12-02\n2019-12-04 23:59:59\nNA\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputation-without-creating-a-new-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputation-without-creating-a-new-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputation Without Creating a New Variable",
    "text": "Imputation Without Creating a New Variable\nIf imputation is required without creating a new variable the convert_dtc_to_dt() function can be called to obtain a vector of imputed dates. It can be used for example here:\n\nmh &lt;- tribble(\n  ~MHSTDTC,     ~TRTSDT,\n  \"2019-04\",    ymd(\"2019-04-15\"),\n  \"2019-04-01\", ymd(\"2019-04-15\"),\n  \"2019-05\",    ymd(\"2019-04-15\"),\n  \"2019-06-21\", ymd(\"2019-04-15\")\n) %&gt;%\n  filter(\n    convert_dtc_to_dt(\n      MHSTDTC,\n      highest_imputation = \"M\",\n      date_imputation = \"first\"\n    ) &lt; TRTSDT\n  )\n\n\n\n\n\n\nMHSTDTC\nTRTSDT\n\n\n\n\n2019-04\n2019-04-15\n\n\n2019-04-01\n2019-04-15"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#using-more-than-one-imputation-rule-for-a-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#using-more-than-one-imputation-rule-for-a-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Using More Than One Imputation Rule for a Variable",
    "text": "Using More Than One Imputation Rule for a Variable\nUsing different imputation rules depending on the observation can be done by using the higher-order function slice_derivation(), which applies a derivation function differently (by varying its arguments) in different subsections of a dataset. For example, consider this Vital Signs case where pre-dose records require a different treatment to other records:\n\nvs &lt;- tribble(\n  ~VSDTC,                ~VSTPT,\n  \"2019-08-09T12:34:56\", NA,\n  \"2019-10-12\",          \"PRE-DOSE\",\n  \"2019-11-10\",          NA,\n  \"2019-12-04\",          NA\n) %&gt;%\n  slice_derivation(\n    derivation = derive_vars_dtm,\n    args = params(\n      dtc = VSDTC,\n      new_vars_prefix = \"A\"\n    ),\n    derivation_slice(\n      filter = VSTPT == \"PRE-DOSE\",\n      args = params(time_imputation = \"first\")\n    ),\n    derivation_slice(\n      filter = TRUE,\n      args = params(time_imputation = \"last\")\n    )\n  )\n\n\n\n\n\n\nVSDTC\nVSTPT\nADTM\nATMF\n\n\n\n\n2019-08-09T12:34:56\nNA\n2019-08-09 12:34:56\nNA\n\n\n2019-11-10\nNA\n2019-11-10 23:59:59\nH\n\n\n2019-12-04\nNA\n2019-12-04 23:59:59\nH\n\n\n2019-10-12\nPRE-DOSE\n2019-10-12 00:00:00\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#last-updated",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#last-updated",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:37.065659"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#details",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#details",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html",
    "href": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html",
    "title": "roxy.shinylive - shinylive applications in roxygen documentation",
    "section": "",
    "text": "Continuing my exploration of WebR (see my previous post here), I’m happy to introduce a new tool for Shiny package developers - {roxy.shinylive}. The package is designed for anyone building their Shiny applications or modules as an R package. With just a few lines of code, you can easily embed an iframe to a Shinylive application based on the code from the “Examples” section of your documentation.\nTypically, you might have something like this:\n#' This is a super app constructor.\n#' @param ... something\n#' @return shiny app object\n#' @examples\n#' if (interactive()) {\n#'   my_super_app()\n#' }\nmy_super_app &lt;- function(...) {\n  ...\n  shiny::shinyApp(...)\n}\nNow, let’s use @examplesShinyLive tag and make small adjustments to the example code to make it work in Shinylive:\n#' @examplesShinyLive\n#' library(mypackage)\n#' interactive &lt;- function() TRUE\n#' {{ next_example }}\n#' @examples\n#' (...)\nVoilà! Now your documentation includes Shinylive app! As a result, the end-users are able to see your application in action without needing to install anything. This makes your package more accessible and closer to the end-users.\nIn addition, the package exports create_shinylive_url() function, which creates an URL based on the application code as a string. This opens up more possibilities, such as embedding application(s) in README files, vignettes or even outside of package documentation. Combined with knitr::knit_code$get(\"&lt;chunk id&gt;\") and knitr::include_url(), you can reuse other chunk(s) code to embed iframes in RMarkdown or Quarto documents.\nFor a practical example of implementation, please see the documentation of teal.modules.general or teal.modules.clinical: a function documentation or a vignette. (Please note that it might take a while for WebR to download and install all the packages)\nSpecial thanks to Sam Parmar from Pfizer - the author of the {lzstring} package, which makes encoding / decoding possible. Yet another example of cross-pharma collaboration!\nPS. Yes - it’s coming to CRAN soon.\nPS2. I’ve been also thinking about a similar solution for non-Shiny R codes. See this and this issues for more updates.\nI’d love for you to try out {roxy.shinylive} and see how it can enhance your Shiny package development workflow. Whether you have feedback, suggestions, or feature requests, please feel free to file an issue on the https://github.com/insightsengineering/roxy.shinylive, or join our pharmaverse Slack channel to stay up to date and be part of the discussion."
  },
  {
    "objectID": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html#last-updated",
    "href": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html#last-updated",
    "title": "roxy.shinylive - shinylive applications in roxygen documentation",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:32.879449"
  },
  {
    "objectID": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html#details",
    "href": "posts/2024-10-11_roxy.shinyl.../roxy.shinylive_-_shinylive_applications_in_roxygen_documentation.html#details",
    "title": "roxy.shinylive - shinylive applications in roxygen documentation",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "R/readme.html",
    "href": "R/readme.html",
    "title": "Files in this folder",
    "section": "",
    "text": "Some of these files help in creating/developing blog-posts, others are used by our CICD pipeline.\n\ncreate_blogpost.R: use this script to create a new blogpost based on our blogpost template.\nCICD.R: use this script to spellcheck and stylecheck your blogpost.\nallowed_tags.R: contains vector of allowed blog post tags\n\n\n\n\nhelp_create_blogpost.R: script containing the function(s) used by create_blogpost.R\nswitch.R: Used by CICD spellcheck workflow.\ncheck_post_tags.R: Used by Check-Post-Tags CICD workflow\nupdate_post_dates.R: Used by Update-Post-Dates CICD workflow"
  },
  {
    "objectID": "R/readme.html#development-files",
    "href": "R/readme.html#development-files",
    "title": "Files in this folder",
    "section": "",
    "text": "help_create_blogpost.R: script containing the function(s) used by create_blogpost.R\nswitch.R: Used by CICD spellcheck workflow.\ncheck_post_tags.R: Used by Check-Post-Tags CICD workflow\nupdate_post_dates.R: Used by Update-Post-Dates CICD workflow"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the pharmaverse blog!",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCouncil Member updates\n\n\n\n\n\n\nCommunity\n\n\n\nNotification of latest council member renewals and welcoming Julia!\n\n\n\n\n\nFeb 21, 2025\n\n\npharmaverse council\n\n\n\n\n\n\n\n\n\n\n\n\nCollecting all the data!\n\n\n\n\n\n\nSDTM\n\n\nADaM\n\n\nCommunity\n\n\nTechnical\n\n\n\nWhere is all the data? An intermittent attempt to continuously compile, collate, consolidate, and curate publicly available CDISC data useful for Clinical Reporting in R\n\n\n\n\n\nFeb 17, 2025\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nHello admiralmetabolic!\n\n\n\n\n\n\nADaM\n\n\nCommunity\n\n\nTechnical\n\n\n\nWe are happy to announce the release of the newest member of the {admiral} family - {admiralmetabolic}!\n\n\n\n\n\nJan 21, 2025\n\n\nAnders Askeland, Edoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\n{admiral} 1.2 is here!\n\n\n\n\n\n\nADaM\n\n\nCommunity\n\n\nTechnical\n\n\n\nWhat to expect from the 1.2 release of {admiral}? New functions, crisper documentation, upcoming plans and more!\n\n\n\n\n\nJan 17, 2025\n\n\nBen Straub, Edoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nA collaborative triumph: Re-using test data between the NEST and admiral teams\n\n\n\n\n\n\nADaM\n\n\nTLG\n\n\nCommunity\n\n\n\nThis post describes one of the first joint efforts between the NEST and admiral teams, where test data from the pharmaverseadam package was used within the NEST framework.\n\n\n\n\n\nJan 15, 2025\n\n\nJoe Zhu, Edoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a better universe with dverse\n\n\n\n\n\n\nTechnical\n\n\n\nHow to create a global search across an entire collection of R packages: A use case for the toy ‘admiralverse’\n\n\n\n\n\nDec 13, 2024\n\n\nMauro Lepore\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the aNCA Package for Automated Non-Compartmental Analysis\n\n\n\n\n\n\nShiny\n\n\nADaM\n\n\nTLG\n\n\nSubmissions\n\n\n\nA Roche x Appsilon x Human Predictions Collaboration\n\n\n\n\n\nDec 2, 2024\n\n\nErcan Suekuer, Jana Spinner, Gerardo R\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Rebuilt a Lost ECG Data Script in R\n\n\n\n\n\n\nSDTM\n\n\nTechnical\n\n\n\nDuring my Data Science placement, I faced the challenge of recreating an ECG dataset for the {pharmaversesdtm} project after the original R script was lost. I explored the existing data, identified key parameters, and experimented with R packages to replicate the dataset structure and ensure SDTM compliance. Despite challenges with ensuring accurate ECG measurements, I eventually regenerated the dataset, learning valuable lessons in problem-solving and resilience.\n\n\n\n\n\nOct 31, 2024\n\n\nVladyslav Shuliar\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing sdtm.oak\n\n\n\n\n\n\nSDTM\n\n\n\nAn EDC & Data Standards agnostic solution that enables the pharmaceutical programming community to develop SDTM datasets in R\n\n\n\n\n\nOct 24, 2024\n\n\nRammprasad Ganapathy\n\n\n\n\n\n\n\n\n\n\n\n\nThe Tension of High-Performance Computing: Reproducibility vs. Parallelization\n\n\n\n\n\n\nSubmissions\n\n\nTechnical\n\n\n\nDiscover how to manage parallel processing and ensure reproducibility in drug development using the {mirai} package and other HPC tools.\n\n\n\n\n\nOct 16, 2024\n\n\nAlexandros Kouretsis, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nroxy.shinylive - shinylive applications in roxygen documentation\n\n\n\n\n\n\nShiny\n\n\nTechnical\n\n\n\nIntroducing a roxy.shinylive - a new tool for Shiny package developers\n\n\n\n\n\nOct 11, 2024\n\n\nPawel Rucki\n\n\n\n\n\n\n\n\n\n\n\n\nUndergraduate University Statistics Report using pharmaverseadam data\n\n\n\n\n\n\nADaM\n\n\nCommunity\n\n\n\nA short journal highlighting how I was able to use {pharmaverseadam} for my university work \n\n\n\n\n\nSep 16, 2024\n\n\nSyon Parashar\n\n\n\n\n\n\n\n\n\n\n\n\nWriting my first custom CICD action for the pharmaverseblog\n\n\n\n\n\n\nTechnical\n\n\n\nI recently wrote my first custom CICD pipeline for the pharmaverseblog, in an effort to police the use of post tags within the blog. These are my learnings and experiences!\n\n\n\n\n\nSep 11, 2024\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nMeet our Diversity Champion – Laura Needleman\n\n\n\n\n\n\nCommunity\n\n\n\nOn behalf of the pharmaverse council, we’d like to introduce you to Laura Needleman – an active member of our community who has volunteered to support us as a champion around DE&I.\n\n\n\n\n\nSep 3, 2024\n\n\nLaura Needleman\n\n\n\n\n\n\n\n\n\n\n\n\nteal and Posit Shiny Contest 2024\n\n\n\n\n\n\nShiny\n\n\nCommunity\n\n\n\nIntroducing the {teal} category in the Post Shiny Contest 2024 and informing users on resources to make their teal app for the submission.\n\n\n\n\n\nAug 20, 2024\n\n\nDony Unardi, Pawel Rucki\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Top 5 pharmaverse Packages\n\n\n\n\n\n\nTechnical\n\n\nCommunity\n\n\nShiny\n\n\n\nThis blog explores the top 5 popular pharmaverse packages for clinical reporting featuring {rtables}, {admiral}, {teal}, {riskmetric}, and {tidyCDISC} for improving data analysis and to ensure compliance.\n\n\n\n\n\nAug 15, 2024\n\n\nGift Kenneth, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Groups Updates\n\n\n\n\n\n\nCommunity\n\n\n\nAn update from pharmaverse council around how packages will get included in pharmaverse in future.\n\n\n\n\n\nJul 24, 2024\n\n\npharmaverse council\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Clinical Data Dashboards with {teal} and {pharmaverseadam}\n\n\n\n\n\n\nTechnical\n\n\nShiny\n\n\nCommunity\n\n\n\nLearn how to quickly develop interactive and reproducible clinical data dashboards using {teal} and {pharmaverseadam}.\n\n\n\n\n\nJul 22, 2024\n\n\nPavel Demin, Dror Berel, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nUnix versus SAS Time\n\n\n\n\n\n\nTechnical\n\n\n\nThis blog explores SAS and R date handling differences, focusing on epoch discrepancies and data types, and provides key tips for accurate date conversions to prevent a 10-year date shift.\n\n\n\n\n\nJul 16, 2024\n\n\nCéline Piraux\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing a new Coursera course for hands-on clinical data science using R.\n\n\n\n\n\n\nCommunity\n\n\n\nReleasing our second (hands-on) Coursera course, aimed at enhancing the skills of data scientists and shedding light on the impact of R open-source software within the industry. \n\n\n\n\n\nJul 3, 2024\n\n\nJoel Laxamana, Stefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\n{admiral} 1.1.1 is here!\n\n\n\n\n\n\nADaM\n\n\n\nGet to know what awaits you with the new release of the {admiral} package, including enhanced error messaging, improved documentation and much more!\n\n\n\n\n\nJun 20, 2024\n\n\nEdoardo Mancini, Ben Straub\n\n\n\n\n\n\n\n\n\n\n\n\nNew {admiral} extension packages: {admiralpeds} & more coming soon!\n\n\n\n\n\n\nMetadata\n\n\nADaM\n\n\nCommunity\n\n\n\nWe are very excited to announce that new extension packages are expected very soon in the {admiral} family: {admiralpeds}, a pediatrics extension, and another extension (name TBC) for obesity in clinical trials\n\n\n\n\n\nJun 17, 2024\n\n\nFanny Gautier, Lina Patil\n\n\n\n\n\n\n\n\n\n\n\n\nDiversity & Inclusion in pharmaverse\n\n\n\n\n\n\nCommunity\n\n\n\nAn update from pharmaverse council regarding diversity and inclusion within our community.\n\n\n\n\n\nMay 31, 2024\n\n\npharmaverse council\n\n\n\n\n\n\n\n\n\n\n\n\nOur experience as new admiral developers, coming from a CRO\n\n\n\n\n\n\nCommunity\n\n\nConferences\n\n\n\nAs the first CRO having joined the {admiral} family, we are very excited to share with you our first steps and thoughts in this new adventure\n\n\n\n\n\nMay 29, 2024\n\n\nFanny Gautier, Lina Patil\n\n\n\n\n\n\n\n\n\n\n\n\nTLG Catalog 🤝 WebR\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nIntroducing WebR to TLG Catalog: A Game Changer for Interactive Learning\n\n\n\n\n\nMay 8, 2024\n\n\nPawel Rucki\n\n\n\n\n\n\n\n\n\n\n\n\nDe-Mystifying R Programming in Clinical Trials\n\n\n\n\n\n\nCommunity\n\n\n\nA blog highlighting the benefits/limitations of using R Programming and using the right tools to create value\n\n\n\n\n\nMay 2, 2024\n\n\nVenkatesan Balu\n\n\n\n\n\n\n\n\n\n\n\n\nAppsilon and Sanofi join the pharmaverse council!\n\n\n\n\n\n\nCommunity\n\n\n\nUpdates to the pharmaverse council\n\n\n\n\n\nApr 29, 2024\n\n\nAri Siggaard Knoph\n\n\n\n\n\n\n\n\n\n\n\n\nteal.modules.clinical v0.9.0 is now on CRAN!\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nThis package release now completes the suite of {teal} family of packages recently released to CRAN.\n\n\n\n\n\nApr 8, 2024\n\n\nLeena Khatri\n\n\n\n\n\n\n\n\n\n\n\n\nxportr 0.4.0\n\n\n\n\n\n\nADaM\n\n\nSDTM\n\n\n\nxportr is a tool tailor-made for clinical programmers, enabling them to generate CDISC-compliant XPT files for SDTM and/or ADaM. It streamlines the preparation on xpt files, ensuring they are primed for submission to clinical data validation applications or regulatory agencies.\n\n\n\n\n\nMar 29, 2024\n\n\nSadchla Mascary\n\n\n\n\n\n\n\n\n\n\n\n\nTips for First Time Contributors\n\n\n\n\n\n\nCommunity\n\n\n\nOpen source can be daunting the first time you dive in - this blog will help you get started!\n\n\n\n\n\nMar 11, 2024\n\n\nRoss Farrugia\n\n\n\n\n\n\n\n\n\n\n\n\nInside the pharmaverse\n\n\n\n\n\n\nCommunity\n\n\n\nA short blog to help the Pharmaverse community understand how Pharmaverse is governed.\n\n\n\n\n\nMar 4, 2024\n\n\nMichael Rimler\n\n\n\n\n\n\n\n\n\n\n\n\nRhino: A Step Forward in Validating Shiny Apps\n\n\n\n\n\n\nSubmissions\n\n\nCommunity\n\n\n\nIn this post, we explore the importance of validation in the pharmaceutical industry and how Rhino framework aids in ensuring reliability and accuracy of Shiny applications.\n\n\n\n\n\nMar 1, 2024\n\n\nKamil Żyła, Ege Can Taşlıçukur, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nFilter out the noise!\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nA brief exposition of the filter_* functions in {admiral} - what they do and how to use them.\n\n\n\n\n\nFeb 26, 2024\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nISCR 17th Annual Conference 2024\n\n\n\n\n\n\nConferences\n\n\n\nThis blog highlights my experience of presenting at Indian Society for Clinical Research (ISCR) 17th Annual Conference 2024.\n\n\n\n\n\nFeb 26, 2024\n\n\nPooja Kumari\n\n\n\n\n\n\n\n\n\n\n\n\nteal is now available on CRAN 🎉\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nAnnouncing the release of teal v0.15.0 on CRAN!\n\n\n\n\n\nFeb 14, 2024\n\n\nDony Unardi\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Containers and WebAssembly in Submissions to the FDA\n\n\n\n\n\n\nSubmissions\n\n\n\nIn this post, we dig into the ongoing efforts undertaken to evaluate new technologies for submissions to the Food and Drug Administration (FDA). These transformative approaches, including WebAssembly and containers, hold immense potential to transform the regulatory landscape and streamline the submission process.\n\n\n\n\n\nFeb 1, 2024\n\n\nAndré Veríssimo, Tymoteusz Makowski, Pedro Silva, Vedha Viyash, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nPK Examples\n\n\n\n\n\n\nCommunity\n\n\nADaM\n\n\nMetadata\n\n\n\nExplore PK ADaM Examples on Pharmaverse Examples Page\n\n\n\n\n\nJan 26, 2024\n\n\nJeff Dickinson\n\n\n\n\n\n\n\n\n\n\n\n\nadmiral 1.0.0\n\n\n\n\n\n\nADaM\n\n\n\n1.0.0 brings a commitment to stability, new features, a few bug fixes, argument alignment and onboarding resources!\n\n\n\n\n\nJan 4, 2024\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nEnd of Year Update from the pharmaverse Council\n\n\n\n\n\n\nCommunity\n\n\n\n2023 Was a big year - let’s talk about it!\n\n\n\n\n\nJan 4, 2024\n\n\nMike Stackhouse\n\n\n\n\n\n\n\n\n\n\n\n\nBelieve in a higher order!\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nA brief foray into the higher order functions in the {admiral} package.\n\n\n\n\n\nNov 27, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nFloating point\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nThe untold story of how admiral deals with floating points.\n\n\n\n\n\nOct 30, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing the R Submissions Pilot 2 Shiny Application using Rhino\n\n\n\n\n\n\nSubmissions\n\n\nCommunity\n\n\n\nA short blog post about a Rhino pilot submission.\n\n\n\n\n\nOct 10, 2023\n\n\nIsmael Rodriguez, Vedha Viyash, André Veríssimo, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nDate/Time Functions and Imputation in {admiral}\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nDates, times and imputation can be a frustrating facet of any programming language. Here’s how {admiral} makes all of this easy!\n\n\n\n\n\nSep 26, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nThe pharmaverse (hi)story\n\n\n\n\n\n\nCommunity\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nNicholas Eugenio\n\n\n\n\n\n\n\n\n\n\n\n\nRounding\n\n\n\n\n\n\nTechnical\n\n\n\nExploration of some commonly used rounding methods and their corresponding functions in SAS and R, with a focus on ‘round half up’ and reliable solutions for numerical precision challenges.\n\n\n\n\n\nAug 22, 2023\n\n\nKangjie Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s all relative? - Calculating Relative Days using admiral\n\n\n\n\n\n\nADaM\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nCross-Industry Open Source Package Development\n\n\n\n\n\n\nCommunity\n\n\nConferences\n\n\nADaM\n\n\n\nThis post is based on a talk given at the regional useR! conference on July 21st 2023 in Basel.\n\n\n\n\n\nJul 25, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Code Sections\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nCode sections - are you using them right?\n\n\n\n\n\nJul 14, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nBlanks and NAs\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nReading SAS datasets into R. The data is not always as it seems!\n\n\n\n\n\nJul 10, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\ncardinal\n\n\n\n\n\n\nTLG\n\n\n\nThe {cardinal} initiative is an industry collaborative effort under pharmaversewith the aspiration of building and open-sourcing a comprehensive catalog of harmonized TLGs for clinical study reporting.\n\n\n\n\n\nJul 9, 2023\n\n\nVincent Shen\n\n\n\n\n\n\n\n\n\n\n\n\nHello pharmaverse\n\n\n\n\n\n\nCommunity\n\n\n\nShort, fun and user-driven content around the pharmaverse.\n\n\n\n\n\nJun 28, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nHackathon Feedback Application\n\n\n\n\n\n\nShiny\n\n\nCommunity\n\n\nADaM\n\n\n\nGoing through the process of creating a shiny app for the admiral hackathon. The shiny app allows users to check their solutions autonomously, gives feedback, and rates their results.\n\n\n\n\n\nJun 27, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nAdmiral Hackathon 2023 Revisited\n\n\n\n\n\n\nCommunity\n\n\nADaM\n\n\n\nLet’s have a look at the Admiral Hackathon 2023 together.\n\n\n\n\n\nJun 27, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nDerive a new parameter computed from the value of other parameters\n\n\n\n\n\n\nADaM\n\n\n\nUse admiral::derive_param_computed() like a calculator to derive new parameters/rows based on existing ones\n\n\n\n\n\nJun 27, 2023\n\n\nKangjie Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "session_info.html",
    "href": "session_info.html",
    "title": "Session Info",
    "section": "",
    "text": "session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       UTC\n date     2025-02-28\n pandoc   3.1.3 @ /usr/bin/ (via rmarkdown)\n quarto   1.6.42 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package          * version  date (UTC) lib source\n admiral          * 1.2.0    2025-01-15 [1] RSPM\n admiraldev         1.2.0    2025-01-15 [1] RSPM\n admiralmetabolic * 0.1.0    2025-01-20 [1] RSPM\n admiralonco      * 1.2.0    2025-01-27 [1] RSPM\n admiralophtha    * 1.2.0    2025-01-16 [1] RSPM\n admiralpeds      * 0.2.0    2025-01-16 [1] RSPM\n admiralvaccine   * 0.4.0    2025-01-31 [1] RSPM\n assertthat         0.2.1    2019-03-21 [1] RSPM\n attempt            0.3.1    2020-05-03 [1] RSPM\n backports          1.5.0    2024-05-23 [1] RSPM\n BiocManager        1.30.25  2024-08-28 [1] RSPM\n bslib              0.9.0    2025-01-30 [1] RSPM\n cachem             1.1.0    2024-05-16 [1] RSPM\n cellranger         1.1.0    2016-07-27 [1] RSPM\n checkmate          2.3.2    2024-07-29 [1] RSPM\n cicerone           1.0.4    2021-01-10 [1] RSPM\n cli                3.6.4    2025-02-13 [1] RSPM\n colorspace         2.1-1    2024-07-26 [1] RSPM\n config             0.3.2    2023-08-30 [1] RSPM\n covr               3.6.4    2023-11-09 [1] RSPM\n cranlogs           2.1.1    2019-04-29 [1] RSPM\n crayon             1.5.3    2024-06-20 [1] RSPM\n curl               6.2.1    2025-02-19 [1] RSPM\n data.table         1.17.0   2025-02-22 [1] RSPM\n devtools           2.4.5    2022-10-11 [1] RSPM\n diffdf           * 1.1.1    2024-09-24 [1] RSPM\n digest             0.6.37   2024-08-19 [1] RSPM\n dplyr            * 1.1.4    2023-11-17 [1] RSPM\n dverse           * 0.2.0    2024-12-08 [1] RSPM\n ellipsis           0.3.2    2021-04-29 [1] RSPM\n evaluate           1.0.3    2025-01-10 [1] RSPM\n farver             2.1.2    2024-05-13 [1] RSPM\n fastmap            1.2.0    2024-05-15 [1] RSPM\n forcats          * 1.0.0    2023-01-29 [1] RSPM\n formatters       * 0.5.10   2025-01-09 [1] RSPM\n fs                 1.6.5    2024-10-30 [1] RSPM\n generics           0.1.3    2022-07-05 [1] RSPM\n GGally             2.2.1    2024-02-14 [1] RSPM\n ggcorrplot         0.1.4.1  2023-09-05 [1] RSPM\n ggplot2          * 3.5.1    2024-04-23 [1] RSPM\n ggstats            0.8.0    2025-01-07 [1] RSPM\n glue               1.8.0    2024-09-30 [1] RSPM\n golem              0.5.1    2024-08-27 [1] RSPM\n gt                 0.11.1   2024-10-04 [1] RSPM\n gtable             0.3.6    2024-10-25 [1] RSPM\n haven              2.5.4    2023-11-30 [1] RSPM\n here             * 1.0.1    2020-12-13 [1] RSPM\n hms                1.1.3    2023-03-21 [1] RSPM\n htmltools          0.5.8.1  2024-04-04 [1] RSPM\n htmlwidgets        1.6.4    2023-12-06 [1] RSPM\n httpuv             1.6.15   2024-03-26 [1] RSPM\n httr               1.4.7    2023-08-15 [1] RSPM\n IDEAFilter         0.2.0    2024-04-15 [1] RSPM\n insight            1.0.2    2025-02-06 [1] RSPM\n janitor          * 2.2.1    2024-12-22 [1] RSPM\n jquerylib          0.1.4    2021-04-26 [1] RSPM\n jsonlite         * 1.9.0    2025-02-19 [1] RSPM\n knitr              1.49     2024-11-08 [1] RSPM\n later              1.4.1    2024-11-27 [1] RSPM\n lattice            0.22-6   2024-03-20 [3] CRAN (R 4.4.2)\n lazyeval           0.2.2    2019-03-15 [1] RSPM\n lifecycle          1.0.4    2023-11-07 [1] RSPM\n link             * 2024.4.0 2024-03-11 [1] RSPM\n logger             0.4.0    2024-10-22 [1] RSPM\n lubridate        * 1.9.4    2024-12-08 [1] RSPM\n magrittr         * 2.0.3    2022-03-30 [1] RSPM\n Matrix             1.7-1    2024-10-18 [3] CRAN (R 4.4.2)\n memoise            2.0.1    2021-11-26 [1] RSPM\n metacore         * 0.1.3    2024-05-02 [1] RSPM\n metatools        * 0.1.6    2024-07-23 [1] RSPM\n mime               0.12     2021-09-28 [1] RSPM\n miniUI             0.1.1.1  2018-05-18 [1] RSPM\n mirai            * 2.1.0    2025-02-07 [1] RSPM\n munsell            0.5.1    2024-04-01 [1] RSPM\n nanonext           1.5.1    2025-02-16 [1] RSPM\n patchwork        * 1.3.0    2024-09-16 [1] RSPM\n pharmaverseadam  * 1.1.0    2024-10-25 [1] RSPM\n pharmaversesdtm  * 1.2.0    2025-01-23 [1] RSPM\n pillar             1.10.1   2025-01-07 [1] RSPM\n pkgbuild           1.4.6    2025-01-16 [1] RSPM\n pkgconfig          2.0.3    2019-09-22 [1] RSPM\n pkgload            1.4.0    2024-06-28 [1] RSPM\n plotly             4.10.4   2024-01-13 [1] RSPM\n plyr               1.8.9    2023-10-02 [1] RSPM\n profvis            0.4.0    2024-09-20 [1] RSPM\n promises           1.3.2    2024-11-28 [1] RSPM\n purrr            * 1.0.4    2025-02-05 [1] RSPM\n R6                 2.6.1    2025-02-15 [1] RSPM\n RColorBrewer       1.1-3    2022-04-03 [1] RSPM\n Rcpp               1.0.14   2025-01-12 [1] RSPM\n reactable        * 0.4.4    2023-03-12 [1] RSPM\n readr            * 2.1.5    2024-01-10 [1] RSPM\n readxl             1.4.3    2023-07-06 [1] RSPM\n remotes            2.5.0    2024-03-17 [1] RSPM\n rex                1.2.1    2021-11-26 [1] RSPM\n riskmetric       * 0.2.4    2024-01-09 [1] RSPM\n rlang              1.1.5    2025-01-17 [1] RSPM\n rmarkdown          2.29     2024-11-04 [1] RSPM\n rprojroot          2.0.4    2023-11-05 [1] RSPM\n rtables          * 0.6.11   2025-01-10 [1] RSPM\n sass               0.4.9    2024-03-15 [1] RSPM\n scales             1.3.0    2023-11-28 [1] RSPM\n sdtm.oak         * 0.1.1    2024-11-12 [1] RSPM\n sessioninfo      * 1.2.3    2025-02-05 [1] RSPM\n shiny            * 1.10.0   2024-12-14 [1] RSPM\n shinyjs            2.1.0    2021-12-23 [1] RSPM\n shinyTime          1.0.3    2022-08-19 [1] RSPM\n shinyWidgets       0.9.0    2025-02-21 [1] RSPM\n sjlabelled         1.2.0    2022-04-10 [1] RSPM\n snakecase          0.11.1   2023-08-27 [1] RSPM\n spelling         * 2.3.1    2024-10-04 [1] RSPM\n stringi            1.8.4    2024-05-06 [1] RSPM\n stringr          * 1.5.1    2023-11-14 [1] RSPM\n survival           3.7-0    2024-06-05 [3] CRAN (R 4.4.2)\n teal             * 0.16.0   2025-02-23 [1] RSPM\n teal.code        * 0.6.1    2025-02-14 [1] RSPM\n teal.data        * 0.7.0    2025-01-28 [1] RSPM\n teal.logger        0.3.2    2025-02-14 [1] RSPM\n teal.slice       * 0.6.0    2025-02-03 [1] RSPM\n tibble           * 3.2.1    2023-03-20 [1] RSPM\n tidyCDISC        * 0.2.1    2023-03-16 [1] RSPM\n tidyr            * 1.3.1    2024-01-24 [1] RSPM\n tidyselect         1.2.1    2024-03-11 [1] RSPM\n tidyverse        * 2.0.0    2023-02-22 [1] RSPM\n timechange         0.3.0    2024-01-18 [1] RSPM\n timevis            2.1.0    2022-11-03 [1] RSPM\n tippy              0.1.0    2021-01-11 [1] RSPM\n triebeard          0.4.1    2023-03-04 [1] RSPM\n tzdb               0.4.0    2023-05-12 [1] RSPM\n urlchecker         1.0.1    2021-11-30 [1] RSPM\n urltools           1.7.3    2019-04-14 [1] RSPM\n usethis            3.1.0    2024-11-26 [1] RSPM\n vctrs              0.6.5    2023-12-01 [1] RSPM\n viridisLite        0.4.2    2023-05-02 [1] RSPM\n withr              3.0.2    2024-10-28 [1] RSPM\n xfun               0.51     2025-02-19 [1] RSPM\n xml2               1.3.6    2023-12-04 [1] RSPM\n xportr           * 0.4.2    2025-01-07 [1] RSPM\n xtable             1.8-4    2019-04-21 [1] RSPM\n yaml               2.3.10   2024-07-26 [1] RSPM\n zoo                1.8-13   2025-02-22 [1] RSPM\n\n [1] /home/runner/work/_temp/Library\n [2] /opt/R/4.4.2/lib/R/site-library\n [3] /opt/R/4.4.2/lib/R/library\n * ── Packages attached to the search path.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html",
    "href": "posts/2025-02-17_data__packages/data__packages.html",
    "title": "Collecting all the data!",
    "section": "",
    "text": "The purpose of this blog is to maintain an ongoing list of publicly available data packages, data in packages or data sources that align to CDISC standards. My hope is that this could be a resource for:\nThe data presented below is just a start and is shown in order of how I found them. Feel free to get in touch with me for additions or clarifications. You can find me on pharmaverse slack by joining here. In fact, I encourage, nay implore you, to get in touch as this can’t be all the data that we have available to us!"
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#pharmaversesdtm-sdtm-test-data-for-the-pharmaverse-family-of-packages",
    "href": "posts/2025-02-17_data__packages/data__packages.html#pharmaversesdtm-sdtm-test-data-for-the-pharmaverse-family-of-packages",
    "title": "Collecting all the data!",
    "section": "pharmaversesdtm: SDTM Test Data for the Pharmaverse Family of Packages",
    "text": "pharmaversesdtm: SDTM Test Data for the Pharmaverse Family of Packages\nA set of Study Data Tabulation Model (SDTM) datasets from the Clinical Data Interchange Standards Consortium (CDISC) pilot project used for testing and developing Analysis Data Model (ADaM) datasets inside the pharmaverse family of packages. A CDISC Pilot was conducted somewhere between 2008 and 2010. This is that Pilot data but slowly brought up to current CDISC standards. There are also new datasets in the same style (same STUDYID, USUBJIDs, etc.) added by the   {admiral}  and the   {admiral}  extension package teams that provide test data for new domains or specific TAs (ophthalmology, vaccines, etc.).\nMost common SDTM datasets can be found as well as some specific disease area SDTMs that are not available in the CDISC pilot datasets.\nAvailable on CRAN. This package is actively maintained on GitHub"
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#pharmaverseadam-adam-test-data-for-the-pharmaverse-family-of-packages",
    "href": "posts/2025-02-17_data__packages/data__packages.html#pharmaverseadam-adam-test-data-for-the-pharmaverse-family-of-packages",
    "title": "Collecting all the data!",
    "section": "pharmaverseadam: ADaM Test Data for the Pharmaverse Family of Packages",
    "text": "pharmaverseadam: ADaM Test Data for the Pharmaverse Family of Packages\nA set of Analysis Data Model (ADaM) datasets constructed using the Study Data Tabulation Model (SDTM) datasets contained in the   {pharmaversesdtm}  package and the template scripts from the   {admiral}  family of packages.\nAvailable on CRAN. This package is actively maintained on GitHub"
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#admiral-adam-in-r-asset-library",
    "href": "posts/2025-02-17_data__packages/data__packages.html#admiral-adam-in-r-asset-library",
    "title": "Collecting all the data!",
    "section": "admiral: ADaM in R Asset Library",
    "text": "admiral: ADaM in R Asset Library\nA toolbox for programming Clinical Data Interchange Standards Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R. ADaM datasets are a mandatory part of any New Drug or Biologics License Application submitted to the United States Food and Drug Administration (FDA). Analysis derivations are implemented in accordance with the “Analysis Data Model Implementation Guide.\nLimited datasets like ADSL, ADLB are provided in   {admiral} , because the template scripts available in this package are used to create the ADaMs in   {pharmaverseadam} .\nAvailable on CRAN. This package is actively maintained on GitHub."
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#random.cdisc.data-create-random-adam-datasets",
    "href": "posts/2025-02-17_data__packages/data__packages.html#random.cdisc.data-create-random-adam-datasets",
    "title": "Collecting all the data!",
    "section": "random.cdisc.data: Create Random ADaM Datasets",
    "text": "random.cdisc.data: Create Random ADaM Datasets\nA set of functions to create random Analysis Data Model (ADaM) datasets and cached datasets. You can find a list of the possible random CDISC datasets generated here. ADaM dataset specifications are described by the Clinical Data Interchange Standards Consortium (CDISC) Analysis Data Model Team. These datasets are used to power the TLG Catalog, though the NEST team is actively substituting them for   {pharmaverseadam}  datasets instead - see a recent blog post about this very effort!\nAvailable on CRAN. The package is actively maintained on GitHub by the NEST team."
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#safetydata-clinical-trial-data",
    "href": "posts/2025-02-17_data__packages/data__packages.html#safetydata-clinical-trial-data",
    "title": "Collecting all the data!",
    "section": "safetyData: Clinical Trial Data",
    "text": "safetyData: Clinical Trial Data\nThe package re-formats PHUSE’s sample ADaM and SDTM datasets as an R package following R data best practices.\nPHUSE released the data under the permissive MIT license, so reuse with attribution is encouraged. The data are especially useful for prototyping new tables, listings and figures and for writing automated tests.\nBasic documentation for each data file is provided in help files (e.g. ?adam_adae). Full data specifications in the form of define.xml files can also be found at the links above (pdf for ADaM and pdf for SDTM).\nAvailable on CRAN. The package is available on GitHub."
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#nest-accelerating-clinical-reporting",
    "href": "posts/2025-02-17_data__packages/data__packages.html#nest-accelerating-clinical-reporting",
    "title": "Collecting all the data!",
    "section": "NEST: Accelerating Clinical Reporting",
    "text": "NEST: Accelerating Clinical Reporting\nNEST is a collection of open-sourced R packages, which enables faster and more efficient insights generation under clinical research settings, for both exploratory and regulatory purposes.\nThey have a wealth of data generated for documentation, demonstrations and testing. You can find all the datasets and what packages they live in here."
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#collect-all-the-data",
    "href": "posts/2025-02-17_data__packages/data__packages.html#collect-all-the-data",
    "title": "Collecting all the data!",
    "section": "Collect all the data!",
    "text": "Collect all the data!\nAs you can see the list is short! Let me know if you have sources (big and small) and we can add to this list."
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#last-updated",
    "href": "posts/2025-02-17_data__packages/data__packages.html#last-updated",
    "title": "Collecting all the data!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:31.078201"
  },
  {
    "objectID": "posts/2025-02-17_data__packages/data__packages.html#details",
    "href": "posts/2025-02-17_data__packages/data__packages.html#details",
    "title": "Collecting all the data!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-07-24_working__groups_.../working__groups__updates.html",
    "href": "posts/2024-07-24_working__groups_.../working__groups__updates.html",
    "title": "Working Groups Updates",
    "section": "",
    "text": "Hi community,\nWe wanted to share with you an update from council discussions around our pharmaverse working groups. Up until now, these have been the decision-holders of which packages are included/excluded from the pharmaverse. Although open for all to join, they ended up being quite lean and likely not representative of the true depth of this community. So we’ve decided to open up the package decisions to our full community in future via our Slack workspace, to make for a more inclusive community where everyone has a voice. This is now updated on our website – see the FAQ section on the homepage.\nWe don’t want to be overly prescriptive with how this process will work as it might evolve over time, but we’ll be openly sharing any future package applications to pharmaverse via our Slack and we welcome your input to help make the call on which to accept/decline. Any individual from our community could express support via a thumbs up on the GitHub issue or add a comment with any concerns. If the requestor is unable to resolve any concerns, then ultimately any contentious applications will be raised to the pharmaverse council to adjudicate. The final decision and rationale will then always be documented on the issue.\nMoving forwards, instead of working groups we will maintain some sub-communities for particular connected networks such as our package maintainers, or those teams powering pharmaverse-specifics such as our blog and website. Additionally, for any wider open source industry challenges we would recommend any would-be contributors towards PHUSE DVOST, given our strong and continued partnership with PHUSE.\nNote that the above does not impact the role of the pharmaverse council - all our accountabilities are still explained here. Now only we delegate the responsibility for “curation” of pharmaverse packages to our entire community.\nOn behalf of the pharmaverse council"
  },
  {
    "objectID": "posts/2024-07-24_working__groups_.../working__groups__updates.html#last-updated",
    "href": "posts/2024-07-24_working__groups_.../working__groups__updates.html#last-updated",
    "title": "Working Groups Updates",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:34.637889"
  },
  {
    "objectID": "posts/2024-07-24_working__groups_.../working__groups__updates.html#details",
    "href": "posts/2024-07-24_working__groups_.../working__groups__updates.html#details",
    "title": "Working Groups Updates",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "",
    "text": "The pharmaverse is a collaborative project where leading pharmaceutical companies and passionate individuals come together to create helpful tools for clinical reporting. By using R programming and the open-source community, the pharmaverse makes it easier to gain insights and increase transparency in the pharma industry.\nIn this post, let’s explore the top 5 pharmaverse packages (by GitHub Stars ⭐), their interesting features, and how they can be used in clinical data analysis and reporting."
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#rtables-crafting-regulatory-ready-tables",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#rtables-crafting-regulatory-ready-tables",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "{rtables}: Crafting Regulatory-Ready Tables",
    "text": "{rtables}: Crafting Regulatory-Ready Tables\n\nLet’s start our journey with {rtables}, a package contributed by Roche, which enables the creation of tables for reporting clinical trials. It offers a flexible and efficient way to generate publication-quality tables, simplifying the reporting process and ensuring consistency across different trials.\n{rtables} was primarily designed to address the needs of the pharmaceutical industry for creating regulatory-ready tables for clinical trial reporting. You can use it to generate tables to summarize patient demographics, adverse events, efficacy endpoints, and other clinical trial data.\n\nFeatures of {rtables}\nThe {rtables} package offers several features that make it a useful tool for creating complex tables:\n\n{rtables} allows you to define tables with a hierarchical, tree-like structure, enabling the creation of multi-level tables with nested rows and columns.\nEach cell in an rtable can contain any high-dimensional data structure, and you can apply custom formatting, such as rounding, alignment, and cell spans, to individual cells.\nIt supports various output formats, including HTML, ASCII, LaTeX, and PDF, allowing you to generate tables that can be easily integrated into reports, presentations, and shiny applications.\nThe values in an rtable are stored in a non-rounded state, supporting cross-checking and further analysis.\nThis package is designed to work with CDISC standards, allowing you to seamlessly integrate it into your clinical data reporting workflows.\n\n\n\nExample Code\nThe following example demonstrates how to create a basic table using the {rtables} package. We start by installing and loading the required libraries. \nNext, we create a simulated dataset representing clinical trial data. We then define a function to calculate the mean of a biomarker and use {rtables} functions to build and print the table.\n# Install rtables if it's not already installed\nif (!requireNamespace(\"rtables\", quietly = TRUE)) {\n  install.packages(\"rtables\")\n}\n\n# Load required library\nlibrary(rtables)\n\n# Create a simulated dataset\nset.seed(123)\nex_adsl &lt;- data.frame(\n  ARM = sample(c(\"Placebo\", \"Drug X\", \"Combination\"), 100, replace = TRUE),\n  BMRKR2 = sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE),\n  RACE = sample(c(\"Asian\", \"Black\", \"White\"), 100, replace = TRUE),\n  SEX = sample(c(\"Male\", \"Female\"), 100, replace = TRUE),\n  BMRKR1 = rnorm(100, mean = 5, sd = 2)\n)\n\n# Define a function to calculate the mean of a biomarker\nbiomarker_ave &lt;- function(x, ...) {\n  val &lt;- if (length(x) &gt; 0) round(mean(x), 2) else \"no data\"\n  in_rows(\"Biomarker 1 (mean)\" = rcell(val))\n}\n\n# Create the table\ntable &lt;- basic_table() |&gt;\n  split_cols_by(\"ARM\") |&gt;\n  split_cols_by(\"BMRKR2\") |&gt;\n  add_colcounts() |&gt;\n  split_rows_by(\"RACE\", split_fun = trim_levels_in_group(\"SEX\")) |&gt;\n  split_rows_by(\"SEX\") |&gt;\n  summarize_row_groups() |&gt;\n  analyze(\"BMRKR1\", biomarker_ave) |&gt;\n  build_table(ex_adsl)\n\n# Print the table\nprint(table)\nResults\n\nYou can learn more about rtables:\n\nA Not So Short Introduction to rtables \nIntroduction to {rtables}\nGitHub - insightsengineering/rtables: Reporting tables with R"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#admiral-simplifying-adam-dataset-creation",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#admiral-simplifying-adam-dataset-creation",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "{admiral}: Simplifying ADaM Dataset Creation",
    "text": "{admiral}: Simplifying ADaM Dataset Creation\n\nNext let’s take a look at the {admiral} package, initially developed through a collaboration between Roche and GSK. It provides a toolbox of reusable functions and utilities with dplyr-like syntax to prepare CDISC ADaM (Analysis Data Model) datasets. It serves as a valuable resource for statistical programmers looking to build ADaMs according to varying analysis needs while ensuring traceability and compliance with FDA standards. Many more pharma companies and CROs have contributed to the package since its inception in 2021.\n\nFeatures of {admiral}\nLet’s take a look at some features of the package: \n\n{admiral} offers modular functions for adding variables and records to datasets in a step-by-step manner. This makes it easy to adjust code by adding, removing, or modifying derivations.\nIt provides template R scripts as a starting point for creating different ADaM datasets. These templates can be generated using the use_ad_template() function.\nThe package offers a flexible and efficient way to generate ADaM datasets from source data, ensuring compliance with CDISC standards and traceability of the data transformation process.\nBy providing a consistent set of functions and utilities for ADaM dataset creation, {admiral} helps to standardize the reporting process across different clinical trials and pharmaceutical companies.\nModular design allows for extensions that focus on specific areas or needs. Three open-source packages are currently extending {admiral}: {admiralonco}, {admiraloptha} and {admiralvaccine}. All of them were created from the admiral template. Two more are in the works: {admiralpeds} and {admiralmetabolic}.\n\n\n\nExample Code\nLet’s see how to use the {admiral} package to prepare an ADaM dataset. \nWe start by loading the necessary libraries and creating a sample study dataset. We then derive the ADSL (Subject-Level Analysis Dataset) and add additional variables for further analysis.\nlibrary(admiral)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Sample study data\nstudy_data &lt;- tibble::tribble(\n  ~USUBJID, ~AGE, ~SEX, ~ARM, ~RANDDTC, ~RAND2DTC, ~VISIT, ~VISITDY, ~VSDTC, ~VSTPT, ~VSORRESU, ~VSORRES, ~PARAMCD,\n  \"01\", 34, \"M\", \"Placebo\", \"2022-12-10\", \"2023-01-15\", \"Screening\", -7, \"2023-01-08\", \"Pre-dose\", \"kg\", 80, \"WEIGHT\",\n  \"02\", 45, \"F\", \"Treatment\", \"2023-01-02\", \"2023-01-17\", \"Baseline\", 1, \"2023-01-17\", \"Pre-dose\", \"kg\", 65, \"WEIGHT\",\n  \"03\", 54, \"F\", \"\", \"2022-10-16\", \"2023-01-09\", \"Screening\", -7, \"2023-01-09\", \"Pre-dose\", \"kg\", 78, \"WEIGHT\"\n)\n\n# Function to aggregate by age group\nformat_agegr1 &lt;- function(var_input) {\n  case_when(\n    var_input &lt; 35 ~ \"&lt;35\",\n    between(var_input, 34, 50) ~ \"35-49\",\n    var_input &gt; 50 ~ \"&gt;64\",\n    TRUE ~ \"Missing\"\n  )\n}\n\n# Derive ADSL (Subject-Level Analysis Dataset)\nadsl &lt;- study_data |&gt;\n  select(USUBJID, AGE, SEX, ARM, RANDDTC, RAND2DTC) |&gt;\n  distinct() |&gt;\n  # Convert blanks strings to NA\n  convert_blanks_to_na() |&gt;\n  # admiral does not yet support aggregation function, but dplyr can be used\n  mutate(\n    AGEGR1 = format_agegr1(AGE)\n  ) |&gt;\n  # Convert from character to DATE\n  derive_vars_dt(\n    dtc = RANDDTC,\n    new_vars_prefix = \"TRTS\"\n  ) |&gt;\n  derive_vars_dt(\n    dtc = RAND2DTC,\n    new_vars_prefix = \"TRTE\"\n  ) |&gt;\n  derive_vars_duration(\n    new_var = TRTD,\n    start_date = TRTSDT,\n    end_date = TRTEDT,\n    add_one = FALSE\n  )\n\n# Display the ADSL dataset\nprint(adsl)\nResults\n\nMore on {admiral}:\n\n{admiral} Cheatsheet\nYouTube playlist of {admiral} videos\nadmiral \nGet Started • admiral \nRecap: Exploring Clinical Submissions with admiral: An R-Based ADaM Solution with Ben Straub"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#teal-interactive-data-exploration",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#teal-interactive-data-exploration",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "{teal}: Interactive Data Exploration",
    "text": "{teal}: Interactive Data Exploration\n\n{teal} is an open-source R Shiny framework developed by Roche that enables the creation of interactive data exploration applications for the pharmaceutical industry. {teal} is particularly well-suited for exploring and analyzing data from clinical trials, enabling researchers and clinicians to quickly identify trends, patterns, and insights.\n\n{teal}’s reporting functionality can be used to generate regulatory-ready tables, figures, and listings. Study teams currently use it to explore data interactively and get the code to reproduce those TLGs. In the future, we hope to use it for submission to governing bodies. It can also be used to build interactive dashboards for monitoring and analyzing adverse events in clinical trials, supporting pharmacovigilance efforts. Its modular design allows for the integration of specialized modules for the analysis and visualization of high-dimensional biomarker data.\n\nFeatures of {teal}\n\n{teal} offers a flexible filter panel that allows users to easily filter and explore their data in real-time.\nThe “Show R Code” functionality enables users to reproduce the visualizations and analyses from the application in a new R session, promoting transparency and reproducibility.\nIts reporter functionality allows users to build custom reports based on the visualizations filtered and parameterized in the app. Users can generate PDFs or PPT documents that include images and optional code.\nThe package is built on a modular architecture, providing a range of pre-built, customizable modules that can be easily integrated into applications to address specific data analysis and visualization needs.\nIt is designed to seamlessly integrate with other pharmaverse packages, such as {admiral} and {rtables}.\n\n\n\nExample\nHere’s the Patient Profile {teal} application for patient-level analysis of clinical trial data from the teal.gallery.\n\nHere’s how you can run the app yourself:\nsource(\"https://raw.github.com/insightsengineering/teal.gallery/main/_internal/utils/sourceme.R\")\n\n# Run the app\nrestore_and_run(\"patient-profile\", package_repo = \"https://insightsengineering.r-universe.dev\")\nYou can also find the deployed version of the application. \nSome Resources on {teal}\n\nGitHub - Teal Gallery (A Gallery of Exploratory Web Apps used for Analyzing Clinical Trial Data)\nSimplifying Clinical Data Dashboards with {teal} and {pharmaverseadam}\nGitHub - insightsengineering/teal: Exploratory Web Apps for Analyzing Clinical Trial Data \nShiny Gatherings #8: Teal’s Role in Pharma Innovation with Paweł Rucki"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#riskmetric-assessing-package-quality-and-risk",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#riskmetric-assessing-package-quality-and-risk",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "{riskmetric}: Assessing Package Quality and Risk",
    "text": "{riskmetric}: Assessing Package Quality and Risk\n\nThe {riskmetric} package provides a framework to quantify the “risk” of R packages by assessing various metrics. Developed by the R Validation Hub, it helps organizations evaluate the quality and suitability of R packages for validated environments. The resulting risk is parameterized by the organization, which makes the decision on how to weigh the risk from the different metrics.\n\nFeatures of {riskmetric}\n\n{riskmetric} evaluates R packages across several categories, including development best practices, code documentation, community engagement, and development sustainability.\nThe package is designed to be extensible, allowing users to define custom metrics or adapt existing ones to suit their specific requirements.\n{riskmetric} includes functions for generating detailed reports and visualizations of the package risk assessment, facilitating informed decision-making.\n\n\n\n\nExample Code\nThe following example demonstrates how to use the {riskmetric} package to evaluate the risk of selected R packages. We start by loading the necessary libraries and using {riskmetric} functions to assess and score the packages.\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(riskmetric)\n\n# Assess and score R packages\npkg_ref(c(\"riskmetric\", \"utils\", \"tools\")) %&gt;%\n  pkg_assess() %&gt;%\n  pkg_score()\n\nResults\n\nIt is closely related to {riskassessment} (the app’s main goal is to help those making “package inclusion” requests for validated GxP environments) and {riskscore} (data package for cataloging riskmetric results across public repositories). \n\nMore on Riskmetric\n\nUsing {riskassessment} for R Package Validation \nThe {riskmetric} Package \nGitHub - pharmaR/riskmetric: Metrics to evaluate the risk of R packages"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#tidycdisc-tidying-cdisc-data",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#tidycdisc-tidying-cdisc-data",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "TidyCDISC: Tidying CDISC Data",
    "text": "TidyCDISC: Tidying CDISC Data\n\n{tidyCDISC} is an open-source R package developed by Biogen that provides a set of functions for tidying and manipulating CDISC (Clinical Data Interchange Standards Consortium) datasets. It aims to simplify the process of working with CDISC data by providing an intuitive interface for data transformation tasks and ensuring consistency with the principles of tidy data.\n\nFeatures of TidyCDISC\n\n{tidyCDISC} adheres to the principles of tidy data, ensuring that CDISC datasets are structured in a consistent and easy-to-work-with format.\nThe package supports various CDISC datasets, including SDTM (Study Data Tabulation Model), ADaM (Analysis Data Model), and SEND (Standard for Exchange of Nonclinical Data).\nIt provides a set of functions for common data transformation tasks, such as converting between CDISC datasets, handling missing values, and applying CDISC-specific terminology.\nThe package includes functions for validating CDISC datasets and performing consistency checks across related datasets.\n{tidyCDISC} is designed to work seamlessly with other pharmaverse packages.\n\n\n\nExample\nHere’s a demo version of {tidyCDISC} you can try.\n\nIn the documentation, you can find more examples of applications and how to use {tidyCDISC}.\n\nMore on tidyCDISC\n\nTidyCDISC blog\ntidyCDISC YouTube channel"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#honorable-mention",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#honorable-mention",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "Honorable Mention 😀",
    "text": "Honorable Mention 😀\n\n{xportr}: Creating CDISC-Compliant XPT Files in R\n\nFinally, let’s look at {xportr}, an open-source R package developed by GSK, Atorus, and Appsilon that simplifies the process of creating CDISC-compliant XPT files directly from R. It serves as a valuable tool for clinical programmers working with ADaM or SDTM datasets. \nThis package ensures compatibility with regulatory submission requirements, providing a seamless bridge between R and traditional SAS-based workflows. \n{xportr} is designed to handle the intricacies of the XPORT format, making it easier to share data across different platforms and with regulatory authorities. This capability is crucial for teams working in environments where both R and SAS are used, facilitating smooth and compliant data exchanges.\nFeatures of {xportr}\n\n{xportr} provides functions to associate metadata information to an R data frame, apply appropriate types, lengths, labels, formats, and ordering, and then write out a CDISC-compliant XPT file.\nPerforms various checks on the datasets to ensure CDISC compliance before exporting to XPT format, including variable name formatting, character length limits, type coercion, and more.\nUses a well-defined specification file (e.g., ADaM_spec.xlsx) to apply metadata and formatting rules.\nFlexible API allows for the application of transformations individually or chained together in a pipeline.\n\nIn summary, {xportr} is a valuable tool for clinical programmers working with CDISC data, as it helps ensure regulatory compliance, data quality, and workflow efficiency when creating XPT files for clinical trials and submissions.\nMore on xportr\n\nxportr 0.4.0\nUtilities to Output CDISC SDTM/ADaM XPT Files • xportr"
  },
  {
    "objectID": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#conclusion",
    "href": "posts/2024-08-15_top_five_pharmaverse_packages/top-five-packages.html#conclusion",
    "title": "Exploring the Top 5 pharmaverse Packages",
    "section": "Conclusion",
    "text": "Conclusion\nThe pharmaverse offers a rich ecosystem of tools designed to streamline clinical research workflows, ensuring high-quality data management and reporting. By leveraging packages like {rtables}, {admiral}, {teal}, {riskmetric}, {tidyCDISC}, and {xportr}, pharmaceutical companies can enhance their data analysis capabilities, ensure regulatory compliance, and drive innovation in clinical research. Remember to give the packages that you use and value a star on GitHub. ⭐\n\nTo receive the latest updates on what’s new in the pharmaverse, subscribe to the periodic newsletter!"
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#introduction",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#introduction",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Introduction",
    "text": "Introduction\nI am thrilled to share the exciting news of the release of {teal.modules.clinical} 0.9.0 on CRAN. This significant achievement marks a major milestone for the NEST team in our open-source efforts that will make a profound impact on the entire open-source community.\nThis package release now completes the suite of {teal} family of packages recently released to CRAN (see our other blog post here!). teal is a shiny-based interactive dashboard framework for analyzing data and aims to quickly and easily allow users to create dynamic visualizations. We invite you to delve deeper into the teal family of packages, including {teal.modules.clinical} by visiting our teal website."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#accelerating-clinical-insights",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#accelerating-clinical-insights",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Accelerating clinical insights",
    "text": "Accelerating clinical insights\nDesigned to enable faster insights generation under a clinical data context, the {teal.modules.clinical} package contains a set of standard teal modules to be used with CDISC data to generate many of the common analysis displays used in clinical trial reporting. By leveraging {teal.modules.clinical}, data scientists can visualize, interact, and analyze their data effectively."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#installation",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#installation",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Installation",
    "text": "Installation\nGetting started with {teal.modules.clinical} is incredibly easy. Simply run the command install.packages(\"teal.modules.clinical\") and you’ll be able to install the package directly into your local R studio environment from CRAN. For further information about this release, and information on important breaking changes, please visit the tmc site."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#explore-the-teal-gallery-and-tlg-catalog",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#explore-the-teal-gallery-and-tlg-catalog",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Explore the Teal Gallery and TLG Catalog",
    "text": "Explore the Teal Gallery and TLG Catalog\nTo get a glimpse of the capabilities and potential applications of {teal.modules.clinical}, we encourage you to explore the Teal Gallery and TLG-Catalog. These resources showcase a huge range of examples of interactive visualizations using modules from this package, which can be reused and inspire you when building your teal-shiny app."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#acknowledgments",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#acknowledgments",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe would like to give a huge thanks to the hard work and dedication of the many developers (past and present) for making this release possible. And not to forget our wonderful users for your continued support and enthusiasm.\n\n\n\nExample {teal.modules.clinical} interactive KM-plot created by tm_g_km() function. Read more about this module in the function documentation."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#last-updated",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#last-updated",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:41.86698"
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#details",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#details",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html",
    "title": "Filter out the noise!",
    "section": "",
    "text": "Filtering and merging datasets is the bread and butter of statistical programming. Whether it’s on the way to an ADaM variable derivation, or in an effort to pull out a list of patients matching a specific condition for a TLG, or another task entirely, most steps in the statistical programming workflow feature some combination of these two tasks.\nThe {tidyverse} functions filter(), group_by(), and*_join() are a fantastic toolset for filtering and merging, and can often suffice to carry out these sorts of operations. Often, however, this will be a multi-step process, requiring more than one set of pipe (%&gt;%) chains if multiple datasets are involved. As such, the {admiral} package builds on this concept by offering a very practical toolset of utility functions, henceforth referred to altogether as filter_*(). These are wrappers of common combinations of {tidyverse} function calls that enable the ADaM programmer to carry out such operations “in stride” within their ADaM workflow - in typical {admiral} style!\nMany of the filter_*() functions feature heavily within the {admiral} codebase, but they can be very handy in their own right. You can learn more about them from:\n\nThe relevant section in the Reference page of the admiral documentation website;\nThe short visual explanations in the second page of the {admiral Cheat Sheet};\n\n\n\n\n\n\n\n…and the rest of this blog post!\n\n\n\nThe examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tibble)\n\nWe also create minimally viable ADSL, ADAE and EX datasets to be used where needed in the following examples.\n\nadsl &lt;- tribble(\n  ~USUBJID,      ~AGE, ~SEX,\n  \"01-701-1015\", 63,   \"F\",\n  \"01-701-1034\", 77,   \"F\",\n  \"01-701-1115\", 84,   \"M\",\n  \"01-701-1146\", 75,   \"F\",\n  \"01-701-1444\", 63,   \"M\"\n)\n\nadae1 &lt;- tribble(\n  ~USUBJID,      ~AEDECOD,                    ~AESEV,     ~AESTDTC,\n  \"01-701-1015\", \"DIARRHOEA\",                 \"MODERATE\", \"2014-01-09\",\n  \"01-701-1034\", \"FATIGUE\",                   \"SEVERE\",   \"2014-11-02\",\n  \"01-701-1034\", \"HEADACHE\",                  \"MILD\",     \"2014-12-01\",\n  \"01-701-1034\", \"APPLICATION SITE PRURITUS\", \"MODERATE\", \"2014-08-27\",\n  \"01-701-1115\", \"FATIGUE\",                   \"MILD\",     \"2013-01-14\",\n  \"01-701-1146\", \"FATIGUE\",                   \"MODERATE\", \"2013-06-03\",\n  \"01-701-1146\", \"ANOSMIA\",                   \"MODERATE\", \"2013-08-11\"\n)\n\nadae2 &lt;- tribble(\n  ~USUBJID,      ~ADY, ~ACOVFL,     ~ADURN,\n  \"01-701-1015\",   10,     \"N\",          1,\n  \"01-701-1015\",   21,     \"N\",         50,\n  \"01-701-1015\",   23,     \"Y\",         14,\n  \"01-701-1015\",   32,     \"N\",         31,\n  \"01-701-1015\",   42,     \"N\",         20,\n  \"01-701-1034\",   11,     \"Y\",         13,\n  \"01-701-1034\",   23,     \"N\",          2,\n  \"01-701-1146\",   13,     \"Y\",         12,\n  \"01-701-1444\",   14,     \"N\",         32,\n  \"01-701-1444\",   21,     \"N\",         41\n)\n\n\nex &lt;- tribble(\n  ~USUBJID, ~EXSEQ, ~EXDOSE, ~EXTRT,\n  \"01-701-1015\", 1, 54, \"XANO\",\n  \"01-701-1015\", 2, 54, \"XANO\",\n  \"01-701-1015\", 3, 54, \"XANO\",\n  \"01-701-1034\", 1, 54, \"XANO\",\n  \"01-701-1034\", 2, 54, \"XANO\",\n  \"01-701-1115\", 1, 0, \"PLACEBO\",\n  \"01-701-1115\", 2, 0, \"PLACEBO\",\n  \"01-701-1115\", 3, 0, \"PLACEBO\",\n  \"01-701-1146\", 1, 0, \"PLACEBO\",\n  \"01-701-1146\", 2, 0, \"PLACEBO\",\n  \"01-701-1146\", 3, 0, \"PLACEBO\",\n  \"01-701-1444\", 1, 54, \"XANO\",\n  \"01-701-1444\", 2, 54, \"XANO\"\n)"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#required-packages",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#required-packages",
    "title": "Filter out the noise!",
    "section": "",
    "text": "The examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tibble)\n\nWe also create minimally viable ADSL, ADAE and EX datasets to be used where needed in the following examples.\n\nadsl &lt;- tribble(\n  ~USUBJID,      ~AGE, ~SEX,\n  \"01-701-1015\", 63,   \"F\",\n  \"01-701-1034\", 77,   \"F\",\n  \"01-701-1115\", 84,   \"M\",\n  \"01-701-1146\", 75,   \"F\",\n  \"01-701-1444\", 63,   \"M\"\n)\n\nadae1 &lt;- tribble(\n  ~USUBJID,      ~AEDECOD,                    ~AESEV,     ~AESTDTC,\n  \"01-701-1015\", \"DIARRHOEA\",                 \"MODERATE\", \"2014-01-09\",\n  \"01-701-1034\", \"FATIGUE\",                   \"SEVERE\",   \"2014-11-02\",\n  \"01-701-1034\", \"HEADACHE\",                  \"MILD\",     \"2014-12-01\",\n  \"01-701-1034\", \"APPLICATION SITE PRURITUS\", \"MODERATE\", \"2014-08-27\",\n  \"01-701-1115\", \"FATIGUE\",                   \"MILD\",     \"2013-01-14\",\n  \"01-701-1146\", \"FATIGUE\",                   \"MODERATE\", \"2013-06-03\",\n  \"01-701-1146\", \"ANOSMIA\",                   \"MODERATE\", \"2013-08-11\"\n)\n\nadae2 &lt;- tribble(\n  ~USUBJID,      ~ADY, ~ACOVFL,     ~ADURN,\n  \"01-701-1015\",   10,     \"N\",          1,\n  \"01-701-1015\",   21,     \"N\",         50,\n  \"01-701-1015\",   23,     \"Y\",         14,\n  \"01-701-1015\",   32,     \"N\",         31,\n  \"01-701-1015\",   42,     \"N\",         20,\n  \"01-701-1034\",   11,     \"Y\",         13,\n  \"01-701-1034\",   23,     \"N\",          2,\n  \"01-701-1146\",   13,     \"Y\",         12,\n  \"01-701-1444\",   14,     \"N\",         32,\n  \"01-701-1444\",   21,     \"N\",         41\n)\n\n\nex &lt;- tribble(\n  ~USUBJID, ~EXSEQ, ~EXDOSE, ~EXTRT,\n  \"01-701-1015\", 1, 54, \"XANO\",\n  \"01-701-1015\", 2, 54, \"XANO\",\n  \"01-701-1015\", 3, 54, \"XANO\",\n  \"01-701-1034\", 1, 54, \"XANO\",\n  \"01-701-1034\", 2, 54, \"XANO\",\n  \"01-701-1115\", 1, 0, \"PLACEBO\",\n  \"01-701-1115\", 2, 0, \"PLACEBO\",\n  \"01-701-1115\", 3, 0, \"PLACEBO\",\n  \"01-701-1146\", 1, 0, \"PLACEBO\",\n  \"01-701-1146\", 2, 0, \"PLACEBO\",\n  \"01-701-1146\", 3, 0, \"PLACEBO\",\n  \"01-701-1444\", 1, 54, \"XANO\",\n  \"01-701-1444\", 2, 54, \"XANO\"\n)"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#last-updated",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#last-updated",
    "title": "Filter out the noise!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:45.917193"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#details",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#details",
    "title": "Filter out the noise!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html",
    "title": "The pharmaverse (hi)story",
    "section": "",
    "text": "The pharmaverse: from motivation to present"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#human-history-and-pharmaverse-context",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#human-history-and-pharmaverse-context",
    "title": "The pharmaverse (hi)story",
    "section": "Human history and pharmaverse context",
    "text": "Human history and pharmaverse context\nSince the Australian Aboriginal, the earliest peoples recorded to have inhabited the Earth and who have been in Australia for at least 65,000 to 80,000 years (Encyclopædia Britannica), human beings live in group. Whether to protect yourself, increase your life expectancy or simply share tasks.\nFor most aspects of life, it doesn’t make sense to think, act or work alone for two main reasons:\n\nYou will spend more energy and time;\nSomeone else may be facing (or have faced) the same situation.\n\nThe English poet John Donne used to say “No man is an island entire of itself; every man is a piece of the continent, a part of the main;”. I can’t disagree with him. And I dare say that Ari Siggaard Knoph (Novo Nordisk), Michael Rimler (GSK), Michael Stackhouse (Atorus), Ross Farrugia (Roche), and Sumesh Kalappurakal (Janssen) can’t disagree with him either. They are the founders of pharmaverse, members of its Council and kindly shared their memories of how independent companies, in mid-2020, worked together in the creation of a set of packages developed to support the clinical reporting pipeline.\nIf you are not familiar with this pipeline, the important thing to know is that, in a nutshell, pharmaceutical companies must follow a bunch of standardized procedures and formats (from Clinical Data Interchange Standards Consortium, CDISC) when submitting clinical results to Health Authorities. The focus is on this: different companies seeking the same standards for outputs.\nParaphrasing Ross Farrugia (Roche) Breaking boundaries through open-source collaboration presentation in R/Pharma 2022 and thinking of the development of a new drug, we are talking about a “post-competitive” scenario: the drug has already been discovered and the companies should “just” produce and deliver standardized results."
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#clinical-reporting-outputs",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#clinical-reporting-outputs",
    "title": "The pharmaverse (hi)story",
    "section": "Clinical reporting outputs",
    "text": "Clinical reporting outputs\nRationally, we can say that companies face the same challenges in these steps of the process. Not so intuitively, we can also say they were working in silos on that before 2018. Just as Isaac Newton and Gottfried W. Leibniz independently developed the theory of infinitesimal calculus, pharmaceutical companies were independently working on R solutions for this pipeline.\nBut on August 16 and 17 of the mentioned year above, they gathered at the first edition of R/Pharma conference to discuss R and open-source tooling for drug development (the reasons why open-source is an advantageous approach can be found in this post written by Stefan Thoma). And according to Isabela Velásquez’s article, Pharmaverse: Packages for clinical reporting workflows, one of the most popular questions in this conference was “Is the package code available or on CRAN?”.\nWell, many of them were. And not necessarily at that date, but just to mention a few: pharmaRTF and Tplyr from Atorus, r2rtf from Merck, rtables from Roche, etc. The thing is that, overall, there were almost 10000 other packages as well (today, almost 20000). And that took to another two questions:\n\nWith this overwhelming number of packages on CRAN, how to find the ones related to solving “clinical reporting problems”?\nOnce the packages were found, how to choose which one to use among those that have the same functional purpose?\n\nSo, again, companies re-started to working in silos to find those answers. But now, in collaborative silos and with common goals: create extremely useful packages to solve pharmaceutical-specific gaps once and solve them well!"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#first-partnerships",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#first-partnerships",
    "title": "The pharmaverse (hi)story",
    "section": "First partnerships",
    "text": "First partnerships\nIn 2020, Michael Stackhouse (Atorus) and Michael Rimler (GSK) talked and formed a partnership between their companies to develop a few more packages, including metacore, to read, store and manipulate metadata for ADaMs/SDTMs in a standardized object; xportr, to create submission compliant SAS transport files and perform pharma specific dataset level validation checks; and logrx (ex-timber), to build log to support reproducibility and traceability of an R script.\nAround the same time, Thomas Neitmann (currently at Denali Therapeutics, then at Roche) and Michael Rimler (GSK) discovered that both were working with ADaM in R, so Thomas Neitmann (currently at Denali Therapeutics, then at Roche), Ross Farrugia (Roche) and Michael Rimler (GSK) saw an opportunity there and GSK started their partnership with Roche to build and release admiral package.\nThe idea of working together, the sense of community, and the appetite from organizations built more and more, with incentive and priority established up into the programming heads council.\nJanssen had a huge effort in building R capabilities going on as well, by releasing tidytlg and envsetup), so eventually Michael Rimler (GSK), Michael Stackhouse (Atorus) and Ross Farrugia (Roche) formalized pharmaverse and formed the council, adding in Sumesh Kalappurakal (Janssen) and Ari Siggaard Knoph (Novo Nordisk) later joined as the fifth council member."
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#release-growth-and-developments",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#release-growth-and-developments",
    "title": "The pharmaverse (hi)story",
    "section": "Release, growth and developments",
    "text": "Release, growth and developments\nAt the end of their presentation “Closing the Gap: Creating an End to End R Package Toolkit for the Clinical Reporting Pipeline.”, in R/Pharma 2021, Ben Straub (GSK) and Eli Miller (Atorus) welcomed the community to the pharmaverse, a curated collection of packages developed by various pharmaceutical companies to support open-source clinical workflows.\nFrom the outset, the name pharmaverse was chosen so that it could be a neutral home, unrelated to any company. Also, it was established as not being a consortium, which means that founders don’t own, fund, or maintain the packages. Some individuals and companies maintain them but often allowing for community contributions and being licensed permissively so that there is always a feeling of community ownership. The focus of pharmaverse early on, and today, is on inter organization cooperation, to build an environment where, if organizations identify that they have a joint problem that they want to solve, this is the right space to work on and release it.\nPharmaverse has grown a lot, at the time of writing this post we have &gt;25 packages recommended in pharmaverse, and this has led to a partnership with PHUSE to get support from their organization and platform, and because they are eager to advance and support pharmaverse mission.\nDespite all its structure, it is impossible to say that we have a single solution for each clinical reporting analysis when it comes to pharmaverse, a single pathway is impractical. Instead, it is necessary to accept viable tools fitting different pathways into pharmaverse to direct and give people options as to what might work for them. After all, even though we live together as a community, we still have our own unique internal problems.\n\n\n\nSample of pharmaverse packages"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#last-updated",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#last-updated",
    "title": "The pharmaverse (hi)story",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:49.827613"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#details",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#details",
    "title": "The pharmaverse (hi)story",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html",
    "title": "{admiral} 1.2 is here!",
    "section": "",
    "text": "As crazy as it may sound, the   {admiral}  package is nearing its third CRAN birthday: our first release dates all the way back to 17th February 2022! Time really does fly when you’re programming ADaMs in R. The codebase has grown quite a lot! For some context, if you’d only allowed yourself to write twenty lines of code a day, starting from the 17th February 2022, by now you would have compiled the whole of the   {admiral}  codebase (excluding documentation and unit tests). And so, what better early birthday present for   {admiral}  than the new and exciting 1.2 release!\nRead through this blog post to find out more about the exciting contents of the 1.2 release, as well some of our plans for 2025."
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function",
    "title": "{admiral} 1.2 is here!",
    "section": "New function: ",
    "text": "New function: \n derive_vars_cat() \nWe wanted a function that could take in user-defined inputs for category-like variables, e.g. AVALCATy and AVALCAyN. While this is typically defined in the metadata for an ADaM, we thought having a dedicated function in   {admiral}  could help either with double programming or smaller projects that don’t have metadata fully developed. Let us know what you think!\nBelow you can expand to get a few simple examples of this new function with dummy data in action. Be sure to check out the function’s documentation on the admiral website for full understanding of   derive_vars_cat() .\n\n\n\n\n\n\nExpand for Full Code Walkthrough\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(admiral)\nlibrary(tibble)\n\n# Example 1\nadvs &lt;- tibble::tribble(\n  ~USUBJID,       ~VSTEST,  ~AVAL,\n  \"01-701-1015\", \"Height\", 147.32,\n  \"01-701-1015\", \"Weight\",  53.98,\n  \"01-701-1023\", \"Height\", 162.56,\n  \"01-701-1023\", \"Weight\",     NA,\n  \"01-701-1028\", \"Height\",     NA,\n  \"01-701-1028\", \"Weight\",     NA,\n  \"01-701-1033\", \"Height\", 175.26,\n  \"01-701-1033\", \"Weight\",  88.45\n)\n\n\n# Example 1\n# A simple defintion with multiple conditions\ndefinition2 &lt;- exprs(\n  ~VSTEST,   ~condition,  ~AVALCAT1, ~AVALCA1N,\n  \"Height\",  AVAL &gt; 160,  \"&gt;160 cm\",         1,\n  \"Height\", AVAL &lt;= 160, \"&lt;=160 cm\",         2,\n  \"Weight\",   AVAL &gt; 70,   \"&gt;70 kg\",         1,\n  \"Weight\",  AVAL &lt;= 70,  \"&lt;=70 kg\",         2\n)\n\nderive_vars_cat(\n  dataset = advs,\n  definition = definition2,\n  by_vars = exprs(VSTEST)\n)\n\n# Example 2\n# A simple defintion with slightly more complex conditions\ndefinition3 &lt;- exprs(\n  ~VSTEST,                ~condition,  ~AVALCAT1, ~AVALCA1N,\n  \"Height\",               AVAL &gt; 170,  \"&gt;170 cm\",         1,\n  \"Height\", AVAL &lt;= 170 & AVAL &gt; 160, \"&lt;=170 cm\",         2,\n  \"Height\",              AVAL &lt;= 160, \"&lt;=160 cm\",         3\n)\n\nderive_vars_cat(\n  dataset = advs,\n  definition = definition3,\n  by_vars = exprs(VSTEST)\n)\n\n# Example 3\n# Now using for MCRITyML and the MCRITyMN variables\nadlb &lt;- tibble::tribble(\n  ~USUBJID,     ~PARAM, ~AVAL, ~AVALU,  ~ANRHI,\n  \"01-701-1015\", \"ALT\",   150,  \"U/L\",      40,\n  \"01-701-1023\", \"ALT\",    70,  \"U/L\",      40,\n  \"01-701-1036\", \"ALT\",   130,  \"U/L\",      40,\n  \"01-701-1048\", \"ALT\",    30,  \"U/L\",      40,\n  \"01-701-1015\", \"AST\",    50,  \"U/L\",      35\n)\n\ndefinition_mcrit &lt;- exprs(\n  ~PARAM,                      ~condition,    ~MCRIT1ML, ~MCRIT1MN,\n  \"ALT\",                    AVAL &lt;= ANRHI,    \"&lt;=ANRHI\",         1,\n  \"ALT\", ANRHI &lt; AVAL & AVAL &lt;= 3 * ANRHI, \"&gt;1-3*ANRHI\",         2,\n  \"ALT\",                 3 * ANRHI &lt; AVAL,   \"&gt;3*ANRHI\",         3\n)\n\nadlb %&gt;%\n  derive_vars_cat(\n    definition = definition_mcrit,\n    by_vars = exprs(PARAM)\n  )"
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function-1",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function-1",
    "title": "{admiral} 1.2 is here!",
    "section": "New function: ",
    "text": "New function: \n derive_vars_crit_flag() \nThe function derives ADaM-compliant criterion flags, e.g. CRIT1FL, to facilitate subgroup analyses.\nIf a criterion flag can’t be derived with this function, the derivation is not ADaM-compliant. It helps to ensure that:\n\nthe condition of the criterion depends only on variables of the same row,\nthe CRITyFL is populated with valid values, i.e, either “Y” and NA or “Y”, “N”, and NA,\nthe CRITy variable is populated correctly.\n\nBelow you can expand to get a few simple examples of this new function with dummy data in action. Be sure to check out the function’s documentation on the   {admiral}  website for a full understanding of   derive_vars_crit_flag() .\n\n\n\n\n\n\nExpand for Full Code Walkthrough\n\n\n\n\n\n\nlibrary(admiral)\nlibrary(tibble)\nadbds &lt;- tribble(\n  ~PARAMCD, ~AVAL,\n  \"AST\",    42,\n  \"AST\",    52,\n  \"AST\",    NA_real_,\n  \"ALT\",    33,\n  \"ALT\",    51\n)\n\n# Create a criterion flag with values \"Y\" and NA\nderive_vars_crit_flag(\n  adbds,\n  condition = AVAL &gt; 50,\n  description = \"Absolute value &gt; 50\"\n)\n\n# Create criterion flag with values \"Y\", \"N\", and NA and parameter dependent\n# criterion description\nderive_vars_crit_flag(\n  adbds,\n  crit_nr = 2,\n  condition = AVAL &gt; 50,\n  description = paste(PARAMCD, \"&gt; 50\"),\n  values_yn = TRUE,\n  create_numeric_flag = TRUE\n)"
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function-transform_scale",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#new-function-transform_scale",
    "title": "{admiral} 1.2 is here!",
    "section": "New function: transform_scale()",
    "text": "New function: transform_scale()\nThe final new function we are adding is transform_scale(). Coming directly from a user request, with this function we now have functionality to linearly transform a vector of values with a certain associated range into values from another range.\n\nlibrary(admiral)\n\ntransform_range(\n  source = c(1, 4, 3, 6, 5),\n  source_range = c(1, 5),\n  target_range = c(0, 100)\n)\n\nWithin ADaM programming, this is a common task, arising for instance when transforming questionnaire values into a standardized scale (e.g. 1-100). As such, we see this function being employed principally within calls to   derive_param_computed()  during the creation of datasets such as ADQS. You can read more about this topic within our Questionnaire ADaMs vignette."
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#admiral-at-recent-conferences",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#admiral-at-recent-conferences",
    "title": "{admiral} 1.2 is here!",
    "section": "admiral at Recent Conferences",
    "text": "admiral at Recent Conferences\nThis post is also a great opportunity to showcase and publicize some of the fantastic outreach that members of the   {admiral}  community have done since our 1.1 release back in June. A couple of highlights include:\n\nAt useR 2024 in Salzburg:\n\nEdoardo Mancini and Stefan Thoma gave a presentation titled: “  {admiral}  - the   {dplyr}  of the pharmaceutical industry?” - slides here.\n\nAt R/Pharma 2024:\n\nBen Straub and Fanny Gautier led an “ADaM Workshop” using admiral, metacore/metatools and xportr to derive ADSL and ADVS - slides here, repo here.\n\nAt PHUSE EU Connect 2024 in Strasbourg:\n\nYirong Cao and Yi Yan gave a talk titled: Insights of Using   {admiralonco}  at BMS: Facilitating Oncology Efficacy ADaM Datasets - slides here, paper here.\nEdoardo Mancini spoke about Two hats, one noggin: Perspectives on working as a developer and as a user of the admiral R package for creating ADaMs - slides here, paper here).\nFederico Baratin gave his take on admiralvaccine Project JouRney - slides here, paper here).\n\n\nAs you can see,   {admiral}  is spreading like wildfire in the pharma industry, and we encourage anyone within our community to spread the word and continue making these super presentations. Please let us know if you present on   {admiral}  we would love to give you a shout out.\nFor the full archive of   {admiral} -related presentations, please visit our Presentation Archive."
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#last-updated",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#last-updated",
    "title": "{admiral} 1.2 is here!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:54.862639"
  },
  {
    "objectID": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#details",
    "href": "posts/2025-01-17_1.2_admiral.../1.2_admiral_release.html#details",
    "title": "{admiral} 1.2 is here!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html",
    "title": "Cross-Industry Open Source Package Development",
    "section": "",
    "text": "This post is based on a talk given at the regional useR! conference on July 21st 2023 in Basel. I took the opportunity to present my personal perspective on the current cross-industry package development efforts with a particular focus on the transformation of the job description of statistical programmers. As I have only recently started my position at Roche, my personal perspective is the perspective of a newcomer. I have a background in Psychology and Statistics and joined Roche in November 2022 as an intern switching to a permanent position as a statistical programmer – what they call analytical data scientist now – in April 2023. I spend about 20% of my time in such a cross-industry package development project, which was a major reason for applying for this position. In this post I would like to explain how we work in this project, and why this had such an impact on my decision to join Roche."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#context",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#context",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Context",
    "text": "Context\nMy decision was influenced by two current industry trends:\nFirst, the switch to a more language agnostic and open source approach for clinical reporting and data analysis. At the moment, R seems to be the best fitting tool for the job, but the systems used here are language agnostic in general. This is related, but does not necessarily lead to the second trend: The move toward cross-industry collaboration when developing clinical reporting software.\nAs the industry moves toward new (to them) programming languages, fit-for-purpose tools need(ed) to be developed. The realization that siloed solutions – in an area where competitiveness does not benefit patients – are simply resource hungry ways to solve the same problem in parallel provided a great argument for a shift toward collaborations.\nSuch cross-industry collaborations gave rise to the pharmaverse, a curated collection of R packages designed to solve clinical reporting in R. {admiral}, the project that I work on, is part of the pharmaverse and covers the creation of ADaM data sets (CDISC standard data). These data sets are subsequently used to produce tables, listings, and graphs and are usually part of the submission package for regulators."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#insights",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#insights",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Insights",
    "text": "Insights\nWhen creating an open source package in an industry where currently there is a lot of traction you have to move fast. No, I don’t mean: Move fast and break things. I mean: Communicate! Get people on board! We aimed {admiral} to be the package for ADaM creation even before deciding to create the package together with GSK. By being transparent about our endeavor, e.g. Thomas Neitmann (then at Roche) posting on LinkedIn, we managed to connect with Michael Rimler from GSK and soon realized that we were dealing with the exact same challenge at both companies, and that a collaborative effort would improve the final product while reducing individual efforts. A working prototype of {admiral} was to be created by GSK and Roche within six months, and would then be open sourced. In our effort to communicate openly, we informed statistical programmers from over 20 companies about the {admiral} project and invited them to try it out and provide feedback once released. In the end, we received over 500 comments from over 50 programmers.\n\n# check out the latest admiral release from CRAN:\ninstall.packages(\"admiral\")\n\nlibrary(admiral)\n\nThis was instrumental in creating a product that was optimised for general usage in the clinical reporting field and ensured that other companies would not unknowingly invest into their own solution to this challenge. Open sourcing early is particularly beneficial because this ensures from the get-go that code created is aimed at a general audience, and not company specific (perhaps by accident).\n{admiral} was created with the long term goal of having a stable and flexible solution for the clinical reporting pipeline. In that spirit, its permissive apache 2.0 licence (jointly owned by Roche and GSK) further strengthens trust into the project, namely for three reasons:\n\nJointly owned means that efforts to monetize the code-base by one company could be vetoed by the other. The permissive licence ensures that in such an unlikely case, the code-base that has been published would stay available and could always be used and improved upon by others.\nHaving this package backed by Roche and GSK ensures (as far as this is ever possible) funding for properly maintaining the packages. This is crucial, as a package is rarely finished.\nUp to now, experts from many more companies have joined {admiral} or one of its therapeutic area specific package-extensions, inspiring even more trust into its reliability.\n\nIf you would like to learn more about licenses for open source projects in the clinical reporting world please check out the recent PHUSE E2E Guidance on open source license"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#development-workflow",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#development-workflow",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Development workflow",
    "text": "Development workflow\nEvery improvement, task, or feature we want to implement on {admiral} starts as an issue on our GitHub repository. It is the centerpiece of our development workflow, along with our developer guides which describe in detail the strategies, conventions, and workflows used in development. The guides help us keep the {admiral} package internally consistent (e.g. naming conventions, function logic) but also ensure that {admiral} adjacent packages follow the same conventions and share the user interface. This is further helped by the implemented CICD pipeline which ensures styling convention and spelling (and much more).\nThe core package developers team meets once a week (twice a week before a release) to discuss progress and priorities. Here, the role of product lead (currently Edoardo Mancini at Roche) and technical lead (currently Ben Straub at GSK) is to set priorities and track the release schedule. These stand-up meetings are centered around the project-board which gives a complete overview of activities and progress. Issues are mostly self-assigned so developers can really chose what they want to work on.\n\n\n\nGitHub project board\n\n\nBy design, {admiral} is community built. Most developers working on the project are statistical programmers working on clinical reporting themselves. As an open source project, community input is highly valued, and anyone using {admiral} is encouraged to submit issues or take on issues as part of the development team. We also do occasional events to bring the statistical programmers community and the developers closer together. Just last February we organised the {admiral} hackathon which had up to 500 participants."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#impact",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#impact",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Impact",
    "text": "Impact\nFor Roche, cross-industry package development work out in their favor: They get access to software created by specialists and users from across the industry but paying only a fraction of the developmental costs. Of course, they don’t have total developmental control but they do get a seat at the table. Any gaps between the open source {admiral} package and the proprietary Roche workflow were bridged by the internal {admiralroche} package.\nThe switch towards a more language agnostic platform, and open source languages specifically, opens the door to a broad population of university graduates with diverse backgrounds. I personally would not have considered this position five years ago due to a misalignment of skills and job requirements. Working towards an industry standard open source solution will also ensure that skills learned at one company are more easily transferable to external positions, further making the position much more attractive. Access to such a broad pool of potential candidates is clearly beneficial for recruitment at Roche, but also facilitates diversity in teams which makes for a more interesting and effective work place.\nOpen source development comes with much more transparency by definition. Recognition of contributions are built in - anyone can see who did what. This recognition escapes the confines of your company as it is visible to anyone looking at the repository. Anyone can not only see at any time what is being worked on, what discussions are happening and which direction is being taken, but can also participate and contribute. Transparency also applies to errors in the code and how the team is dealing with them. In such an environment it is practically impossible to hide or cover up errors and corrections. Instead, they have to be dealt with publicly and in the open. This openness about errors also helps seeing errors as a natural occurrence that needs to be dealt with. Space for errors encourages learning and is really beneficial for growing both skills and integrity.\nAs you work on a team that spans multiple companies, traditional corporate hierarchies do not apply. Of course, there will always be a sort of hierarchy of experience or skills, but these work in your favor: You will know who to ask for help, and teams are generally very happy for contributors of any skill level. Contributions also need not be in code: Inputs into discussions and domain knowledge contributions are highly valued as well. The flip-side of working in a team without your manager oversight: They may not be directly aware of the work you do. That’s why you have to write blog posts :)\nThe possibility for statistical programmers to pivot towards developing software or writing blog-posts such as this really transforms and broadens their job description. It is this transformation that is reflected by the choice of Roche to re-brand statistical programmers as analytical data scientists. The fact that cross-industry development is being advocated for really lets programmers expand their network outside of their company.\nThe {admiral} project serves as a testament to the power of collaborative open-source development and the potential it holds for the future of work in this industry."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#last-updated",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#last-updated",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:27:58.612709"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#details",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#details",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html",
    "href": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html",
    "title": "Introducing sdtm.oak",
    "section": "",
    "text": "{sdtm.oak}  v0.1 is now available on CRAN. In this blog post, we will introduce the package, key concepts, and examples.   {sdtm.oak}  is developed in collaboration with volunteers from several companies, including Roche, Pfizer, GSK, Pattern Institute, Transition Technologies Science, and Atorus Research.   {sdtm.oak}  is also sponsored by CDISC COSA with a vision of being part of CDISC 360 to address end-to-end standards development and implementation."
  },
  {
    "objectID": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html#last-updated",
    "href": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html#last-updated",
    "title": "Introducing sdtm.oak",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:03.613486"
  },
  {
    "objectID": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html#details",
    "href": "posts/2024-10-24_introducing.../introducing_sdtm.oak.html#details",
    "title": "Introducing sdtm.oak",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "",
    "text": "The R Consortium Submission Working Group has now successfully made two pilot submissions to the FDA. All the submissions done by the group are focused on improving practices for R-based clinical trial regulatory submissions. Now, the R submission Working Groups, in collaboration with Appsilon and Posit, are exploring new technologies such as Containers and WebAssembly. In this article, we dive into the details of this exploration."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#how-everything-started",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#how-everything-started",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "How Everything Started",
    "text": "How Everything Started\n\nPilot 1\nThis pilot was initially submitted on November 22, 2021. This submission was the first publicly available R-based submission to the FDA. This was a test submission that aimed to explore the submission of an R package to the FDA following the eCTD specifications. The submission included an R package, R scripts for analysis, R-based analysis data reviewed guide (ADRG), and other important components. The final response letter from the FDA was received on March 14, 2022."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-2",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-2",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 2",
    "text": "Pilot 2\nThis was one of the first submission packages containing a Shiny application. The main goal of this pilot was to test the submission of an R-based Shiny application bundled into a submission package and transfer it successfully to FDA reviewers. The submitted application was built using the datasets and analyses that were used for the R Submission Pilot 1. The deployed version of this application is available on this site. Alternatively, a Rhino-based version of the application can be found here.\nThe final response letter from the FDA was reviewed on September 27, 2023.\n\n\n\n\n\nIn this submission, there were many open-source R packages that were used to create and execute the Shiny application. A very well-known shiny-based interactive exploration framework {teal} was used mainly for analyzing the clinical trial data; this package is included in the pharmaverse package repository. The full list of open-source and proprietary R analysis packages is available on this Analysis Data Reviewer’s Guide prepared by the R Consortium R Submissions Working Group for the Pilot 2."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-3",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-3",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 3",
    "text": "Pilot 3\nThis pilot was successfully submitted to the FDA on Aug 28, 2023. This was the first publicly available R submission that included R scripts to produce ADaM datasets and TLFs. Both the ADaMs (SDTM .xpt sources from the CDISC Pilot study) and the TLFs (ADaMs .xpt sourced from the ADaMs generated in R by the Pilot 3 team) were created using R.  The next step for this pilot is to await FDA’s review and approval, which may take several months to complete."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-4",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-4",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 4",
    "text": "Pilot 4\nThis pilot aims to explore using technologies such as containers and WebAssembly software to package a Shiny application into a self-contained unit, streamlining the transfer and execution process for enhanced efficiency.\nThis pilot is expected to be divided into two parallel submissions:\n(a) will investigate WebAssembly and\n(b) will investigate containers.\n\nThe Journey with WebAssembly and Containers\nOur team at Appsilon teamed up with the dynamic Pilot 4 crew to explore WebAssembly technology and containers. George Stagg and Winston Chang also joined the working group to discuss the web-assembly portion of Pilot 4. This partnership brought together our engineering prowess to contribute to these tools, injecting fresh perspectives into the ongoing pilot project.\nSome of the outcomes of the collaboration:\n\nWe were able to set up a robust container environment for this pilot project. \nWe aided the progress made on the use of both experimental technologies: containers and WebAssembly.\nWe developed a working prototype submission using Podman container technology.\nWe developed a working early-stage prototype for wrapping a small Shiny application using WebAssembly.\n\n\nWebAssembly\n\nWebAssembly allows languages like R to be executed at near-native speed directly within web browsers, providing users with the ability to run R code without having R installed locally. WebR is essentially the R programming language adapted to run in a web browser environment using WebAssembly. This project is under active development. \n\n\n\nThe Pilot 4 Shiny App Up and Running on webR!\n\n\n\n\n\nThe deployed example of the Shiny app running on webR is available here. Check out the video of the application running below.\n\nDuring this pilot, engineers at Appsilon developed a prototype of a Shiny application running on webR. The application reuses most of the code from the previous pilot apps with some tweaks and a couple of hacks/changes to get around non CRAN dependencies, specially for data loading, WebR compatibilities, and shimming some of the functionality from {teal} and other packages that are (for now) not available on CRAN.\n\nwebR Shiny App\nDuring the second iteration, which was recently held, Pedro Silva shared the process of developing this Shiny app running on webR.\n\n\nThe Process\n\nLeverage the last 2 iterations of the application\n\nReuse as much code as possible\nAvoid touching the logic part\n\nRestrict the number of dependencies to packages on CRAN\n\nReplace/shim functionality that was lost from removing dependencies\n\n\nHere is the list of dependencies to packages on CRAN; those that worked are colored green, and those that were removed are marked in orange. We ended up with just 3 problematic dependencies (bold).\n\n\n\n\n\nIssues with library(cowplot):\n\nSome issues with low-level dependencies when deployed\n\nSolution:\n\nReplace functionality with HTML\n\nIssues with library(teal):\n\nUses {shiny.widgets} (not working for webR)\n\nSolution:\n\nRedo the UI\nLoad modules directly\nRecreate filter functionality\n\nIssues with library(teal.data):\n\nUse rds exports\n\nSolution:\n\nShim functionality, load data directly\n\n\nLeverage shinylive and httpuv to export and serve the application\n\nShinylive can help streamline the export process\n\nProblems\n\nshiny.live won’t let us have non-R files in the application directory - this is an outstanding bug that George asked us to raise an issue for.\nWe wouldn’t be able to run the application as a traditional shiny app.\n\nSolution:\n\nCustom build script\n\n\n{httpuv} can help serve the application\n\n{httpuv} would run natively on a machine to serve the Shiny app\n\n\n\n\n\nApplication Structure\nThe figure below shows an overview of what we ended with:\n\nSome of the issues and solutions found at the very beginning:\n\nThe previous applications were built using golem and another one in Rhino; the support for these frameworks is not great in webR up to now.\n\nSolution\n\n{box} works out of the box (reuse the rhino version modules)\nSimplify the structure and use a simple shiny modular structure\n\n\nShinylive does not like non-R files when generating the bundle\n\nSolution\n\nKeep the app folder as clean as possible for now (www folder only)\n\n\n{teal} and {teal.data} are not on CRAN\n\nSolution\n\nShim and used functionality\nUse a simple tab system for the UI structure\n\n\n\nThe FDA was previously told that the shiny application being prepared for the Pilot 4 submission would not be a 1 to 1 mapping from the previous one submitted for the Pilot 2 due to certain constraints such as {teal} not being on CRAN; however, this didn’t represent a problem for them since they would mainly like to test the technology.\nPedro Silva, one of the engineers working on the development of this app, mentioned “While WebR is still in development, it shows tremendous promise! The loading is definitely still a pain point (over 100mb to set up the environment!) but it will only get better moving forward.”\n\n\nContainers\n\n\n\n\n\nContainerization, particularly through technologies like Docker, Podman or Singularity, offers several advantages for deploying Shiny apps.\n\nChoosing the Right Container\nChoosing the right container was a question that arose in this project. Although Docker is the most popular, we decided to move forward with Podman. \nIn our exploration of containerization tools for deploying Shiny applications, we’ve identified key distinctions between Docker and Podman that influenced our choice. \nPodman stands out for its daemonless architecture, enhancing security by eliminating the need for a central daemon process. Unlike Docker, Podman supports running containers as non-root users, a critical feature for meeting FDA reviewer requirements. Developed by Red Hat and maintained as an open-source project, Podman prioritizes security with its rootless container support, offering a robust solution for security-conscious users. \n\n\nGoals\nA Container-based method to deploy Pilot 2 Shiny App.\n\n\nWhat we did\n\nConfigurable Podman Dockerfile / docker-compose.yml\n\nR version\nRegistry / organization name / image name (differences between docker.io and ghcr.io)\n\nDocumentation on creating the container\nCI: Automated build on amd64 and arm64 platforms\n\n\n\nPodman short-demo\n\nBelow is the dockerfile (recipe) for the container:"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#last-updated",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#last-updated",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:06.999998"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#details",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#details",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html",
    "title": "Unix versus SAS Time",
    "section": "",
    "text": "Recent discussions within the R Consortium Submission Working Group have highlighted challenges in handling dates between SAS and Unix systems. In addition, during the CDISC Pilot Data update at the Phuse Test Data Factory, using R’s Unix time format resulted in dates (TRTSDT, TRTEDT, and ADT in adlbc.xpt) being mistakenly advanced by +10 years.\nThis blog post explores the differences in date handling between SAS and R, focusing on epoch discrepancies and data types. It also discusses key considerations for conversion tools to ensure accurate date conversions and maintain data integrity."
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#epoch-in-r-and-sas",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#epoch-in-r-and-sas",
    "title": "Unix versus SAS Time",
    "section": "Epoch in R and SAS",
    "text": "Epoch in R and SAS\nAn epoch is a reference point from which time is measured. In computing, it’s often used as the starting point for a system’s time-keeping, allowing for the easy calculation of time intervals. Different systems use different epochs, which can lead to confusion if not properly managed.\nIn SAS, the epoch is defined as midnight on January 1, 1960. (01JAN1960:00:00:00).\nR uses the Unix epoch which begins epoch begins at midnight on January 1, 1970 (01JAN1970:00:00:00)\nFor example, a date with a value of 19725 corresponds to 03JAN2014 in SAS and 03JAN2024 in R."
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#date-differences-between-r-and-sas",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#date-differences-between-r-and-sas",
    "title": "Unix versus SAS Time",
    "section": "Date Differences Between R and SAS",
    "text": "Date Differences Between R and SAS\nAnother important aspect to consider is the difference in data types for numeric dates in R and SAS.\n\nSAS: SAS only supports numeric and character data types. Therefore, numeric dates in SAS need a format applied to be human-readable.\nR: In contrast, R has specific data types for dates, which allow numeric dates to be rendered with a date format directly in data frames. The data types in R are: “Date” for dates, “hms” or “difftime” for time, and “POSIXct” or “POSIXt” for datetime.\n\nThese differences can lead to discrepancies if not properly accounted for during data conversion, potentially resulting in a date shift of 10 years when transferring data between SAS and R."
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#considerations-for-conversion",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#considerations-for-conversion",
    "title": "Unix versus SAS Time",
    "section": "Considerations for Conversion",
    "text": "Considerations for Conversion\nTo illustrate how date conversions between SAS and R can be managed, let’s look at the haven package in R. This package facilitates the conversion of data between SAS and R.\n\nFrom SAS to R: Numeric variables with dates formats are identified as dates and converted to POSIX date(time) formats.\nDetection of date, time, datetime variable from SAS by haven package\n\nFrom R to SAS: POSIX date variables are converted to SAS numeric dates. By default, a format is added to these dates to make them human-readable. This format can be customized by changing the attribute or by using the xportr package to pull the format from metadata and apply it to the corresponding variable.\nDetection of date, time, datetime variable from R by haven package"
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#dealing-with-date-interoperability-in-dataset-json",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#dealing-with-date-interoperability-in-dataset-json",
    "title": "Unix versus SAS Time",
    "section": "Dealing with Date Interoperability in Dataset-JSON",
    "text": "Dealing with Date Interoperability in Dataset-JSON\nDataset-JSON v1.1 could potentially be used in future data exchanges between SAS and R. Given the differing approaches to handling dates across programming languages, the challenge lies in managing dates within a programming language agnostic data exchange format.\nWhat is Dataset-JSON? Dataset-JSON is a dataset exchange standard that uses JSON. It is designed to meet regulatory submission requirements and other dataset exchange scenarios. It has the potential to replace XPT as the default format for clinical and device data submissions to regulatory authorities.\nTo address interoperability issues across programming languages, options include storing numeric dates as ISO 8601 strings with a targetdatatype attribute to differentiate between character and numeric dates, or associating the epoch with the numeric date values. Refer to the Dataset-JSON specification for the latest date handling specifications. These approaches ensure that conversion tools receive sufficient information to handle dates correctly.\nTo ensure the correct handling of dates in Dataset-JSON, you can use the datasetjson R package, which is built to read and write CDISC Dataset JSON formatted datasets."
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#conclusion",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#conclusion",
    "title": "Unix versus SAS Time",
    "section": "Conclusion",
    "text": "Conclusion\nThe 10-year date shift between R and SAS can be avoided by using appropriate conversion tools such as xportr and haven. To ensure correct date conversions, verify that the datetime variables in SAS and R have the correct data type and a date format for SAS. By paying attention to these details, you can maintain data integrity and avoid significant discrepancies during data conversion processes."
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#last-updated",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#last-updated",
    "title": "Unix versus SAS Time",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:12.331497"
  },
  {
    "objectID": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#details",
    "href": "posts/2024-07-08_unix_vs_sas_date.../unix_vs_sas_datetime.html#details",
    "title": "Unix versus SAS Time",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#new-functions",
    "href": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#new-functions",
    "title": "Hello admiralmetabolic!",
    "section": "New Functions",
    "text": "New Functions\nThe first function,   admiralmetabolic::derive_param_waisthgt() , is a wrapper to   admiral::derive_param_computed()  and enables users to add derived waist to height ratio parameters to a vital signs ADVS ADaM using waist circumference and height records. Here is an example of the function in action:\n\nlibrary(tibble)\nlibrary(admiral)\nlibrary(admiralmetabolic)\n\n# Derive Waist to Height Ratio. In this example height is measured only once per subject.\n\nadvs &lt;- tribble(\n  ~USUBJID,      ~PARAMCD, ~PARAM,                     ~AVAL, ~AVALU, ~VISIT,\n  \"01-101-1001\", \"HEIGHT\", \"Height (cm)\",              147,   \"cm\",   \"SCREENING\",\n  \"01-101-1001\", \"WSTCIR\", \"Waist Circumference (cm)\", 110,   \"cm\",   \"SCREENING\",\n  \"01-101-1001\", \"WSTCIR\", \"Waist Circumference (cm)\", 108,   \"cm\",   \"WEEK 2\",\n  \"01-101-1001\", \"WSTCIR\", \"Waist Circumference (cm)\", 107,   \"cm\",   \"WEEK 3\",\n  \"01-101-1002\", \"HEIGHT\", \"Height (cm)\",              163,   \"cm\",   \"SCREENING\",\n  \"01-101-1002\", \"WSTCIR\", \"Waist Circumference (cm)\", 120,   \"cm\",   \"SCREENING\",\n  \"01-101-1002\", \"WSTCIR\", \"Waist Circumference (cm)\", 118,   \"cm\",   \"WEEK 2\",\n  \"01-101-1002\", \"WSTCIR\", \"Waist Circumference (cm)\", 117,   \"cm\",   \"WEEK 3\",\n)\n\nderive_param_waisthgt(\n  advs,\n  by_vars = exprs(USUBJID, VISIT),\n  wstcir_code = \"WSTCIR\",\n  height_code = \"HEIGHT\",\n  set_values_to = exprs(\n    PARAMCD = \"WAISTHGT\",\n    PARAM = \"Waist to Height Ratio\"\n  ),\n  constant_by_vars = exprs(USUBJID),\n  get_unit_expr = admiral::extract_unit(PARAM)\n)\n\n# A tibble: 14 × 6\n   USUBJID     PARAMCD  PARAM                       AVAL AVALU VISIT    \n   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 01-101-1001 HEIGHT   Height (cm)              147     cm    SCREENING\n 2 01-101-1001 WSTCIR   Waist Circumference (cm) 110     cm    SCREENING\n 3 01-101-1001 WSTCIR   Waist Circumference (cm) 108     cm    WEEK 2   \n 4 01-101-1001 WSTCIR   Waist Circumference (cm) 107     cm    WEEK 3   \n 5 01-101-1002 HEIGHT   Height (cm)              163     cm    SCREENING\n 6 01-101-1002 WSTCIR   Waist Circumference (cm) 120     cm    SCREENING\n 7 01-101-1002 WSTCIR   Waist Circumference (cm) 118     cm    WEEK 2   \n 8 01-101-1002 WSTCIR   Waist Circumference (cm) 117     cm    WEEK 3   \n 9 01-101-1001 WAISTHGT Waist to Height Ratio      0.748 &lt;NA&gt;  SCREENING\n10 01-101-1001 WAISTHGT Waist to Height Ratio      0.735 &lt;NA&gt;  WEEK 2   \n11 01-101-1001 WAISTHGT Waist to Height Ratio      0.728 &lt;NA&gt;  WEEK 3   \n12 01-101-1002 WAISTHGT Waist to Height Ratio      0.736 &lt;NA&gt;  SCREENING\n13 01-101-1002 WAISTHGT Waist to Height Ratio      0.724 &lt;NA&gt;  WEEK 2   \n14 01-101-1002 WAISTHGT Waist to Height Ratio      0.718 &lt;NA&gt;  WEEK 3   \n\n\nNote that this function has built-in support for:\n\nUnit conversion in cases where units differ across the numerator and denominator (allowed units: m, mm, ft, in);\nSingle or multiple height measurement by modifying the argument constant_by_vars. Please visit the function’s reference page for more details.\n\n  admiralmetabolic::derive_param_waisthip()  is very similar: it is also a wrapper to   admiral::derive_param_computed() , but this time enables users to add derived waist to hip ratio parameters using waist- and hip circumference. It boasts the same unit-flexibility as   admiralmetabolic::derive_param_waisthgt()  - let’s look at that in action:\n\nadvs2 &lt;- tribble(\n  ~USUBJID,      ~PARAMCD, ~PARAM,                     ~AVAL, ~AVALU, ~VISIT,\n  \"01-101-1001\", \"HIPCIR\", \"Hip Circumference (cm)\",   125,   \"cm\",   \"SCREENING\",\n  \"01-101-1001\", \"HIPCIR\", \"Hip Circumference (cm)\",   124,   \"cm\",   \"WEEK 2\",\n  \"01-101-1001\", \"WSTCIR\", \"Waist Circumference (in)\", 43.31, \"in\",   \"SCREENING\",\n  \"01-101-1001\", \"WSTCIR\", \"Waist Circumference (in)\", 42.52, \"in\",   \"WEEK 2\",\n  \"01-101-1002\", \"HIPCIR\", \"Hip Circumference (cm)\",   135,   \"cm\",   \"SCREENING\",\n  \"01-101-1002\", \"HIPCIR\", \"Hip Circumference (cm)\",   133,   \"cm\",   \"WEEK 2\",\n  \"01-101-1002\", \"WSTCIR\", \"Waist Circumference (in)\", 47.24, \"in\",   \"SCREENING\",\n  \"01-101-1002\", \"WSTCIR\", \"Waist Circumference (in)\", 46.46, \"in\",   \"WEEK 2\"\n)\n\nderive_param_waisthip(\n  advs2,\n  by_vars = exprs(USUBJID, VISIT),\n  wstcir_code = \"WSTCIR\",\n  hipcir_code = \"HIPCIR\",\n  set_values_to = exprs(\n    PARAMCD = \"WAISTHIP\",\n    PARAM = \"Waist to Hip Ratio\"\n  ),\n  get_unit_expr = admiral::extract_unit(PARAM)\n)\n\nℹ Unit conversion performed for \"HIPCIR\". Values converted from \"cm\" to \"in\".\n\n\n# A tibble: 12 × 6\n   USUBJID     PARAMCD  PARAM                       AVAL AVALU VISIT    \n   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 01-101-1001 HIPCIR   Hip Circumference (cm)   125     cm    SCREENING\n 2 01-101-1001 HIPCIR   Hip Circumference (cm)   124     cm    WEEK 2   \n 3 01-101-1001 WSTCIR   Waist Circumference (in)  43.3   in    SCREENING\n 4 01-101-1001 WSTCIR   Waist Circumference (in)  42.5   in    WEEK 2   \n 5 01-101-1002 HIPCIR   Hip Circumference (cm)   135     cm    SCREENING\n 6 01-101-1002 HIPCIR   Hip Circumference (cm)   133     cm    WEEK 2   \n 7 01-101-1002 WSTCIR   Waist Circumference (in)  47.2   in    SCREENING\n 8 01-101-1002 WSTCIR   Waist Circumference (in)  46.5   in    WEEK 2   \n 9 01-101-1001 WAISTHIP Waist to Hip Ratio         0.880 &lt;NA&gt;  SCREENING\n10 01-101-1001 WAISTHIP Waist to Hip Ratio         0.871 &lt;NA&gt;  WEEK 2   \n11 01-101-1002 WAISTHIP Waist to Hip Ratio         0.889 &lt;NA&gt;  SCREENING\n12 01-101-1002 WAISTHIP Waist to Hip Ratio         0.887 &lt;NA&gt;  WEEK 2"
  },
  {
    "objectID": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#vignettes-and-template",
    "href": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#vignettes-and-template",
    "title": "Hello admiralmetabolic!",
    "section": "Vignettes and Template",
    "text": "Vignettes and Template\nOn the   {admiralmetabolic}  package website, users will find the obesity ADVS vignette. This documentation page will guide users towards creating an obesity ADVS ADaM, touching upon how to use our two new functions   admiralmetabolic::derive_param_waisthgt()  and   admiralmetabolic::derive_param_waisthip()  as well as how to apply existing   {admiral}  tools such as the brand-new (as of admiral 1.2)   admiral::derive_vars_cat()  to create weight classes within the AVALCAT variable and   admiral::derive_vars_crit_flag()  to create criterion flag/variable pairs for endpoints such as a 5% reduction in weight at a certain visit.\nOf course, this vignette is coupled with the obesity ADVS template program, which users can use as a starting point for their ADaM scripts. The template can also be loaded and saved directly from the R console by running:\n\nadmiral::use_ad_template(\"ADVS\", package = \"admiralmetabolic\")\n\nThe Control of Eating Questionnaire (COEQ) is used in many obesity trials as an endpoint. As such,   {admiralmetabolic}  also contains a vignette geared towards the creation of an ADaM dataset ADCOEQ for the COEQ. Due to the flexibility of the parent   {admiral}  package, functions such as   admiral::derive_summary_records()  can be employed to create ADCOEQ. Users are invited to visit the vignette for more details and a full walkthrough.\nOnce again, this vignette comes hand-in-hand with the ADCOEQ program, which can be accessed through the link above or directly from the R console by running:\n\nadmiral::use_ad_template(\"ADCOEQ\", package = \"admiralmetabolic\")"
  },
  {
    "objectID": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#last-updated",
    "href": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#last-updated",
    "title": "Hello admiralmetabolic!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:18.56102"
  },
  {
    "objectID": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#details",
    "href": "posts/2025-01-21_hello_admiralmetabolic/hello_admiralmetabolic.html#details",
    "title": "Hello admiralmetabolic!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html",
    "href": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "",
    "text": "Cytel is the first CRO involved in the {admiral} open-source project. Having the ambition to demonstrate our skills in the open-source projects, it was with lots of excitement that we accepted the challenge to enter the {admiral} development team family. We are thus thrilled to share with you this new & challenging experience."
  },
  {
    "objectID": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html#last-updated",
    "href": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html#last-updated",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:21.952152"
  },
  {
    "objectID": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html#details",
    "href": "posts/2024-05-29_our_experience_a.../our_experience_as_new_admiral_developers.html#details",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html",
    "href": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html",
    "title": "Simplifying Clinical Data Dashboards with {teal} and {pharmaverseadam}",
    "section": "",
    "text": "Every developer must solve two difficult problems when creating a Shiny application (in fact, any application) from the ground up: software architecture and data design. In the world of clinical data analysis, however, much development has been aimed at providing a jump-start approach to creating R/Shiny applications that would take away most of the pain caused by these two problems.\nThis blog should help you get an idea of how easy it is to get started with the pharmaverse ecosystem. We will create an interactive clinical data dashboard using {teal} and will use {pharmaverseadam} as the data source.\nFirst, you would need to install the two packages. It is recommended to use {pak} to take advantage of parallel downloads and builds (note that you can use it together with {renv} for locking dependencies)."
  },
  {
    "objectID": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html#building-a-simple-teal-app",
    "href": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html#building-a-simple-teal-app",
    "title": "Simplifying Clinical Data Dashboards with {teal} and {pharmaverseadam}",
    "section": "Building a Simple Teal App",
    "text": "Building a Simple Teal App\nNext, we would create a simple app - basically following the official teal guide, but we will use datasets from the {pharmaverseadam} package.\n# Step 1: import packages\nlibrary(teal)\nlibrary(pharmaverseadam)\n\n# Step 2: create a teal data object\ndata &lt;- cdisc_data(\n  ADAE = pharmaverseadam::adae,\n  ADSL = pharmaverseadam::adsl\n)\n\n# Step 3: initialize teal app\napp &lt;- init(\n  data = data,\n  modules = example_module()\n)\n\n# Step 4: run shiny app\nshinyApp(app$ui, app$server)\n\nLet’s Take a Closer Look at Each Step:\n\nIn step 1, we import libraries to use functions and datasets exported by those.\nIn step 2, we create a {teal} data object that can be used in a {teal} app. There are a couple of subtle details here: instead of the default teal_data() function, we use a special wrapper around it, designed specifically for clinical trial data - cdisc_data(). Its advantage is that it will automatically generate join_keys() for the datasets that we pass in. Please note, that the dataset names should be all caps. The resulting data object is an s4 class instance, and we can verify that the output is correct by checking data names and join_keys slots:\n\nr$&gt; data@datanames\n[1] \"ADAE\" \"ADSL\"\n\nr$&gt; data@join_keys\nA join_keys object containing foreign keys between 2 datasets:\nADSL: [STUDYID, USUBJID]\n  &lt;-- ADAE: [STUDYID, USUBJID]\nADAE: [STUDYID, USUBJID, ASTDTM, AETERM, AESEQ]\n  --&gt; ADSL: [STUDYID, USUBJID]\n\nIn step 3, we initialize the {teal} app by passing it a data object and adding an example module—it’s not much, but it will let us verify that the app works. The output app object is just a list with UI and server functions.\nFinally step 4 should be familiar to all Shiny users - we need to pass the UI and server function from the app object generated at step 3.\n\nWhen we run the app, this is what we will see in the web browser:\n\nAs you can see, with just a few lines of code, we were able to create a working app with some interesting capabilities. We have two datasets that we can switch between. For each dataset, we have keys defined (marked with a special key icon in the variable drop-down). When we filter one dataset, the other one gets filtered as well because they are connected with a key.\nThis is truly impressive, but there is just one problem… If we click the “Show R code” button, we will notice that the data used in the app is not “reproducible.” This simply means that the app currently does not have information about where the data comes from, so it cannot instruct users on how to obtain the same data.\n\nLet’s fix this. We will have to make the code slightly more verbose:\ndata &lt;- within(teal_data(), {\n  ADAE &lt;- pharmaverseadam::adae\n  ADSL &lt;- pharmaverseadam::adsl\n})\ndatanames(data) &lt;- c(\"ADAE\", \"ADSL\")\njoin_keys(data) &lt;- default_cdisc_join_keys[datanames(data)]\nThe recommended method is to generate the data using a within function. However, this method requires manually providing databases using a helper function. We also need to provide join_keys ourselves, but given that the data names are standard ADaM names, we can take advantage of a special default_cdisc_join_keys object.\nIt is also worth noting that {teal} has its implementation of within generic.\nThis is what we will see now when running the application:\n\nNow we have reproducible data. But what about the app itself? Surely, {teal} features don’t end here. There is a collection of pre-built shiny modules that can be used in teal applications. We can install them with pak:\npak::pkg_install(\n  c(\"sparkline\", \"teal.modules.general\", \"teal.modules.clinical\")\n)\nFirst, we suggest exploring the “general” modules that are applicable to any kind of data. The only (!) thing we need to do, is to add two more modules to the app initializer:\napp &lt;- init(\n  data = data,\n  modules = modules(\n    example_module(),\n    tm_data_table(\"Table View\"),\n    tm_variable_browser(\"Variables\")\n  )\n)\nThis is possible thanks to the magic that {teal} is doing under the hood - passing the data object to each module. And now we will have access to a nice tabular view of the data, and a tool to explore each variable in greater detail.\n\nThis is great on its own, but as a bonus, we even get the ability to build a report based on some of the modules that we have. For example, we can generate some plots in the variable browser, add them to the report and preview it. Some modules would also add a block of R code showing how to get the exact same data that was used to generate a report card.\nFinally, let’s add a simple barchart module that comes from the clinical modules package. We will use an example from the {teal.modules.clinical} documentation:\nbarchart_module &lt;- tm_g_barchart_simple(\n  label = \"ADAE Analysis\",\n  x = data_extract_spec(\n    dataname = \"ADAE\",\n    select = select_spec(\n      choices = variable_choices(\n        pharmaverseadam::adae,\n        c(\"ARM\", \"ACTARM\", \"SEX\")\n      ),\n      selected = \"ACTARM\",\n      multiple = FALSE\n    )\n  )\n)\n\nThe best part about this module is that when a card is added to the report, it has R code that will reproduce exactly the same output that we see in the app.\nHere is the entire code for the application. In just 40 lines of code we were able to create a feature-rich application with the ability to interact with ADaM data, create visualizations and generate reproducible reports.\nlibrary(sparkline)\nlibrary(teal)\nlibrary(teal.data)\nlibrary(teal.modules.clinical)\nlibrary(teal.modules.general)\n\ndata &lt;- within(teal_data(), {\n  ADAE &lt;- pharmaverseadam::adae\n  ADSL &lt;- pharmaverseadam::adsl\n  # nolint end\n})\ndatanames(data) &lt;- c(\"ADAE\", \"ADSL\")\njoin_keys(data) &lt;- default_cdisc_join_keys[datanames(data)]\n\nbarchart_module &lt;- tm_g_barchart_simple(\n  label = \"ADAE Analysis\",\n  x = data_extract_spec(\n    dataname = \"ADAE\",\n    select = select_spec(\n      choices = variable_choices(\n        pharmaverseadam::adae,\n        c(\"ARM\", \"ACTARM\", \"SEX\")\n      ),\n      selected = \"ACTARM\",\n      multiple = FALSE\n    )\n  )\n)\n\napp &lt;- init(\n  data = data,\n  modules = modules(\n    example_module(),\n    tm_data_table(\"Table View\"),\n    tm_variable_browser(\"Variables\"),\n    barchart_module\n  )\n)\n\nshinyApp(app$ui, app$server)"
  },
  {
    "objectID": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html#conclusion",
    "href": "posts/2024-07-22_teal_app_development_pharmaverseadam/teal-app-development.html#conclusion",
    "title": "Simplifying Clinical Data Dashboards with {teal} and {pharmaverseadam}",
    "section": "Conclusion ",
    "text": "Conclusion \nIn conclusion, {teal} and {pharmaverseadam} make it much easier to create interactive and reproducible clinical data dashboards. By following this guide, you can quickly build a Shiny app that not only visualizes your data but also maintains reproducibility and customization options. \nGet the latest updates from the pharmaverse delivered to your inbox. Subscribe to our newsletter today."
  },
  {
    "objectID": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html",
    "href": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html",
    "title": "How I Rebuilt a Lost ECG Data Script in R",
    "section": "",
    "text": "As a Data Science placement student at Roche UK, I was given an exciting opportunity to enhance my R programming skills while contributing to the open-source community. Under the guidance of my manager, Edoardo Mancini, I undertook a unique and challenging task within the {pharmaversesdtm} project that tested both my technical expertise and problem-solving abilities.\nThe project involved recreating the eg domain (Electrocardiogram data) from the SDTM datasets used within {pharmaversesdtm}. The original dataset had been sourced from the CDISC pilot project, but since that source was no longer available, we had no direct reference. Fortunately, a saved copy of the dataset still existed, allowing me to analyze it and attempt to reproduce it as closely as possible."
  },
  {
    "objectID": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#how-i-solved-the-problem",
    "href": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#how-i-solved-the-problem",
    "title": "How I Rebuilt a Lost ECG Data Script in R",
    "section": "How I Solved the Problem",
    "text": "How I Solved the Problem\n\nExplored and Analyzed the Data\nThe first step was to thoroughly explore the existing ECG dataset of over 25,000 entries. I needed to understand the structure and key variables that defined the dataset, such as the “one row for each patient’s test during each visit” format. By analyzing these elements, I was able to gain a clear picture of how the dataset was organized. I also examined the range of values, variance, and other characteristics of the tests to ensure that my recreated version would align with the original dataset’s structure and statistical properties.\nTo provide a clearer understanding of how the data is structured, let’s take a quick look at the information collected during a patient’s visit. Below is an example of data for patient 01-701-1015 during their WEEK 2 visit:\n\n\n# A tibble: 10 × 7\n   USUBJID     EGTEST             VISIT  EGDTC      EGTPT EGSTRESN EGSTRESC\n   &lt;chr&gt;       &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   \n 1 01-701-1015 QT Duration        WEEK 2 2014-01-16 \"1\"        449 449     \n 2 01-701-1015 QT Duration        WEEK 2 2014-01-16 \"2\"        511 511     \n 3 01-701-1015 QT Duration        WEEK 2 2014-01-16 \"3\"        534 534     \n 4 01-701-1015 Heart Rate         WEEK 2 2014-01-16 \"1\"         63 63      \n 5 01-701-1015 Heart Rate         WEEK 2 2014-01-16 \"2\"         83 83      \n 6 01-701-1015 Heart Rate         WEEK 2 2014-01-16 \"3\"         66 66      \n 7 01-701-1015 RR Duration        WEEK 2 2014-01-16 \"1\"        316 316     \n 8 01-701-1015 RR Duration        WEEK 2 2014-01-16 \"2\"        581 581     \n 9 01-701-1015 RR Duration        WEEK 2 2014-01-16 \"3\"        570 570     \n10 01-701-1015 ECG Interpretation WEEK 2 2014-01-16 \"\"          NA ABNORMAL\n\n\nIn this example, USUBJID identifies the subject, while EGTEST specifies the type of ECG test performed. VISIT refers to the visit during which the test occurred, and EGDTC records the date of the test. EGTPT indicates the condition under which the ECG test was conducted. EGSTRESN provides the numeric result, and EGSTRESC gives the corresponding categorical result.\n\n\nWrote the New R Script\nArmed with insights from my analysis, I set about writing a new R script to replicate the lost one. This involved a lot of trial and error, as I kept refining the code to ensure it generated a dataset that closely resembled the original ECG data in both structure and content. In order to give you, my reader, better understanding of the solution, I’ll walk you through the main parts of the script.\n\nLoading Libraries and Data\nTo begin, I loaded the necessary libraries and read in the vital signs (vs) dataset, This dataset is functional to my cause because it has the same structure and schedule as the eg data, so I can recreate the eg visit schedule for each patient from it. By setting a seed for the random data generation, I ensured that the process was reproducible, allowing others to verify my results and maintain consistency in future analyses. Additionally, the metatools package was loaded to facilitate adding labels to the variables later, which enhanced the readability of the dataset.\n\nlibrary(dplyr)\nlibrary(metatools)\nlibrary(pharmaversesdtm)\n\ndata(\"vs\")\nset.seed(123)\n\n\n\nExtracting Unique Date/Time of Measurements\nNext, I extracted the unique combination of subject IDs, visit names, and visit dates from the vs dataset.\n\negdtc &lt;- vs %&gt;%\n  select(USUBJID, VISIT, VSDTC) %&gt;%\n  distinct() %&gt;%\n  rename(EGDTC = VSDTC)\n\negdtc\n\n# A tibble: 2,741 × 3\n   USUBJID     VISIT               EGDTC     \n   &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt;     \n 1 01-701-1015 SCREENING 1         2013-12-26\n 2 01-701-1015 SCREENING 2         2013-12-31\n 3 01-701-1015 BASELINE            2014-01-02\n 4 01-701-1015 AMBUL ECG PLACEMENT 2014-01-14\n 5 01-701-1015 WEEK 2              2014-01-16\n 6 01-701-1015 WEEK 4              2014-01-30\n 7 01-701-1015 AMBUL ECG REMOVAL   2014-02-01\n 8 01-701-1015 WEEK 6              2014-02-12\n 9 01-701-1015 WEEK 8              2014-03-05\n10 01-701-1015 WEEK 12             2014-03-26\n# ℹ 2,731 more rows\n\n\nThis data was used later to match the generated ECG data to the correct visit and time points.\n\n\nGenerating a Grid of Patient Data\nSubsequently, I created a grid of all possible combinations of subject IDs, test codes (e.g., QT, HR, RR, ECGINT), time points (e.g., after lying down, after standing), and visits. These combinations represented different test results collected across multiple visits.\n\neg &lt;- expand.grid(\n  USUBJID = unique(vs$USUBJID),\n  EGTESTCD = c(\"QT\", \"HR\", \"RR\", \"ECGINT\"),\n  EGTPT = c(\n    \"AFTER LYING DOWN FOR 5 MINUTES\",\n    \"AFTER STANDING FOR 1 MINUTE\",\n    \"AFTER STANDING FOR 3 MINUTES\"\n  ),\n  VISIT = c(\n    \"SCREENING 1\",\n    \"SCREENING 2\",\n    \"BASELINE\",\n    \"AMBUL ECG PLACEMENT\",\n    \"WEEK 2\",\n    \"WEEK 4\",\n    \"AMBUL ECG REMOVAL\",\n    \"WEEK 6\",\n    \"WEEK 8\",\n    \"WEEK 12\",\n    \"WEEK 16\",\n    \"WEEK 20\",\n    \"WEEK 24\",\n    \"WEEK 26\",\n    \"RETRIEVAL\"\n  ), stringsAsFactors = FALSE\n)\n\n# Filter the dataset for one subject and one visit\nfiltered_eg &lt;- eg %&gt;%\n  filter(USUBJID == \"01-701-1015\" & VISIT == \"WEEK 2\")\n\n# Display the result\nfiltered_eg\n\n       USUBJID EGTESTCD                          EGTPT  VISIT\n1  01-701-1015       QT AFTER LYING DOWN FOR 5 MINUTES WEEK 2\n2  01-701-1015       HR AFTER LYING DOWN FOR 5 MINUTES WEEK 2\n3  01-701-1015       RR AFTER LYING DOWN FOR 5 MINUTES WEEK 2\n4  01-701-1015   ECGINT AFTER LYING DOWN FOR 5 MINUTES WEEK 2\n5  01-701-1015       QT    AFTER STANDING FOR 1 MINUTE WEEK 2\n6  01-701-1015       HR    AFTER STANDING FOR 1 MINUTE WEEK 2\n7  01-701-1015       RR    AFTER STANDING FOR 1 MINUTE WEEK 2\n8  01-701-1015   ECGINT    AFTER STANDING FOR 1 MINUTE WEEK 2\n9  01-701-1015       QT   AFTER STANDING FOR 3 MINUTES WEEK 2\n10 01-701-1015       HR   AFTER STANDING FOR 3 MINUTES WEEK 2\n11 01-701-1015       RR   AFTER STANDING FOR 3 MINUTES WEEK 2\n12 01-701-1015   ECGINT   AFTER STANDING FOR 3 MINUTES WEEK 2\n\n\nIn order to demonstrate the data more clearly, I have displayed the combinations for only one subject and one visit for you to see, as the full table is very large. Each of these test codes corresponds to specific ECG measurements: QT refers to the QT interval (a measurement made on an electrocardiogram used to assess some of the electrical properties of the heart), HR represents heart rate, RR is the interval between R waves, and ECGINT refers to the ECG interpretation.\nAs I analyzed the original ECG dataset, I learned more about these test codes and their relevance to the clinical data.\n\n\nGenerating Random Test Results\nFor each combination in the grid, I generated random test results using a normal distribution to simulate realistic values for each test code. To determine the means and standard deviations, I used the original EG dataset as a reference. By analyzing the range and distribution of values in the original dataset, I was able to extract realistic means and standard deviations for each numerical ECG test (QT, HR, RR).\nEGSTRESN = case_when(\nEGTESTCD == \"RR\" & EGELTM == \"PT5M\" ~ floor(rnorm(n(), 543.9985, 80)),\nEGTESTCD == \"RR\" & EGELTM == \"PT3M\" ~ floor(rnorm(n(), 536.0161, 80)),\nEGTESTCD == \"RR\" & EGELTM == \"PT1M\" ~ floor(rnorm(n(), 532.3233, 80)),\nEGTESTCD == \"HR\" & EGELTM == \"PT5M\" ~ floor(rnorm(n(), 70.04389, 8)),\nEGTESTCD == \"HR\" & EGELTM == \"PT3M\" ~ floor(rnorm(n(), 74.27798, 8)),\nEGTESTCD == \"HR\" & EGELTM == \"PT1M\" ~ floor(rnorm(n(), 74.77461, 8)),\nEGTESTCD == \"QT\" & EGELTM == \"PT5M\" ~ floor(rnorm(n(), 450.9781, 60)),\nEGTESTCD == \"QT\" & EGELTM == \"PT3M\" ~ floor(rnorm(n(), 457.7265, 60)),\nEGTESTCD == \"QT\" & EGELTM == \"PT1M\" ~ floor(rnorm(n(), 455.3394, 60))\n)\nThis approach ensured that the synthetic data aligned closely with the patterns and variability observed in the original clinical data.\n\n\nFinalizing the Dataset\nFinally, I added labels to the dataframe for easier analysis and future use by utilizing the metatools::add_labels() function.\nadd_labels(\nSTUDYID = \"Study Identifier\",\nUSUBJID = \"Unique Subject Identifier\",\nEGTEST = \"ECG Test Name\",\nVISIT = \"Visit Name\",\nEGSTRESC = \"Character Result/Finding in Std Format\",\nEGSTRESN = \"Numeric Result/Finding in Standard Units\",\n&lt;etc&gt;\n)\nThis provided descriptive names for each column in the dataset, making it more intuitive to understand the data during analysis and ensuring clarity in its subsequent use.\n\n\nLimitations\nHowever, this approach has certain limitations. One key issue is that the simulations do not account for the time structure, as each observation is generated independently (i.i.d.), which does not reflect real-world dynamics. Additionally, sampling from a normal distribution may not always be appropriate and can sometimes yield unrealistic results, such as negative heart rate (HR) values. To mitigate this, I manually reviewed the generated data to ensure that only plausible values were included. Below are the valid ranges I established for this purpose:\n\n# Filter the data for the relevant test codes (QT, RR, HR)\neg_filtered &lt;- pharmaversesdtm::eg %&gt;%\n  filter(EGTESTCD %in% c(\"QT\", \"HR\", \"RR\"))\n\n# Display the minimum and maximum values for each test code\nvalue_ranges &lt;- eg_filtered %&gt;%\n  group_by(EGTESTCD) %&gt;%\n  summarize(\n    min_value = min(EGSTRESN, na.rm = TRUE),\n    max_value = max(EGSTRESN, na.rm = TRUE)\n  )\n\n# Show the result\nvalue_ranges\n\n# A tibble: 3 × 3\n  EGTESTCD min_value max_value\n  &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 QT             242       671\n2 HR              40       107\n3 RR             236       889\n\n\n\n\n\nConclusion\nThis project not only sharpened my R programming skills but also provided invaluable experience in reverse-engineering data, analyzing large healthcare datasets, and tackling real-world challenges in the open-source domain. By following a structured approach, I was able to successfully recreate the EG dataset synthetically, ensuring it mirrors realistic clinical data. This achievement not only enhances my technical capabilities but also contributes to the broader open-source community, as the synthetic dataset will be featured in the next release of {pharmaversesdtm}, offering a valuable resource for future research and development."
  },
  {
    "objectID": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#last-updated",
    "href": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#last-updated",
    "title": "How I Rebuilt a Lost ECG Data Script in R",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:30.808034"
  },
  {
    "objectID": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#details",
    "href": "posts/2024-10-31_how__i__reb.../how__i__rebuilt_a__lost__ec_g__data__script_in__r.html#details",
    "title": "How I Rebuilt a Lost ECG Data Script in R",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html",
    "href": "posts/2023-06-27__hackathon_app/index.html",
    "title": "Hackathon Feedback Application",
    "section": "",
    "text": "We recently created a shiny application for the admiral hackathon in February 2023. The admiral hackathon was an event designed to make statistical programmers from the pharmaceutical industry more comfortable with the admiral R package which allows users to efficiently transform data from one data standard (SDTM) to another (ADaM).\nHackathon participants formed groups of up to five people and were then tasked to create R-scripts that map the SDTM data to ADaM according to specifics defined in the metadata.\nThe purpose of the shiny app was threefold:\nIn this blog post I want to highlight some of the thoughts that went into this application. Please keep in mind that this work was done under tight time restraints.\nThe hackathon application is still online (although data-upload is switched off) and the GitHub repository is publicly available. The application is embedded into this post right after this paragraph. I have also uploaded to GitHub a .zip file of the workspace to which hackathon participants had access via posit cloud. For more context you can watch recordings of the hackathon-meetings."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#permanent-data",
    "href": "posts/2023-06-27__hackathon_app/index.html#permanent-data",
    "title": "Hackathon Feedback Application",
    "section": "Permanent Data",
    "text": "Permanent Data\nThe biggest challenge you have to consider for this app is the permanent data storage. Shiny apps run on a server. Although we can write files on this server, whenever the app restarts, the files are lost. Therefore, a persistent data storage solution is required.\n\nGoogle drive\nI decided to leverage Google drive using the googledrive package package. This allowed me to save structured data (the team registry and the submission scores) as well as unstructured data (their R-script files).\n\n\n\n\n\n\nAuthentication\n\n\n\nTo access Google drive using the googledrive package we need to authenticate. This can be done interactively using the command googledrive::drive_auth() which takes you to the Google login page. After login you receive an authentication token requested by R.\nFor non-interactive authentication this token must be stored locally. In our case where the shiny app must access the token once deployed, the token must be stored on the project level.\nI have included the authentication procedure I followed in the R folder in google_init.R. You can find more extensive documentation of the non-interactive authentication.\n\n\nThe initial concept was: Each team gets their own folder including the most recent submission for each task, and a .csv file containing team information. To keep track of the submissions and the respective scores we wrote a .csv file in the mock-hackathon folder, so one folder above the team folders.\nSaving the team info as a .csv file worked fine as each team received their own file which – once created – was not touched anymore. As each upload for every team should simply add a row to the submissions.csv file, appending the file would be ideal. This was not possible using the googledrive package package. Instead, for each submission, the submissions file was downloaded, appended, and uploaded again. Unfortunately, this lead to a data loss, as the file was continuously overwritten, especially when two teams would submit simultaneously.\n\n\n\n\n\n\nRecover the Lost Data\n\n\n\nWhenever the submissions.csv file was uploaded, the previous version was sent to the Google drive bin. We ended up with over 3000 submissions.csv files containing a lot of redundant information. I had to write the following chunk to first get the unique file IDs of the 3000 submissions.csv files, create an empty submissions data-frame, and then download each file and add its information to the submisisons data-frame. To keep the data-frame as light as possible, after each append I deleted all duplicate submissions.\n\n# get all task_info.csv ID's\n# each row identifies one file in the trash\ntask_info_master &lt;- drive_find(\n  pattern = \"task_info.csv\",\n  trashed = TRUE\n)\n\n\n# set up empty df to store all submissions\norigin &lt;- tibble(\n  score = numeric(),\n  task = character(),\n  team = character(),\n  email = character(),\n  time = character()\n)\n\n# downloads, reads, and returns one csv file given a file id\nget_file &lt;- function(row) {\n  tf &lt;- tempfile()\n  row %&gt;%\n    as_id() %&gt;%\n    drive_download(path = tf)\n  new &lt;- read_csv(tf) %&gt;%\n    select(score, task, team) %&gt;%\n    distinct()\n}\n\n\n# quick and dirty for loop to subsequently download each file, extract information\n#  merge with previous information and squash it (using distinct()).\nfor (i in 1:nrow(task_info_master)) {\n  origin &lt;- rbind(origin, get_file(row = task_info_master[i, ])) %&gt;%\n    distinct()\n\n  # save progress in a separate file after every 100 downloaded and merged sheets\n  if (i %% 100 == 0) {\n    print(i)\n    write_csv(origin, paste(\"prog_data/task_info_prog_\", i, \".csv\", sep = \"\"))\n    # update on progress\n    message(i / nrow(task_info_master) * 100)\n  }\n}\n\nWhen doing such a time-intensive task, make sure to try it first with only a couple of files to see whether any errors are produced. I am not quite sure how long this took but when I returned from my lunch break everything had finished.\n\n\nIf you want to stay in the Google framework, I recommend using the googlesheets4 package for structured data. googlesheets4 allows appending new information to an already existing sheet without the need to download the file first. As both packages follow the same style, going from one to the other is really simple. googlesheets4 requires authentication as well. However, you can reuse the cached token from the googledrive package authentication by setting gs4_auth(token = drive_token()).\n\n\nSecurity Concerns\nConnecting a public shiny app to your Google account introduces a security vulnerability in general. Especially so because we implemented the upload of files to Google drive. And even more problematic: We run a user generated script and display some of its output. A malicious party might be able to extract the authentication token of our Google account or could upload malware to the drive.\nTo reduce the risk, I simply created an un-associated Google account to host the drive. There are certainly better options available, but this seemed a reasonable solution achieved with very little effort."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#register-team",
    "href": "posts/2023-06-27__hackathon_app/index.html#register-team",
    "title": "Hackathon Feedback Application",
    "section": "Register Team",
    "text": "Register Team\nWe wanted to allow users to sign up as teams using the shiny app. The app provides a simple interface where users could input a team name and the number of members. This in turn would open two fields for each user to input their name and email address.\nWe do simple checks to make sure at least one valid email address is supplied, and that the group name is acceptable. The group name cannot be empty, already taken, or contain vulgar words.\nThe team registration itself was adding the team information to the Google sheets file event_info into the sheet teams and to create a team folder in which to store the uploaded R files.\nThe checks and registration is implemented in the register_team() function stored in interact_with_google.R.\n\n\n\nScreenshot of the register team interface\n\n\nThe challenge here was to adapt the number of input fields depending on the number of team members. This means that the team name and email interface must be rendered: First, we check how many team members are part of the group, this is stored in the input$n_members input variable. Then we create a tagList with as many elements as team members. Each element contains two columns, one for the email, one for the member name. This tagList is then returned and displayed to the user.\n\n# render email input UI of the register tab\noutput$name_email &lt;- shiny::renderUI({\n  # create field names\n  N &lt;- input$n_members\n  NAME &lt;- sapply(1:N, function(i) {\n    paste0(\"name\", i)\n  })\n  EMAIL &lt;- sapply(1:N, function(i) {\n    paste0(\"email\", i)\n  })\n\n  output &lt;- tagList()\n\n\n  firstsecondthird &lt;- c(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\")\n  for (i in 1:N) {\n    output[[i]] &lt;- tagList()\n    output[[i]] &lt;- fluidRow(\n      shiny::h4(paste(firstsecondthird[i], \" Member\")),\n      column(6,\n        textInput(NAME[i], \"Name\"),\n        value = \" \" # displayed default value\n      ),\n      column(6,\n        textInput(EMAIL[i], \"Email\"),\n        value = \" \"\n      )\n    )\n  }\n  output\n})\n\nThe team information is then uploaded to Google drive. Because some teams have more members than others, we have to create the respective data-frame with the number of team members in mind.\nThe following chunk creates the registration data. Noteworthy here the creation of the NAME and EMAIL variables which depend on the number of members in this team. Further, the user input of these fields is extracted via input[[paste0(NAME[i])]] within a for-loop.\nWe also make the data-creation dependent on the press of the Register Group button and cache some variables.\n\n## registration\nregistrationData &lt;-\n  reactive({\n    N &lt;- input$n_members\n    NAME &lt;- sapply(1:N, function(i) {\n      paste0(\"name\", i)\n    })\n    EMAIL &lt;- sapply(1:N, function(i) {\n      paste0(\"email\", i)\n    })\n    names &lt;- character(0)\n    emails &lt;- character(0)\n\n    for (i in 1:N) {\n      names[i] &lt;- input[[paste0(NAME[i])]]\n      emails[i] &lt;- input[[paste0(EMAIL[i])]]\n    }\n    # create df\n    dplyr::tibble(\n      team_name = input$team_name,\n      n_members = N,\n      member_name = names,\n      member_email = emails\n    )\n  }) %&gt;%\n  bindCache(input$team_name, input$n_members, input$name1, input$email1) %&gt;%\n  bindEvent(input$register) # wait for button press"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#upload-source-script",
    "href": "posts/2023-06-27__hackathon_app/index.html#upload-source-script",
    "title": "Hackathon Feedback Application",
    "section": "Upload & Source Script",
    "text": "Upload & Source Script\nTo upload a script, participants had to select their team first. The input options were based on the existing folders on the Google-drive in the mock_hackathon folder. To upload a particular script participants had to also select the task to be solved. The uploaded script is then uploaded to the team folder following a standardised script naming convention.\nThere are different aspects to be aware of when sourcing scripts on a shiny server. For example, you have to anticipate the packages users will include in their uploaded scripts, as their scripts will load but not install packages. Further, you should keep the global environment of your shiny app separate from the environment in which the script is sourced. This is possible by supplying an environment to the source() function, e.g: source(path_to_script, local = new.env())\nAnother thing we had to consider was to replicate the exact folder-structure on the shiny server that participants were working with when creating the scripts, as they were required to source some scripts and to save their file into a specific folder. This was relatively straight forward as we provided participants with a folder structure in the posit cloud instance they were using. They had access to the sdtm folder in which the data was stored, and the adam folder into which they saved their solutions. The structure also included a folder with metadata which was also available on the shiny server.\nFor some tasks, participants required some ADaM-datasets stored in the adam folder, essentially the output from previous tasks. This was achieved by first creating a list mapping tasks to the required ADaM datasets:\n\ndepends_list &lt;- list(\n  \"ADADAS\" = c(\"ADSL\"),\n  \"ADAE\" = c(\"ADSL\"),\n  \"ADLBC\" = c(\"ADSL\"),\n  \"ADLBH\" = c(\"ADSL\"),\n  \"ADLBHY\" = c(\"ADSL\"),\n  \"ADSL\" = NULL,\n  \"ADTTE\" = c(\"ADSL\", \"ADAE\"),\n  \"ADVS\" = c(\"ADSL\")\n)\n\nThis list is sourced from the R/parameters.R file when initiating the application. We then call the get_depends() function sourced from R/get_depends.R which copies the required files from the key folder (where our solutions to the tasks were stored) to the adam folder. After sourcing the uploaded script the content in the adam folder is deleted."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#compare-to-solution-file",
    "href": "posts/2023-06-27__hackathon_app/index.html#compare-to-solution-file",
    "title": "Hackathon Feedback Application",
    "section": "Compare to Solution File",
    "text": "Compare to Solution File\nWe want to compare the file created by participants with our solution (key) file stored in the key folder. The diffdf::diffdf() function allows for easy comparison of two data-frames and directly provides extensive feedback for the user:\n\nlibrary(dplyr)\ndf1 &lt;- tibble(\n  numbers = 1:10,\n  letters = LETTERS[1:10]\n)\ndf2 &lt;- tibble(\n  numbers = 1:10,\n  letters = letters[1:10]\n)\n\ndiffdf::diffdf(df1, df2)\n\nWarning in diffdf::diffdf(df1, df2): \nNot all Values Compared Equal\n\n\nDifferences found between the objects!\n\nSummary of BASE and COMPARE\n  ==================================================================\n    PROPERTY             BASE                       COMP            \n  ------------------------------------------------------------------\n      Name                df1                        df2            \n     Class     \"tbl_df, tbl, data.frame\"  \"tbl_df, tbl, data.frame\" \n    Rows(#)               10                         10             \n   Columns(#)              2                          2             \n  ------------------------------------------------------------------\n\n\nNot all Values Compared Equal\n  =============================\n   Variable  No of Differences \n  -----------------------------\n   letters          10         \n  -----------------------------\n\n\n  ========================================\n   VARIABLE  ..ROWNUMBER..  BASE  COMPARE \n  ----------------------------------------\n   letters         1         A       a    \n   letters         2         B       b    \n   letters         3         C       c    \n   letters         4         D       d    \n   letters         5         E       e    \n   letters         6         F       f    \n   letters         7         G       g    \n   letters         8         H       h    \n   letters         9         I       i    \n   letters        10         J       j    \n  ----------------------------------------"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#score",
    "href": "posts/2023-06-27__hackathon_app/index.html#score",
    "title": "Hackathon Feedback Application",
    "section": "Score",
    "text": "Score\nTo compare submissions between participants we implemented a simple scoring function (score_f()) based on the table comparison by diffdf(). The function can be found in the compare_dfs.R file:\n\nscore_f &lt;- function(df_user, df_key, keys) {\n  score &lt;- 10\n  diff &lt;- diffdf::diffdf(df_user, df_key, keys = keys)\n  if (!diffdf::diffdf_has_issues(diff)) {\n    return(score)\n  }\n\n  # check if there are any differences if the comparison is not strict:\n  if (!diffdf::diffdf_has_issues(diffdf::diffdf(df_user,\n    df_key,\n    keys = keys,\n    strict_numeric = FALSE,\n    strict_factor = FALSE\n  ))) {\n    # if differences are not strict, return score - 1\n    return(score - 1)\n  }\n\n  return(round(min(max(score - length(diff) / 3, 1), 9), 2))\n}\n\nEvery comparison starts with a score of 10. We then subtract the length of the comparison object divided by a factor of 3. The length of the comparison object is a simplified way to represent the difference between the two data-frames by one value. Finally, the score is bounded by 1 using max(score, 1).\nThe score is not a perfect capture of the quality of the script uploaded but: 1. helped participants get an idea of how close their data-frame is to the solution file 2. allowed us to raffle prizes based on the merit of submitted r-scripts"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#reactiveness",
    "href": "posts/2023-06-27__hackathon_app/index.html#reactiveness",
    "title": "Hackathon Feedback Application",
    "section": "Reactiveness",
    "text": "Reactiveness\nSome of the app functions can take quite some time to execute, e.g. running the uploaded script. Other tasks, e.g. registering a team, do not intrinsically generate user facing outputs. This would make the app using really frustrating, as users would not know whether the app is correctly working or whether it froze.\nWe implemented two small features that made the app more responsive. One is simple loading icons that integrate into the user interface and show that output is being computed – that something is working. The other is a pop up window which communicates whether team registration was successful, and if not, why not.\nWe further aimed to forward errors generated by the uploaded scripts to the user interface, but errors generated by the application itself should be concealed."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#conclusion",
    "href": "posts/2023-06-27__hackathon_app/index.html#conclusion",
    "title": "Hackathon Feedback Application",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough the application was continuously improved during the hackathon it proved to be a useful resource for participants from day one as it allowed groups to set their own pace. It further allowed admiral developers to gain insights on package usage of a relatively large sample of potential end users. From our perspective, the application provided a great added value to the hackathon and eased the workload of guiding the participants through all the tasks."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#last-updated",
    "href": "posts/2023-06-27__hackathon_app/index.html#last-updated",
    "title": "Hackathon Feedback Application",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:34.425512"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#details",
    "href": "posts/2023-06-27__hackathon_app/index.html#details",
    "title": "Hackathon Feedback Application",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html",
    "href": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html",
    "title": "Undergraduate University Statistics Report using pharmaverseadam data",
    "section": "",
    "text": "As part of my placement year as a Data Sciences Industrial Placement student in Biostatistics at Roche in the UK, I was required to produce a “Business Project” and present it to the entire Data Sciences department. I decided to use pharmaverseadam to design a brand-new R training, “trainStats”, for junior Biostatisticians, since the package includes realistic ADaM datasets that are ideal for statistical analyses. For maximum efficiency, I tied my business project with a quantitative project report, due August 2024, for my undergraduate degree in Mathematics, Operational Research and Statistics at Cardiff University.\nThe quantitative project report investigates statistical analyses on preliminary clinical trial data using the R Studio software as instructed by the trainStats program I have authored, to help ease new Biostatisticians in the industry. The software was built considering the needs of people who are new to the industry and are keen to pursue a career in Biostatistics.\nI had a smooth experience with the   {pharmaverseadam}  package all throughout my business and university project. I was introduced to the package by Ross Farrugia while looking for open-source data to analyze for my project. The package was very easy to read and use, with excellent documentation on the pharmaverseadam website. As I was planning to share aggregated outputs (such as tables, listings and graphs) from clinical datasets externally to the university, even using historical clinical data was not allowed since external use of confidential data did not align with Roche’s data privacy principles.\nThroughout the trainStats documentation, I have primarily used the adoe_ophtha ADaM dataset (containing ophthalmology safety data) from   {pharmaverseadam}  to allow for a variety of exploratory statistical analyses ranging from producing boxplots of the spread of data by visit day, computing standard deviation and confidence intervals for endpoints, as well as programming linear regression models and patient profiles. As adoe_ophtha contains visit day, active arm and endpoint data, it was ideal to use for training purposes. In addition, I did use the adsl dataset too, to encourage trainStats users to join and merge datasets, taking into account patient demographics such as age. Here is a snippet of the code I wrote to generate bar charts by active arm for the “Central Subfield Thickness” endpoint:\n# For Central Subfield Thickness\nadoe_CST$ARM &lt;- factor(adoe_CST$ARM, levels = c(\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"))\n\nggplot(data = subset(adoe_CST, ARM != \"Screen Failure\"), aes(x = ARM, y = AVAL)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Active Arm\") +\n  ylab(\"Central Subfield Thickness / um\") +\n  ylim(0, max(adoe_CST$AVAL))\nBelow, is another example of the code I wrote to produce a boxplot displaying the analysis value of Central Subfield Thickness by patient visit days:\nadoe_CST &lt;- adoe_ophtha %&gt;%\n  filter(PARAMCD == \"SCSUBTH\")\n\nadoe_DR &lt;- adoe_ophtha %&gt;%\n  filter(PARAMCD == \"SDRSSR\")\n\n# Boxplots for each visit day\nboxplot(AVAL ~ AVISITN,\n  data = adoe_CST,\n  main = \"Different boxplots for each visit day\",\n  xlab = \"Visit Number\",\n  ylab = \"Central Subfield Thickness/ um\",\n  col = \"orange\",\n  border = \"brown\"\n)\nAs you can see above, both of these code snapshots display the importance of clear logic and reasoning whilst coding by implementing strong data visualization techniques such as commenting. The code is simple and I personally found that using the   {pharmaverseadam}  package to produce various plots was very straightforward. The objective of trainStats was to help users familiarise themselves with ADaM datasets and my favorite element of the package was that the format of both synthetic ADaM datasets were incredibly similar to that of a true clinical trial ADaM for a study in Ophthalmology.\nTo further develop and improve the   {pharmaverseadam}  package, I believe including more endpoints in the adoe_ophtha dataset would be invaluable for future application and statistical analyses. Often ADOE datasets have several endpoints but the adoe_ophtha dataset only included 2 clinical parameters, namely “Central Subfield Thickness” and “Diabetic Retinopathy Severity Scale”. In addition, since the data is synthetic and randomly generated, the outputs had no significant correlations or trends from a statistical perspective in terms of disease progression or measures of central tendencies. Although, in this case, the emphasis was on understanding logic and reasoning whilst programming the statistical outputs, I experienced difficulties analysing the data quantitatively in my university report due to the high variation in data. Going forward, if there is a method to simulate the data less randomly, then that may be more useful for future dummy analyses on   {pharmaverseadam}  data.\nOverall, my experience of using the   {pharmaverseadam}  package for the first time was excellent. The package was convenient to use in R Studio, and clearly formatted for multi-purpose use. I would definitely recommend using   {pharmaverseadam}  to all users in the industry, who are required to produce a piece of project work or any analyses/summary for external use, or even those keen to publicly publish articles and papers in their areas within pharma to the wider community, in a safe and responsible manner regarding external use of data. I would like to thank Ross Farrugia for introducing me to the package, and especially Edoardo Mancini for talking me through the package and supporting me throughout the business project and university report."
  },
  {
    "objectID": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html#last-updated",
    "href": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html#last-updated",
    "title": "Undergraduate University Statistics Report using pharmaverseadam data",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:38.269794"
  },
  {
    "objectID": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html#details",
    "href": "posts/2024-09-16_university_undergraduate_report/university_undergraduate_report.html#details",
    "title": "Undergraduate University Statistics Report using pharmaverseadam data",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html",
    "title": "TLG Catalog 🤝 WebR",
    "section": "",
    "text": "TLG Catalog website"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#what-is-webr",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#what-is-webr",
    "title": "TLG Catalog 🤝 WebR",
    "section": "What is WebR?",
    "text": "What is WebR?\n\nWebR makes it possible to run R code in the browser without the need for an R server to execute the code: the R interpreter runs directly on the user’s machine.\n\nSource: WebR documentation\nIn short, WebR is a project that aims to port R into WebAssembly (WASM) which then allows to run compiled code in the website. A special thanks to George Stagg from Posit for making this integration possible. While WebR is still in active development, a significant progress had been made recently increasing its robustness and efficiency.\nHowever, it’s important to note a limitation: not all packages are compatible with WebR. A package must be compiled for WebAssembly to be used with WebR. Fortunately, there’s a dedicated WebR binary R package repository hosting close to 20,000 packages. For packages not yet available, you can utilize a dedicated GitHub Actions workflow to build them yourself, or use r-universe platform that will build it for you."
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#implementation-details",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#implementation-details",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Implementation Details",
    "text": "Implementation Details\nThe integration of WebR into TLG Catalog was made possible through a dedicated quarto-webr Quarto extension, which simplifies the integration process. The main challenge was to ensure a DRY (Don’t Repeat Yourself) approach with respect to the existing codebase. This was achieved through leveraging lesser-known knitr features, including knitr::knit_code$get() to reuse code chunks as well as results = \"asis\" to create code chunk from within another (parent) code chunk. The source code for this is open-source and available on GitHub."
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#interactive-teal-applications-via-shinylive",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#interactive-teal-applications-via-shinylive",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Interactive teal Applications via shinylive",
    "text": "Interactive teal Applications via shinylive\nThe benefits of WebR extend beyond TLG outputs. It also enhances all existing teal applications. Users can now interact with applications and even live-edit their source code! Everything is inside the website itself without any additional application hosting service. This was made possible through the shinylive Quarto extension leveraging Shinylive under the hood. A huge thank you to the Shiny team for their contributions!"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#summary",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#summary",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Summary",
    "text": "Summary\nThe addition of interactivity via WebR marks a significant milestone for TLG Catalog. This update unlocks a myriad of possibilities previously unavailable, such as live code editing, step-by-step code execution, access to function documentation, and dynamic data exploration. This advancement brings R closer to users, especially those new to the language, fostering a more engaging and effective learning experience.\nHappy learning!"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#last-updated",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#last-updated",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:41.376865"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#details",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#details",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html",
    "href": "posts/2023-07-14_code_sections/code_sections.html",
    "title": "How to use Code Sections",
    "section": "",
    "text": "The admiral package embraces a modular style of programming, where blocks of code are pieced together in sequence to create an ADaM dataset. However, with the well-documented advantages of the modular approach comes the recognition that scripts will on average be longer. As such, astute programmers working in RStudio are constantly on the lookout for quick ways to effectively navigate their scripts. Enter code sections!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#introduction",
    "href": "posts/2023-07-14_code_sections/code_sections.html#introduction",
    "title": "How to use Code Sections",
    "section": "",
    "text": "The admiral package embraces a modular style of programming, where blocks of code are pieced together in sequence to create an ADaM dataset. However, with the well-documented advantages of the modular approach comes the recognition that scripts will on average be longer. As such, astute programmers working in RStudio are constantly on the lookout for quick ways to effectively navigate their scripts. Enter code sections!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#so-what-are-code-sections-and-why-are-they-useful",
    "href": "posts/2023-07-14_code_sections/code_sections.html#so-what-are-code-sections-and-why-are-they-useful",
    "title": "How to use Code Sections",
    "section": "So, what are code sections and why are they useful?",
    "text": "So, what are code sections and why are they useful?\nCode Sections are separators for long R scripts or functions in RStudio. They can be set up by inserting a comment line followed by four or more dashes in between portions of code, like so:\n\n# First code section ----\n\na &lt;- 1\n\n# Second code section ----\n\nb &lt;- 2\n\n# Third code section ----\n\nc &lt;- 3\n\nRStudio then recognizes the code sections automatically, and enables you to:\n\nCollapse and expand them using the arrow displayed next to the line number, or with the handy shortcuts Alt+L/Shift+Alt+L on Windows or Cmd+Option+L/Cmd+Shift+Option+L on Mac.\nTravel in between them using the navigator at the bottom of the code pane, or by pressing Shift+Alt+J on Windows or Cmd+Shift+Option+J on Mac.\nView an outline of the file using the “Outline” button at the top right of the pane and/or the orange hashtag “Section Navigator” button at the bottom left of the pane.\n\n\n\n\n\n\nCollapsed sections, outline view and the section navigator for the example above.\n\n\n\n\nIt is also possible to create subsections by using two hashtags at the start of a comment line:\n\n# First code section ----\na &lt;- 1\n\n## A code subsection ----\nb &lt;- 2\n\n# Second code section ----\nc &lt;- 3\n\n\n\n\n\n\nCode subsections for the example above.\n\n\n\n\nFor a complete list of Code Sections shortcuts, and for further information, see here."
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#conclusion",
    "href": "posts/2023-07-14_code_sections/code_sections.html#conclusion",
    "title": "How to use Code Sections",
    "section": "Conclusion",
    "text": "Conclusion\nCode sections are an easy way to navigate long scripts and foster good commenting practices. They are used extensively in the admiral package, but there is no reason that you cannot start using them yourself in your day-to-day R programming!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#last-updated",
    "href": "posts/2023-07-14_code_sections/code_sections.html#last-updated",
    "title": "How to use Code Sections",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:28:45.996833"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#details",
    "href": "posts/2023-07-14_code_sections/code_sections.html#details",
    "title": "How to use Code Sections",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html",
    "href": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html",
    "title": "Creating a better universe with dverse",
    "section": "",
    "text": "This article shows how to create a global search for any collection of R packages, i.e. a universe. Your universe may be on CRAN, large, and popular like the tidyverse or pharmaverse. But it may also be not on CRAN, small, company-specific or personal.\nThe example here uses the ‘admiralverse’ – a toy universe defined as every package on CRAN today (2024-12-07), with a website, and a name starting with ‘admiral’.\nYou need:\nlibrary(dverse)\nlibrary(admiral)\nlibrary(admiraldev)\nlibrary(admiralonco)\nlibrary(admiralophtha)\nlibrary(admiralpeds)\nlibrary(admiralvaccine)\nlibrary(dplyr)\nlibrary(DT)\ndverse::document_universe()  takes the names of the packages in the ‘admiralverse’, and a template pointing to each help file (topic) in each package.\nadmiralverse &lt;- c(\n  \"admiral\",\n  \"admiraldev\",\n  \"admiralonco\",\n  \"admiralophtha\",\n  \"admiralpeds\",\n  \"admiralvaccine\"\n)\n# For example: https://pharmaverse.github.io/admiral/reference/queries.html\ntemplate &lt;- \"https://pharmaverse.github.io/{package}/reference/{topic}.html\"\n\ndocs &lt;- dverse::document_universe(admiralverse, template)\ndocs\n## # A tibble: 346 × 7\n##    topic                               alias title concept type  keyword package\n##    &lt;chr&gt;                               &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n##  1 &lt;a href=https://pharmaverse.github… %&gt;%   Pipe… reexpo… help  &lt;NA&gt;    admiral\n##  2 &lt;a href=https://pharmaverse.github… %not… Nega… dev_ut… help  &lt;NA&gt;    admira…\n##  3 &lt;a href=https://pharmaverse.github… %or%  Or    dev_ut… help  &lt;NA&gt;    admira…\n##  4 &lt;a href=https://pharmaverse.github… adbc… Crea… &lt;NA&gt;    vign… &lt;NA&gt;    admira…\n##  5 &lt;a href=https://pharmaverse.github… adce  Crea… &lt;NA&gt;    vign… &lt;NA&gt;    admira…\n##  6 &lt;a href=https://pharmaverse.github… add_… Add … quo     help  &lt;NA&gt;    admira…\n##  7 &lt;a href=https://pharmaverse.github… adfa… Crea… &lt;NA&gt;    vign… &lt;NA&gt;    admira…\n##  8 &lt;a href=https://pharmaverse.github… adis  Crea… &lt;NA&gt;    vign… &lt;NA&gt;    admira…\n##  9 &lt;a href=https://pharmaverse.github… admi… admi… intern… help  intern… admiral\n## 10 &lt;a href=https://pharmaverse.github… admi… Lab … datase… help  datase… admiral\n## # ℹ 336 more rows\nYou may need to exclude some topics, such as reexported objects from other packages. Because topics outside your universe yield broken links, you can exclude them with   dverse::is_online() .\ndocs |&gt; filter(!dverse::is_online(topic))\n## # A tibble: 9 × 7\n##   topic                                alias title concept type  keyword package\n##   &lt;chr&gt;                                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n## 1 &lt;a href=https://pharmaverse.github.… %&gt;%   Pipe… reexpo… help  &lt;NA&gt;    admiral\n## 2 &lt;a href=https://pharmaverse.github.… %not… Nega… dev_ut… help  &lt;NA&gt;    admira…\n## 3 &lt;a href=https://pharmaverse.github.… %or%  Or    dev_ut… help  &lt;NA&gt;    admira…\n## 4 &lt;a href=https://pharmaverse.github.… anti… Join… joins   help  &lt;NA&gt;    admira…\n## 5 &lt;a href=https://pharmaverse.github.… deat… Pre-… source… help  &lt;NA&gt;    admiral\n## 6 &lt;a href=https://pharmaverse.github.… deat… Pre-… source… help  &lt;NA&gt;    admira…\n## 7 &lt;a href=https://pharmaverse.github.… desc  dply… reexpo… help  &lt;NA&gt;    admiral\n## 8 &lt;a href=https://pharmaverse.github.… exprs rlan… reexpo… help  &lt;NA&gt;    admiral\n## 9 &lt;a href=https://pharmaverse.github.… rsp_… Pre-… source… help  &lt;NA&gt;    admira…\n\nonline &lt;- docs |&gt; filter(dverse::is_online(topic))\nConsider the different kinds of documentation in your universe.\nonline |&gt; count(type, keyword)\n## # A tibble: 4 × 3\n##   type     keyword      n\n##   &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt;\n## 1 help     datasets    22\n## 2 help     internal    30\n## 3 help     &lt;NA&gt;       245\n## 4 vignette &lt;NA&gt;        40\nYou may organize the documentation however you like. For inspiration see the pactaverse, the toy ‘admiralverse’, fgeo and tidymodels.\nHere it makes sense to place vignettes, datasets, and public functions in separate tables."
  },
  {
    "objectID": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html#last-updated",
    "href": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html#last-updated",
    "title": "Creating a better universe with dverse",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:08.576979"
  },
  {
    "objectID": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html#details",
    "href": "posts/2024-12-13_document_yo.../document_your_universe_with_dverse.html#details",
    "title": "Creating a better universe with dverse",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html",
    "href": "posts/2023-06-27_hackathon_writeup/index.html",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "",
    "text": "This January and February (2023), the admiral development team and the CDISC Open Source Alliance jointly hosted the admiral hackathon. The idea was to build a community of admiral users, and help participants familiarize themselves with R and admiral. This whole effort was led by Thomas Neitmann and was supported by Zelos Zhu, Sadchla Mascary, and me – Stefan Thoma.\nThe hackathon event was structured in two parts. First, we offered an Introduction to R for SAS programmers, a three hour workshop for R beginners to get them up to speed. Here we covered practical R basics, talking about how the R-workflow differs from a SAS workflow, and discussed common R functions - mostly from the tidyverse. This ensured that hackathon participants were familiar with core R concepts. The workshop recording and the course materials are available online.\nThe main hackathon consisted of several ADAM data generating tasks based on a specs file and synthetic data. Participants were able to solve these tasks in groups at their own pace thanks to a online tool where participants could upload their task specific R scripts and they would get automatic feedback for the data-set produced by their script. Script upload through the feedback application was available all through February, and we offered three additional online meetings throughout the month to discuss challenges and give some tips. If you are interested in learning more about the thoughts that went into the feedback application, you can read about it in this blogpost or check out my public GitHub repository for such an application."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#introduction-to-r-workshop",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#introduction-to-r-workshop",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Introduction to R workshop",
    "text": "Introduction to R workshop\nWe were really excited to see over 500 people from around 40 countries joining our Introduction to R workshop in January! To get to know prospective users and hackathon participants better, we conducted some polls during the meetings. Below you can see that representatives of many different sorts of organisations joined our Introduction to R workshop:\n\n\n\n\n\n\n\n\n\n216 out of 402 confirmed that their company is already using R for clinical trial data analysis, the remaining 131 did not answer this question.\nThe target audience for this workshop was programmers who are very familiar with SAS, but not so familiar with R, our polls confirmed this.\n\n\n\n\n\n\n\n\n\nOverall, we were very happy with how the workshop turned out, and participants overall agreed with this sentiment (although there may be a slight survivorship bias…)."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#admiral-hackathon",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#admiral-hackathon",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "admiral Hackathon",
    "text": "admiral Hackathon\nFollowing the kick-off meeting, 371 participants joined the posit (rStudio) workspace that was made available to all participants at no costs by the posit company. About half the participants planned to spend one to two hours per week on the admiral tasks, the other half planned to allocate even more. 15 participants even planned to spend eight hours or more!\nWe were really happy to see an overwhelming amount of activity on the slack channel we set up with over 250 members. Not only were people engaging with the materials, but we saw how a community was formed where people were encouraged to ask questions and where community members went out of their way to help each other. Shout-out to our community hero: Jagadish Katam without whom most issues related to the task programming raised by the community would not have been addressed as quickly as they were. Huge thanks from the organizers!\nIn the end, a total of 44 teams spanning 87 statistical programmers took part in the admiral hackathon and uploaded solution scripts to the hackathon application solving at least one of the 8 tasks available (ADSL, ADAE, ADLBC, ADVS, ADTTE, ADADAS, ADLBH & ADLBHY). Participants’ scripts were then run on the shiny server and the output data-frame were compared to the solutions we provided. At the read-out there was a live draft of teams to win one-on-one admiral consulting with one of the admiral core developers. Winning probabilities were weighted by the number of points each group received for the quality of their output data-frames and for the number of tasks solved.\nCongratulations to the winners:\n\nViiV Team_GSK\nteamspoRt\nTatianaPXL\nDivyasneelam\nAdaMTeamIndia\nSanofi_BP\nJagadish (our community hero)\nAZ_WAWA\n\nAlthough this was uncertain during the hackathon we were excited to provide a Certificate of Completion to all participants who uploaded a script to the Web Application.\nA recording of the hackathon readout can be found in the CDISC Open Source Alliance Quarterly Spotlight."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#conclusion",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#conclusion",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, we are very happy with how the hackathon turned out. We were not only positively surprised with the huge audience for the Intro to R workshop (CDISC record breaking) and for the admiral hackathon, but even more so with the engagement of all the participants.\nAgain, we would like to thank all the organizers, participants, and sponsors for their time and resources and hope to have provided a useful glimpse into our solution for ADAM creation within the end-to-end clinical data analysis open source R framework that the pharmaverse aims to provide.\nAs always, we are very happy to hear more feedback on the hackathon as well as on admiral in general. Simply submit an issue on the admiral GitHub repository. You would like to join the admiral core developers? Please reach out to Edoardo Mancini (product owner) or Ben Straub (technical lead)."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#last-updated",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#last-updated",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:14.480675"
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#details",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#details",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "",
    "text": "Pic 1: ISCR 17th Annual Conference 2024\n\n\nIndian Society for Clinical Research (ISCR), launched in June 2005, is a not-for-profit professional association of all stakeholders in clinical research.\nISCR hosted its 17th Annual Conference 2024 at Hotel Novotel HICC, Hyderabad, INDIA on the theme Transformations in Clinical Research For Better Patient Outcomes, with Pre-Conference Workshops held on February 1, 2024 (Thursday) and two-day main Conference held on February 02 & 03, 2024 (Friday-Saturday), which were attended by over 800 delegates from academic institutions, ethics committees, bio-pharmaceutical industry, government, patient organizations and clinical research organizations."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#iscr-17th-annual-conference-2024",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#iscr-17th-annual-conference-2024",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "",
    "text": "Pic 1: ISCR 17th Annual Conference 2024\n\n\nIndian Society for Clinical Research (ISCR), launched in June 2005, is a not-for-profit professional association of all stakeholders in clinical research.\nISCR hosted its 17th Annual Conference 2024 at Hotel Novotel HICC, Hyderabad, INDIA on the theme Transformations in Clinical Research For Better Patient Outcomes, with Pre-Conference Workshops held on February 1, 2024 (Thursday) and two-day main Conference held on February 02 & 03, 2024 (Friday-Saturday), which were attended by over 800 delegates from academic institutions, ethics committees, bio-pharmaceutical industry, government, patient organizations and clinical research organizations."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#session-recap",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#session-recap",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Session Recap",
    "text": "Session Recap\nI had the privilege to present in front of 100+ delegates across the industry ranging from freshers to seasoned clinical professionals during ISCR 17th Annual Conference 2024 on the topic Travese the PHARMAVERSE: ouR Insights in the Biostatistics and Statistical Programming|02-Feb-2024 track including many more interesting presentations highlighting their experience with R submissions using various open source technologies.\n\n\n\nPic 2: Source - LinkedIn\n\n\nThe session consisted of three presentations, namely:\n\nA real world insight and navigation on bridging FDA submission using R by Soumitra Kar & Mahendran Venkatachalam\nTraverse the ‘PHARMAVERSE’ : ouR insights by Pooja Kumari\nPackage in CRAN : {admiralvaccine} by Divya Kanagaraj and Arjun R\n\nIt was inaugurated with great enthusiast and sharing insights on Opportunities/Challenges of using different technologies like R in regulatory Submissions by the session chair Soumitra Kar. He along with his co-presenter Mahendran Venkatachalam shared their experience of submitting first R-based Submission to FDA. The presentation was a perfect combination of inspiring storytelling, climax and thrill to address FDA review comments and releasing the blockbuster R submission by Novo-Nordisk creating history.\nThis was followed by my presentation on Travese the PHARMAVERSE: ouR Insights, wherein I gave a brief introduction to PHARMAVERSE universe and how we operate. Many R enthusiasts are well versed with the evolution of {admiral} and its propensity to develop ADaMs. However, very few know about other packages such as {metacore}, {metatools}, {xportr} which are developed considering the regulatory agency guidelines and can aid the process of creating ADaM datasets proficiently. I took the opportunity to supercharge the process knowledge of creating submission ready ADaMs covering end-to-end process using these PHARMAVERSE packages along with some to R submission success stories.\n\n\n\nPresentation: Travese the PHARMAVERSE: ouR Insights\n\n\nNext presentation was on {admiralvaccine}, an extension package of {admiral} specific to vaccine studies under the PHARMAVERSE universe by Divya Kanagaraj and Arjun R. They shared their exciting journey of developing the package since inception to final CRAN release from a developer’s perspective. They also talked about the collaborative effort that went into its successful release.\nOverall session was concluded with an interactive Q&A wherein all the presenters and presentations were applauded by the audience as well as the Scientific Committee members. It was an enriching session to witness the growth of R programming leading to R submissions in Clinical Research & Pharmaceutical Industry.\n\n\n\nPic 3: Biostatistics and Statistical Programming | 02-Feb-2024, Session 4, Audience Q&A round, Left to right: Pooja Kumari, GSK; Dhivya Kanagaraj, Pfizer; Arjun Rubalingam, Pfizer; Soumitra Kar, Novo Nordisk; Mahendran Venkatachalam, Novo Nordisk"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#key-takeaways",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#key-takeaways",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe two-day conference was full of great learning and meeting esteemed Clinical Pharmaceutical Industry veterans/newbies discussing on trending topics such as Optimizing Clinical Research through effective collaboration between Statisticians and Statistical Programmers, Can new technologies (AI/ML/IOT) a threat or blessing for Biostatisticians and Statistical Programmers? through Panel discussions.\nThe power of technology coupled with domain expertise can make us deliver quality results faster and serve the world with disease-free healthy life.\n\nWhy should we attend Conferences?\n\nConferences are the best place to Connect, Collaborate and Communicate your thoughts with like-minded tribe.\nBiostatistics and Clinical Statistical Programming industry is growing and adopting open source technologies with great acceptance. As an individual we can contribute to communities like PHARMAVERSE to enhance our end-to-end process knowledge, develop programming skills and contribute to a revolutionary concept.\nIt gives you a platform to strengthen your presentation as well as self-branding skills."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#gallery",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#gallery",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Gallery",
    "text": "Gallery\n\n\n\n\n\n\n\n\n\nPic 4: GCC GSK Biostatistics-India reps. at ISCR, Left to Right: Pooja Kumari; Abhishek Mishra\n\n\n\n\n\n\n\nPic 5: Keep Calm and Explore PHARMAVERSE"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#last-updated",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#last-updated",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:18.596489"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#details",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#details",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html",
    "href": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html",
    "title": "{admiral} 1.1.1 is here!",
    "section": "",
    "text": "{admiral 1.1.1} is out on CRAN! Though it may seem like just yesterday that   {admiral}  achieved its milestone 1.0 release, in actual fact six long months have gone by - testament to our commitment to now release twice-yearly rather than quarterly. During this time, we in the admiral team received lots of positive feedback from our community, but have nevertheless worked tirelessly to improve our package. This new release comes with various quality of life changes, targeted additions to our functions’ capabilities and improvements to our documentation that all contribute to improve each and every user’s experience - all while ensuring that our commitment to stability and a low amount of breaking changes is still met.\nThis blog post will showcase some of the highlights of this release, but you can explore the full contents in our Changelog."
  },
  {
    "objectID": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html#last-updated",
    "href": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html#last-updated",
    "title": "{admiral} 1.1.1 is here!",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:24.027027"
  },
  {
    "objectID": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html#details",
    "href": "posts/2024-06-07_admiral_1.1_is.../admiral_1.1_is_here.html#details",
    "title": "{admiral} 1.1.1 is here!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "",
    "text": "When creating ADaM Basic Data Structure (BDS) datasets, we often encounter deriving a new parameter based on the analysis values (e.g., AVAL) of other parameters.\nThe admiral function derive_param_computed() adds a parameter computed from the analysis value of other parameters.\nIt works like a calculator to derive new records without worrying about merging and combining datasets, all you need is a derivation formula, which also improves the readability of the code."
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#introduction",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#introduction",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "",
    "text": "When creating ADaM Basic Data Structure (BDS) datasets, we often encounter deriving a new parameter based on the analysis values (e.g., AVAL) of other parameters.\nThe admiral function derive_param_computed() adds a parameter computed from the analysis value of other parameters.\nIt works like a calculator to derive new records without worrying about merging and combining datasets, all you need is a derivation formula, which also improves the readability of the code."
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#example",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#example",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Example",
    "text": "Example\nA value level validation use case, where derive_param_computed() is used to validate a derived parameter - PARAMCD = ADPCYMG (Actual Dose per Cycle) in ADEX dataset.\n\nDerivation\nActual Dose per Cycle is derived from the Total Amount of Dose (PARAMCD = TOTDOSE) / Number of Cycles (PARAMCD = NUMCYC)\nIn this example, ADEX.AVAL when ADEX.PARAMCD = ADPCYMG can be derived as:\n\\[\nAVAL (PARAMCD = ADPCYMG) = \\frac{AVAL (PARAMCD = TOTDOSE)}{AVAL (PARAMCD = NUMCYC)}\n\\]\n\n\nLoading Packages and Creating Example Data\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(diffdf)\nlibrary(admiral)\n\nadex &lt;- tribble(\n  ~USUBJID,  ~PARAMCD,  ~PARAM,                       ~AVAL,\n  \"101\",     \"TOTDOSE\", \"Total Amount of Dose (mg)\",  180,\n  \"101\",     \"NUMCYC\",  \"Number of Cycles\",           3\n)\n\n\n\nDerive New Parameter\n\nadex_admiral &lt;- derive_param_computed(\n  adex,\n  by_vars = exprs(USUBJID),\n  parameters = c(\"TOTDOSE\", \"NUMCYC\"),\n  set_values_to = exprs(\n    PARAMCD = \"ADPCYMG\",\n    PARAM = \"Actual Dose per Cycle (mg)\",\n    AVAL = AVAL.TOTDOSE / AVAL.NUMCYC\n  )\n)\n\n\n\n# A tibble: 3 × 4\n  USUBJID PARAMCD PARAM                       AVAL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;\n1 101     TOTDOSE Total Amount of Dose (mg)    180\n2 101     NUMCYC  Number of Cycles               3\n3 101     ADPCYMG Actual Dose per Cycle (mg)    60\n\n\n\n\nCompare\nFor validation purpose, the diffdf package is used below to mimic SAS proc compare.\n\nadex_expected &lt;- bind_rows(\n  adex,\n  tribble(\n    ~USUBJID,  ~PARAMCD,  ~PARAM,                       ~AVAL,\n    \"101\",     \"ADPCYMG\", \"Actual Dose per Cycle (mg)\", 60\n  )\n)\n\ndiffdf(adex_expected, adex_admiral, keys = c(\"USUBJID\", \"PARAMCD\"))\n\nNo issues were found!"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#last-updated",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#last-updated",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:28.011598"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#details",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#details",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html",
    "href": "posts/2023-06-28_welcome/index.html",
    "title": "Hello pharmaverse",
    "section": "",
    "text": "The communications working group (CWG) seeks to promote and showcase how R can be used in the Clinical Reporting pipeline through short and informative blog posts. These posts will be hosted on this pharmaverse blog and promoted on the pharmaverse slack channels as well as on LinkedIn.\nAs the CWG is a small team, we hope to make the blog development process easy enough that pharmaverse community members will be able to easily write blog posts with guidance from the CWG team."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#purpose",
    "href": "posts/2023-06-28_welcome/index.html#purpose",
    "title": "Hello pharmaverse",
    "section": "",
    "text": "The communications working group (CWG) seeks to promote and showcase how R can be used in the Clinical Reporting pipeline through short and informative blog posts. These posts will be hosted on this pharmaverse blog and promoted on the pharmaverse slack channels as well as on LinkedIn.\nAs the CWG is a small team, we hope to make the blog development process easy enough that pharmaverse community members will be able to easily write blog posts with guidance from the CWG team."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#spirit-of-a-blog-post",
    "href": "posts/2023-06-28_welcome/index.html#spirit-of-a-blog-post",
    "title": "Hello pharmaverse",
    "section": "Spirit of a Blog Post",
    "text": "Spirit of a Blog Post\nThe CWG believes that the following 4 points will help guide the creation of Blog Posts.\n\nShort\nPersonalized\nReproducible\nReadable\n\nShort: Posts should aim to be under a 10 minute read. We encourage longer posts to be broken up into multiple posts.\nPersonalized: Posts should have a personality! For example, a person wishing to post on a function in a package needs to differentiate the post from the documentation for function, i.e. we don’t want to just recycle the documentation. How can you add your voice and experience? A bit of cheeky language is also encouraged.\nReproducible: Posts should work with minimal dependencies with data, packages and outside sources. Every dependency introduced in a post adds some risk to the post longevity. As package dependencies change, posts should be built in a way that they can be updated to stay relevant.\nReadable: The CWG sees this site as more of introductory site rather advanced user site. Therefore, the CWG feels that code should be introduced in a way that promotes readability over complexity."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#what-types-of-posts-are-allowed-on-this-site",
    "href": "posts/2023-06-28_welcome/index.html#what-types-of-posts-are-allowed-on-this-site",
    "title": "Hello pharmaverse",
    "section": "What types of posts are allowed on this site?",
    "text": "What types of posts are allowed on this site?\nOverall, we want to stay focus on the Clinical Reporting Pipeline, which we see as the following topics:\n\nPackages in the Clinical Reporting Pipeline\nFunctions from packages in the Clinical Reporting Pipeline\nWider experiences of using R in the Clinical Reporting Pipeline\nConference experiences and the Clinical Reporting Pipeline\n\nHowever, it never hurts to ask if you topic might fit into this medium!\n\nMinimum Post Requirements\n\nA unique image to help showcase the post.\nWorking Code\nSelf-contained data or package data.\nDocumentation of package versions\n\nThat is it! After that you can go wild, but we do ask that it is kept short!"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#how-can-i-make-a-blog-post",
    "href": "posts/2023-06-28_welcome/index.html#how-can-i-make-a-blog-post",
    "title": "Hello pharmaverse",
    "section": "How can I make a Blog Post",
    "text": "How can I make a Blog Post\nStep 1: Reach out to us through pharmaverse/slack or make an issue on our GitHub.\nStep 2: Branch off main\nStep 3: Review the Spirit of the Blog Post in the Pull Request Template\nStep 4: Poke us to do a review!"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#last-updated",
    "href": "posts/2023-06-28_welcome/index.html#last-updated",
    "title": "Hello pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:31.730872"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#details",
    "href": "posts/2023-06-28_welcome/index.html#details",
    "title": "Hello pharmaverse",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html",
    "title": "Inside the pharmaverse",
    "section": "",
    "text": "Greetings, pharmaverse phriends!\nNow that the pharmaverse has been around for a few years and seems to be making an impact on clinical reporting (yay!), we wanted to take an opportunity to cast a light on some of the inner workings of pharmaverse and its Council (us). Fortunately, some of you (you know who you are) innovated and created this fantastic channel for pharmaverse blogs. We can’t think of a better way to communicate than by the channel that the pharmaverse community itself created."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-inception",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-inception",
    "title": "Inside the pharmaverse",
    "section": "Pharmaverse Inception",
    "text": "Pharmaverse Inception\nThe pharmaverse Council was formed in 2020 as a coming together of 4 like-minded individuals embedded in the industry-wide efforts to increase the use of the R language for clinical trial analysis and reporting. These individuals included Ross Farrugia (Roche), Sumesh Kalappurakal (Johnson & Johnson), Michael Rimler (GSK), and Mike Stackhouse (Atorus). Each had historical experience with R adoption, through their primary roles within their respective organizations.\nCollectively, these 4 individuals saw the value in\n\nreducing the choice set of R packages for users,\nidentifying gaps in available R packages for delivering the clinical data pipeline, and\nencouraging the development of new packages and/or features to close those gaps.\n\nThe result would be a subset of permissively licensed and open-sourced packages that anyone could use (or modify) to suit their specific use cases, thereby reducing the incidence of organizations privately solving typical problems in isolation. For more background, please refer to the pharmaverse Charter.\nThe current pharmaverse Council companies include GSK, Roche, Johnson & Johnson, Atorus, and Novo Nordisk, represented by the original four founders and Ari Siggaard Knoph from Novo Nordisk.\nIn March 2023, the PHUSE Board of Directors approached the pharmaverse Council with a proposal offering support to pharmaverse developers and leadership to advance the pharmaverse mission. The PHUSE proposal aligned with its mission to promote “[s]haring ideas, tools and standards around data, statistical and reporting technologies to advance the future of life sciences.” PHUSE is currently a supporter of the pharmaverse and its objectives, evident through activities such as:\n\nthe provision of access to PHUSE’s communication and file-sharing platforms to package teams,\nthe provision of project management support to package teams (if needed),\nenablement and promotion of pharmaverse innovations and activities, and\nenablement of face-to-face community connections at PHUSE events."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-objective",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-objective",
    "title": "Inside the pharmaverse",
    "section": "Pharmaverse Objective",
    "text": "Pharmaverse Objective\nPharmaverse aims to promote the collaborative development of curated open-source R packages for clinical reporting in pharma. Indirectly, we also strive to encourage and increase R adoption within the industry and facilitate communication and collaboration among both R users and R developers."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#becoming-a-council-member",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#becoming-a-council-member",
    "title": "Inside the pharmaverse",
    "section": "Becoming a Council Member",
    "text": "Becoming a Council Member\nMembership to the pharmaverse Council is granted to the organization and is open to any type of organization (requirements further described here). In layperson’s terms, the Council member organization must support a representative on the council and demonstrate a commitment to contributions to the pharmaverse codebase. Specifically, “Commitment to at least 2 open source packages under pharmaverse via reviews, hands-on code development, product leads, or other roles which contribute to the design, development, testing, release, and/or maintenance.” If any representative steps away from their council position, the member organization would identify the replacement. Council meeting minutes are published in the pharmaverse GitHub here."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#package-inclusion",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#package-inclusion",
    "title": "Inside the pharmaverse",
    "section": "Package Inclusion",
    "text": "Package Inclusion\nApplications for a package to be included in the pharmaverse may originate from anybody and anywhere. The Council (through its Working Groups) reviews applications and either (i) approves the package into the pharmaverse or (ii) rejects the package with a rationale. When a rejection has been issued, common reasons have been ‘not now’ with feedback to enhance testing, documentation, or other aspects that enrich the package. Working Groups may also steer the applicant to other packages with similar functionality. In this sense, respective package maintainers (product owners) are encouraged to work collaboratively to reduce duplicated features across packages and improve the overall experience of the user base. The Council and Working Groups are tasked with assessing this aspect of the pharmaverse code base, but the decision on implementation ultimately resides with the individual package maintainers.\nPackage reviews are conducted in the open:\n\nExample 1: {riskassessment} from R Validation Hub working group – result successful: https://github.com/pharmaverse/pharmaverse/issues/195\nExample 2: {rhino} from Appsilon – result successful: https://github.com/pharmaverse/pharmaverse/issues/260\nExample 3: {synthetic.cdisc.data} from Roche – result unsuccessful: https://github.com/pharmaverse/pharmaverse/issues/235\n\nAt the time of publication, there were over 250 contributors to pharmaverse packages, across a network of organizations much broader than just the 5 Council member organizations. pharmaverse Council encourages diversity of individual contributors and the organizations they are affiliated with. The decision on product development team rosters is the sole discretion of the packages. Of course, Council member organizations are providing people to support the pharmaverse ecosystem (e.g., developers and product owners). However, both strategic and operational decisions on the development and maintenance of individual packages reside outside the pharmaverse Council."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#user-adoption",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#user-adoption",
    "title": "Inside the pharmaverse",
    "section": "User adoption",
    "text": "User adoption\nPharma companies are free to choose any selection of R packages for clinical reporting and only they can justify their choices. The pharmaverse website states that “[a]nyone is free to choose any selection of pharmaverse recommended software or those from any other source.” In addition, pharmaverse does not seek to engage or get endorsement from any health authority. We are merely trying to provide a public service to help individuals and organizations involved in clinical reporting navigate a vast field of available R packages licensed as open source.\nThe pharmaverse website provides illustrative examples of how to use pharmaverse and other packages to build common deliverables."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#so-what-really-is-pharmaverse",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#so-what-really-is-pharmaverse",
    "title": "Inside the pharmaverse",
    "section": "So, what really is pharmaverse?",
    "text": "So, what really is pharmaverse?\nPharmaverse is essentially two things:\n\nPharmaverse is a list of packages curated by the pharmaverse Council and Working Groups, primarily communicated via the pharmaverse website. Pharmaverse also maintains an R-universe build for ease of use outside GxP settings.\npharmaverse is a community of R users and R developers “working to promote collaborative development of curated open-source R packages for clinical reporting usage in pharma.”\n\nPharmaverse Council provides a Slack workspace to build community amongst all interested parties and serve as a communication platform for individuals and package teams. At present, membership to this workspace exceeds &gt;1200 members.\nPharmaverse Council also provides a GitHub organization for developers to work in, but hosting packages in the pharmaverse GitHub is not required to be part of pharmaverse.\nThese elements are supported on an all-volunteer basis, mostly with community versions of various platforms (e.g., we use the free tier on Slack)."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#thank-you",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#thank-you",
    "title": "Inside the pharmaverse",
    "section": "Thank you",
    "text": "Thank you\nThank you, each of you, for the part that you play. No matter how big or small, you are helping amplify the impact that open collaboration is having on how we deliver new medicines and vaccines to patients around the world. We hope you are finding, and continue to find, pharmaverse a valuable piece of the clinical reporting puzzle.\nAnd, if not – let us know!\nPharmaverse Council – Ari, Michael, Mike, Ross, and Sumesh\nDisclaimer: This blog contains opinions that are of the authors alone and do not necessarily reflect the strategy of their respective organizations."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#last-updated",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#last-updated",
    "title": "Inside the pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:37.315739"
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#details",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#details",
    "title": "Inside the pharmaverse",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html",
    "title": "Tips for First Time Contributors",
    "section": "",
    "text": "Who remembers their first day at school?\nWhat about if you ever had to move to join a new school mid-year?\nI had that experience as a child, and there was nothing more intimidating than stepping into a classroom of kids that had already made long-standing bonds and cliques, and here’s me joining as an outsider - the new kid! I had a crippling shyness that meant it took many years until I allowed the same level of friendships to grow as I had in my old school.\nMemories like this stay with us, and they can have a habit of holding us back in later life. It never feels comfortable being the “new kid” and it takes a brave step to jump in and be open to the new connections and relationships that might come from this.\nGetting involved with open source can feel similarly daunting. You’ll likely get hit with impostor syndrome making you doubt whether your code is worthy of sharing or you may fear being judged publicly by others. Even more fundamentally, you might lack certain skills or experience (such as git) which could make an initial barrier.\nOne of the key tenets when we started pharmaverse was for this to be driven by a passionate community of people, many of whom that actually use the packages, as who better to empathize with the challenge at hand than the users themselves? So firstly, please erase from your mind the thought that package development is only for the elite and you are “not worthy”. Nothing could be further from the truth!\nTo share a Coretta Scott King quote: “The greatness of a community is most accurately measured by the compassionate actions of its members”. Now, in no way do I mean here to compare our community to her amazing work for the civil rights movement, but the quote is inspiring in many contexts. We are building a pharma global community of people from all walks of life that come together to help build shared solutions to our industry clinical reporting challenges. The best way we can do this is by having compassion with one another, and you often will find exactly that when you first join a package development team - most people are willing to invest in helping you along your learning journey."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-one-overcoming-the-fear",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-one-overcoming-the-fear",
    "title": "Tips for First Time Contributors",
    "section": "",
    "text": "Who remembers their first day at school?\nWhat about if you ever had to move to join a new school mid-year?\nI had that experience as a child, and there was nothing more intimidating than stepping into a classroom of kids that had already made long-standing bonds and cliques, and here’s me joining as an outsider - the new kid! I had a crippling shyness that meant it took many years until I allowed the same level of friendships to grow as I had in my old school.\nMemories like this stay with us, and they can have a habit of holding us back in later life. It never feels comfortable being the “new kid” and it takes a brave step to jump in and be open to the new connections and relationships that might come from this.\nGetting involved with open source can feel similarly daunting. You’ll likely get hit with impostor syndrome making you doubt whether your code is worthy of sharing or you may fear being judged publicly by others. Even more fundamentally, you might lack certain skills or experience (such as git) which could make an initial barrier.\nOne of the key tenets when we started pharmaverse was for this to be driven by a passionate community of people, many of whom that actually use the packages, as who better to empathize with the challenge at hand than the users themselves? So firstly, please erase from your mind the thought that package development is only for the elite and you are “not worthy”. Nothing could be further from the truth!\nTo share a Coretta Scott King quote: “The greatness of a community is most accurately measured by the compassionate actions of its members”. Now, in no way do I mean here to compare our community to her amazing work for the civil rights movement, but the quote is inspiring in many contexts. We are building a pharma global community of people from all walks of life that come together to help build shared solutions to our industry clinical reporting challenges. The best way we can do this is by having compassion with one another, and you often will find exactly that when you first join a package development team - most people are willing to invest in helping you along your learning journey."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-two-where-to-start",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-two-where-to-start",
    "title": "Tips for First Time Contributors",
    "section": "Step Two: Where to Start",
    "text": "Step Two: Where to Start\nOur pharmaverse website has an Individual Contributor page to help you first get started as a contributor, including many tips and some useful free online training resources for topics such as R package development, Git and GitHub.\nHere are some of the recommendations:\n\nStart small - it doesn’t even need to be a code contribution to begin with! We are equally grateful for anyone raising GitHub issues to report bugs or new feature ideas.\nWhen it comes to you feeling ready to start to contribute code, then choose a package that covers an area of clinical reporting that you already feel confident with. Reach out to the team via our Pharmaverse Slack or on GitHub to express interest, and they may even be able to support you onboarding.\nBe kind to yourself and take on one of the easier issues to build confidence - these are often labeled “good first issue” and might include activities like updating a unit test or some documentation.\nExpect to have review findings for your early Pull Requests and that’s absolutely OK as the learning comes from experience. I’ve been there! I share here without shame my first ever code contribution to admiral and 4 rounds of review later we finally got it merged! Over time I’m glad to say things improved, although maybe that depends who you ask… :D"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-three-still-having-doubt",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-three-still-having-doubt",
    "title": "Tips for First Time Contributors",
    "section": "Step Three: Still Having Doubt?",
    "text": "Step Three: Still Having Doubt?\nI wanted to include a testimonial from another member of the community who has more recently been through all of the above steps.\nHere’s Celine Piraux’s story in her own words:\nLast year, I began my involvement with the xportr R package, and I’d like to share my journey towards my first pull request (PR). As a statistical programmer mainly using SAS, with some knowledge of R, I initially had no experience with R package development. Nevertheless, I was eager to contribute to an open-source R package.\nThe first step in getting involved with an R package is to be aware that volunteers are needed. In my case, I noticed a call for volunteers posted on Slack for the xportr package. Since this package aligns with my expertise in metadata, I eagerly jumped at the opportunity.\nWith no prior experience in R package development and apprehensions about potentially breaking the code, I initially adopted an observational approach. I began by installing the package, running its functions, and then exploring the code to understand their implementation. I tested the functions to evaluate if they worked as expected. If I encountered any issues, I documented them in a GitHub issue.\nAfter creating the GitHub issues with my findings, I began to follow the implementation process by another developer to become more familiar with GitHub. This involved observing how branches are created, commits are made, and comparisons between commits and branches are performed. I also learned about the functioning of comments, pull requests, and the review process.\nOnce his PR was completed, it was time for the review phase, during which I ran the new code to understand the changes and experiment with it. This step also involved two technical requirements: firstly, linking my GitHub account with RStudio to access the code in the branch, and secondly, understanding how to execute code within the branch. Since installing packages using install.packages() wouldn’t function for development code, it was essential to know about the function devtools::load_all() to execute the code.\nAs I became familiar with both the package and GitHub, I felt more confident to start my first implementation. I read the book ‘R Packages’ to gain a better understanding of R package development. The first chapter, entitled ‘The Whole Game’, provided a good overview of R package development and introduced useful functions such as load_all(), document(), test(), and check().\nWith this first implementation, I dived into the development process of the package, involving tasks such as creating a new branch, committing changes, writing a descriptive commit message (still not sure of the best approach), updating and adding new tests to cover the modifications, resolving merge conflicts, updating documentation, handling failing checks in CI/CD. Having a checklist in the PR template was very helpful to keep all the necessary steps in mind.\nThe PR was submitted, and the reviewers provided useful feedback and valuable advice on my code. I implemented the comments, and the PR was approved and merged into the main branch.\nAfter successfully merging my first PR, I feel more confident in contributing to open-source R packages. It’s now time to select a new issue to tackle."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-four-so-what-are-you-waiting-for",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-four-so-what-are-you-waiting-for",
    "title": "Tips for First Time Contributors",
    "section": "Step Four: So, what are YOU waiting for…",
    "text": "Step Four: So, what are YOU waiting for…\nTo end, my advice to my 8 year old self from the start of this story would be “Take a risk and dive in - what have you got to lose?!” - and I only hope some part of this message resonates with others reading this blog and considering joining the open source community.\nHonestly, you won’t regret it!\n\n\n\nPicture taken from the pharmaverse website: showing the network graph of the 261 pharmaverse contributors (at the time of writing this blog)"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#last-updated",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#last-updated",
    "title": "Tips for First Time Contributors",
    "section": "Last updated",
    "text": "Last updated\n\n2025-02-28 08:29:41.026523"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#details",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#details",
    "title": "Tips for First Time Contributors",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  }
]