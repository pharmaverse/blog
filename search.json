[
  {
    "objectID": "session_info.html",
    "href": "session_info.html",
    "title": "Session Info",
    "section": "",
    "text": "session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24)\n os       Ubuntu 22.04.4 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       UTC\n date     2024-05-31\n pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package         * version  date (UTC) lib source\n admiral         * 1.0.2    2024-03-05 [1] RSPM\n admiraldev        1.0.0    2023-12-15 [1] RSPM\n backports         1.5.0    2024-05-23 [1] RSPM\n cellranger        1.1.0    2016-07-27 [1] RSPM\n checkmate         2.3.1    2023-12-04 [1] RSPM\n cli               3.6.2    2023-12-11 [1] RSPM\n colorspace        2.1-0    2023-01-23 [1] RSPM\n diffdf          * 1.0.4    2020-03-17 [1] RSPM\n digest            0.6.35   2024-03-11 [1] RSPM\n dplyr           * 1.1.4    2023-11-17 [1] RSPM\n evaluate          0.23     2023-11-01 [1] RSPM\n fansi             1.0.6    2023-12-08 [1] RSPM\n fastmap           1.2.0    2024-05-15 [1] RSPM\n forcats         * 1.0.0    2023-01-29 [1] RSPM\n generics          0.1.3    2022-07-05 [1] RSPM\n ggplot2         * 3.5.1    2024-04-23 [1] RSPM\n glue              1.7.0    2024-01-09 [1] RSPM\n gtable            0.3.5    2024-04-22 [1] RSPM\n haven             2.5.4    2023-11-30 [1] RSPM\n here            * 1.0.1    2020-12-13 [1] RSPM\n hms               1.1.3    2023-03-21 [1] RSPM\n htmltools         0.5.8.1  2024-04-04 [1] RSPM\n htmlwidgets       1.6.4    2023-12-06 [1] RSPM\n janitor         * 2.2.0    2023-02-02 [1] RSPM\n jsonlite        * 1.8.8    2023-12-04 [1] RSPM\n knitr             1.47     2024-05-29 [1] RSPM\n lifecycle         1.0.4    2023-11-07 [1] RSPM\n link            * 2024.4.0 2024-03-11 [1] RSPM\n lubridate       * 1.9.3    2023-09-27 [1] RSPM\n magrittr          2.0.3    2022-03-30 [1] RSPM\n metacore        * 0.1.3    2024-05-02 [1] RSPM\n metatools       * 0.1.5    2023-03-13 [1] RSPM\n munsell           0.5.1    2024-04-01 [1] RSPM\n patchwork       * 1.2.0    2024-01-08 [1] RSPM\n pharmaverseadam * 0.2.0    2024-01-08 [1] RSPM\n pharmaversesdtm * 0.2.0    2023-12-01 [1] RSPM\n pillar            1.9.0    2023-03-22 [1] RSPM\n pkgconfig         2.0.3    2019-09-22 [1] RSPM\n purrr           * 1.0.2    2023-08-10 [1] RSPM\n R6                2.5.1    2021-08-19 [1] RSPM\n reactable       * 0.4.4    2023-03-12 [1] RSPM\n readr           * 2.1.5    2024-01-10 [1] RSPM\n readxl            1.4.3    2023-07-06 [1] RSPM\n rlang             1.1.3    2024-01-10 [1] RSPM\n rmarkdown         2.27     2024-05-17 [1] RSPM\n rprojroot         2.0.4    2023-11-05 [1] RSPM\n scales            1.3.0    2023-11-28 [1] RSPM\n sessioninfo     * 1.2.2    2021-12-06 [1] RSPM\n snakecase         0.11.1   2023-08-27 [1] RSPM\n spelling        * 2.3.0    2024-03-05 [1] RSPM\n stringi           1.8.4    2024-05-06 [1] RSPM\n stringr         * 1.5.1    2023-11-14 [1] RSPM\n tibble          * 3.2.1    2023-03-20 [1] RSPM\n tidyr           * 1.3.1    2024-01-24 [1] RSPM\n tidyselect        1.2.1    2024-03-11 [1] RSPM\n tidyverse       * 2.0.0    2023-02-22 [1] RSPM\n timechange        0.3.0    2024-01-18 [1] RSPM\n tzdb              0.4.0    2023-05-12 [1] RSPM\n utf8              1.2.4    2023-10-22 [1] RSPM\n vctrs             0.6.5    2023-12-01 [1] RSPM\n withr             3.0.0    2024-01-16 [1] RSPM\n xfun              0.44     2024-05-15 [1] RSPM\n xml2              1.3.6    2023-12-04 [1] RSPM\n xportr          * 0.4.0    2024-03-28 [1] RSPM\n yaml              2.3.8    2023-12-11 [1] RSPM\n\n [1] /home/runner/work/_temp/Library\n [2] /opt/R/4.4.0/lib/R/site-library\n [3] /opt/R/4.4.0/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html",
    "title": "Filter out the noise!",
    "section": "",
    "text": "Filtering and merging datasets is the bread and butter of statistical programming. Whether it’s on the way to an ADaM variable derivation, or in an effort to pull out a list of patients matching a specific condition for a TLG, or another task entirely, most steps in the statistical programming workflow feature some combination of these two tasks.\nThe {tidyverse} functions filter(), group_by(), and*_join() are a fantastic toolset for filtering and merging, and can often suffice to carry out these sorts of operations. Often, however, this will be a multi-step process, requiring more than one set of pipe (%&gt;%) chains if multiple datasets are involved. As such, the {admiral} package builds on this concept by offering a very practical toolset of utility functions, henceforth referred to altogether as filter_*(). These are wrappers of common combinations of {tidyverse} function calls that enable the ADaM programmer to carry out such operations “in stride” within their ADaM workflow - in typical {admiral} style!\nMany of the filter_*() functions feature heavily within the {admiral} codebase, but they can be very handy in their own right. You can learn more about them from:\n\nThe relevant section in the Reference page of the admiral documentation website;\nThe short visual explanations in the second page of the {admiral Cheat Sheet};\n\n\n\n\n\n\n\n…and the rest of this blog post!\n\n\n\nThe examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tibble)\n\nWe also create minimally viable ADSL, ADAE and EX datasets to be used where needed in the following examples.\n\nadsl &lt;- tribble(\n  ~USUBJID,      ~AGE, ~SEX,\n  \"01-701-1015\", 63,   \"F\",\n  \"01-701-1034\", 77,   \"F\",\n  \"01-701-1115\", 84,   \"M\",\n  \"01-701-1146\", 75,   \"F\",\n  \"01-701-1444\", 63,   \"M\"\n)\n\nadae1 &lt;- tribble(\n  ~USUBJID,      ~AEDECOD,                    ~AESEV,     ~AESTDTC,\n  \"01-701-1015\", \"DIARRHOEA\",                 \"MODERATE\", \"2014-01-09\",\n  \"01-701-1034\", \"FATIGUE\",                   \"SEVERE\",   \"2014-11-02\",\n  \"01-701-1034\", \"HEADACHE\",                  \"MILD\",     \"2014-12-01\",\n  \"01-701-1034\", \"APPLICATION SITE PRURITUS\", \"MODERATE\", \"2014-08-27\",\n  \"01-701-1115\", \"FATIGUE\",                   \"MILD\",     \"2013-01-14\",\n  \"01-701-1146\", \"FATIGUE\",                   \"MODERATE\", \"2013-06-03\",\n  \"01-701-1146\", \"ANOSMIA\",                   \"MODERATE\", \"2013-08-11\"\n)\n\nadae2 &lt;- tribble(\n  ~USUBJID,      ~ADY, ~ACOVFL,     ~ADURN,\n  \"01-701-1015\",   10,     \"N\",          1,\n  \"01-701-1015\",   21,     \"N\",         50,\n  \"01-701-1015\",   23,     \"Y\",         14,\n  \"01-701-1015\",   32,     \"N\",         31,\n  \"01-701-1015\",   42,     \"N\",         20,\n  \"01-701-1034\",   11,     \"Y\",         13,\n  \"01-701-1034\",   23,     \"N\",          2,\n  \"01-701-1146\",   13,     \"Y\",         12,\n  \"01-701-1444\",   14,     \"N\",         32,\n  \"01-701-1444\",   21,     \"N\",         41\n)\n\n\nex &lt;- tribble(\n  ~USUBJID, ~EXSEQ, ~EXDOSE, ~EXTRT,\n  \"01-701-1015\", 1, 54, \"XANO\",\n  \"01-701-1015\", 2, 54, \"XANO\",\n  \"01-701-1015\", 3, 54, \"XANO\",\n  \"01-701-1034\", 1, 54, \"XANO\",\n  \"01-701-1034\", 2, 54, \"XANO\",\n  \"01-701-1115\", 1, 0, \"PLACEBO\",\n  \"01-701-1115\", 2, 0, \"PLACEBO\",\n  \"01-701-1115\", 3, 0, \"PLACEBO\",\n  \"01-701-1146\", 1, 0, \"PLACEBO\",\n  \"01-701-1146\", 2, 0, \"PLACEBO\",\n  \"01-701-1146\", 3, 0, \"PLACEBO\",\n  \"01-701-1444\", 1, 54, \"XANO\",\n  \"01-701-1444\", 2, 54, \"XANO\"\n)"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#required-packages",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#required-packages",
    "title": "Filter out the noise!",
    "section": "",
    "text": "The examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tibble)\n\nWe also create minimally viable ADSL, ADAE and EX datasets to be used where needed in the following examples.\n\nadsl &lt;- tribble(\n  ~USUBJID,      ~AGE, ~SEX,\n  \"01-701-1015\", 63,   \"F\",\n  \"01-701-1034\", 77,   \"F\",\n  \"01-701-1115\", 84,   \"M\",\n  \"01-701-1146\", 75,   \"F\",\n  \"01-701-1444\", 63,   \"M\"\n)\n\nadae1 &lt;- tribble(\n  ~USUBJID,      ~AEDECOD,                    ~AESEV,     ~AESTDTC,\n  \"01-701-1015\", \"DIARRHOEA\",                 \"MODERATE\", \"2014-01-09\",\n  \"01-701-1034\", \"FATIGUE\",                   \"SEVERE\",   \"2014-11-02\",\n  \"01-701-1034\", \"HEADACHE\",                  \"MILD\",     \"2014-12-01\",\n  \"01-701-1034\", \"APPLICATION SITE PRURITUS\", \"MODERATE\", \"2014-08-27\",\n  \"01-701-1115\", \"FATIGUE\",                   \"MILD\",     \"2013-01-14\",\n  \"01-701-1146\", \"FATIGUE\",                   \"MODERATE\", \"2013-06-03\",\n  \"01-701-1146\", \"ANOSMIA\",                   \"MODERATE\", \"2013-08-11\"\n)\n\nadae2 &lt;- tribble(\n  ~USUBJID,      ~ADY, ~ACOVFL,     ~ADURN,\n  \"01-701-1015\",   10,     \"N\",          1,\n  \"01-701-1015\",   21,     \"N\",         50,\n  \"01-701-1015\",   23,     \"Y\",         14,\n  \"01-701-1015\",   32,     \"N\",         31,\n  \"01-701-1015\",   42,     \"N\",         20,\n  \"01-701-1034\",   11,     \"Y\",         13,\n  \"01-701-1034\",   23,     \"N\",          2,\n  \"01-701-1146\",   13,     \"Y\",         12,\n  \"01-701-1444\",   14,     \"N\",         32,\n  \"01-701-1444\",   21,     \"N\",         41\n)\n\n\nex &lt;- tribble(\n  ~USUBJID, ~EXSEQ, ~EXDOSE, ~EXTRT,\n  \"01-701-1015\", 1, 54, \"XANO\",\n  \"01-701-1015\", 2, 54, \"XANO\",\n  \"01-701-1015\", 3, 54, \"XANO\",\n  \"01-701-1034\", 1, 54, \"XANO\",\n  \"01-701-1034\", 2, 54, \"XANO\",\n  \"01-701-1115\", 1, 0, \"PLACEBO\",\n  \"01-701-1115\", 2, 0, \"PLACEBO\",\n  \"01-701-1115\", 3, 0, \"PLACEBO\",\n  \"01-701-1146\", 1, 0, \"PLACEBO\",\n  \"01-701-1146\", 2, 0, \"PLACEBO\",\n  \"01-701-1146\", 3, 0, \"PLACEBO\",\n  \"01-701-1444\", 1, 54, \"XANO\",\n  \"01-701-1444\", 2, 54, \"XANO\"\n)"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#last-updated",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#last-updated",
    "title": "Filter out the noise!",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:25.105951"
  },
  {
    "objectID": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#details",
    "href": "posts/2024-03-01_admiral_filter_functions/admiral_filter_functions.html#details",
    "title": "Filter out the noise!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "",
    "text": "This was a big year for open-source work in clinical submissions in general. We saw Roche speak about shifting to an open-source backbone for clinical trials. Novo Nordisk spoke publicly of an R based submission to the FDA. These are true marks of progress being made in R becoming a first class language for clinical reporting.\nBack in August, Nicholas Eugenio released a blog post on the history of pharmaverse. It’s funny to think about the fact that only 3 years ago, the idea of cross organization collaboration on R packages and building a community around this in the clinical world was just a conversation between friends. Since then, we have a community of more than 1200 people on Slack, 350 on LinkedIn, interest from over 150 organizations, and over 30 packages. If 2020 through 2022 was the birth of pharmaverse, 2023 was finding our identity as a community. For the council, 2024 will be about continuing to mature and find more ways that we can continue to support the community."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#was-a-big-year",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#was-a-big-year",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "",
    "text": "This was a big year for open-source work in clinical submissions in general. We saw Roche speak about shifting to an open-source backbone for clinical trials. Novo Nordisk spoke publicly of an R based submission to the FDA. These are true marks of progress being made in R becoming a first class language for clinical reporting.\nBack in August, Nicholas Eugenio released a blog post on the history of pharmaverse. It’s funny to think about the fact that only 3 years ago, the idea of cross organization collaboration on R packages and building a community around this in the clinical world was just a conversation between friends. Since then, we have a community of more than 1200 people on Slack, 350 on LinkedIn, interest from over 150 organizations, and over 30 packages. If 2020 through 2022 was the birth of pharmaverse, 2023 was finding our identity as a community. For the council, 2024 will be about continuing to mature and find more ways that we can continue to support the community."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#what-we-accomplished",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#what-we-accomplished",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "What We Accomplished",
    "text": "What We Accomplished\nOne of our biggest moves in 2023 was to form our partnership with PHUSE. This latched us into an existing community with shared values and a platform that helps us continue to build the pharmaverse community. At the PHUSE EU Connect we had our first opportunity to host a pharmaverse meetup and bring together pharmaverse contributors in person. Additionally, there was an excellent panel session highlighting our use and adoption of open-source across industry, including the pharmaceutical, commercial, and software perspectives. As we move forward, we’ll continue to use this platform to find ways we can host events and encourage collaboration within the pharmaverse community.\nThis year our community was also able to launch new platforms to share updates and knowledge throughout the industry. The pharmaverse examples webpage was launched to show pharmaverse packages in action, and the pharmaverse blog (which I’m using right here!) provides a platform to share updates and community news."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#where-next",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#where-next",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Where Next?",
    "text": "Where Next?\nBack in October, I had the opportunity to do an interview with Michael Rimler for the PHUSE video series Open Source Technologies in Clinical Data Analytics. The last question he asked me was what I expect the state of data analytics in life science to be in 2 to 3 years. My response was that when that time comes, I hope I couldn’t have predicted where we would be - because back in 2020 I could never have predicted where we are now. The progress we’ve made is unbelievable, and the pharmaverse community has played a huge role in getting us where we are today. For the pharmaverse community, I hope to see that progress continue as we move into next year. As a council, our goal is to continue to mature this community. How can we support and foster collaboration between our organizations? How can we leverage this platform to drive the industry forward?\nFor you as an individual, there’s always an opportunity to get involved - and you don’t have to be a package developer to contribute. You can join a working group, write examples, or author a blog post. Furthermore, you can get started with the pharmaverse packages, provide feedback via issues, and advocate for their use within your own organization. The pharmaverse community doesn’t exist without you, and we’re happy to have you all here to help us build this together."
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#p.s.",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#p.s.",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "P.S.",
    "text": "P.S.\nAt PHUSE US Connect 2024 this coming February, be on the lookout for one of the keynote presentations from Michael Rimler and Ross Farrugia! We hope to see you there!\nHere’s to a 2024 full of progress and collaboration!"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#last-updated",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#last-updated",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:19.78091"
  },
  {
    "objectID": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#details",
    "href": "posts/2024-01-04_end_of__year__up.../end_of__year__update_from_the__pharmaverse__council.html#details",
    "title": "End of Year Update from the pharmaverse Council",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html",
    "href": "posts/2023-07-24_rounding/rounding.html",
    "title": "Rounding",
    "section": "",
    "text": "Both SAS and base R have the function round(), which rounds the input to the specified number of decimal places. However, they use different approaches when rounding off a 5:\n\nSAS round() rounds half up. This is the most common method of rounding.\nbase R round() rounds to the nearest even. Therefore round(0.5) is 0 and round(-1.5) is -2. Note from the base R round documentation:\n\n\n\nthis is dependent on OS services and on representation error (since e.g. 0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).\n\n\n\n\nAlthough base R does not have the option for “round half up”, there are functions available in other R packages (e.g., janitor, tidytlg).\nIn general, there are many often used rounding methods. In the table below, you can find examples of them applied to the number 1.45.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nExample: 1.45\n1.5\n(round to 1 decimal place)\n1.4\n(round to 1 decimal place)\n2\n1\n1\n\n\n\nHere are the corresponding ways to implement these methods in SAS and R.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nSAS\nround()\nrounde()\nceil()\nfloor()\nint()\n\n\nR\n\njanitor::round_half_up()\n\n\ntidytlg::roundSAS()\n\n\n\n\nbase::round()\n\n\nbase::ceiling()\n\n\nbase::floor()\n\n\nbase::trunc()\n\n\n\n\nThis table is summarized from links below, where more detailed discussions can be found -\n\nTwo SAS blogs about round-to-even and rounding-up-rounding-down\nR documentation: Base R Round, janitor::round_half_up(), tidytlg::roundSAS()\nCAMIS (Comparing Analysis Method Implementations in Software): A cross-industry initiative to document discrepant results between software. Rounding is one of the comparisons, and there are much more on this page!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#rounding-methods",
    "href": "posts/2023-07-24_rounding/rounding.html#rounding-methods",
    "title": "Rounding",
    "section": "",
    "text": "Both SAS and base R have the function round(), which rounds the input to the specified number of decimal places. However, they use different approaches when rounding off a 5:\n\nSAS round() rounds half up. This is the most common method of rounding.\nbase R round() rounds to the nearest even. Therefore round(0.5) is 0 and round(-1.5) is -2. Note from the base R round documentation:\n\n\n\nthis is dependent on OS services and on representation error (since e.g. 0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).\n\n\n\n\nAlthough base R does not have the option for “round half up”, there are functions available in other R packages (e.g., janitor, tidytlg).\nIn general, there are many often used rounding methods. In the table below, you can find examples of them applied to the number 1.45.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nExample: 1.45\n1.5\n(round to 1 decimal place)\n1.4\n(round to 1 decimal place)\n2\n1\n1\n\n\n\nHere are the corresponding ways to implement these methods in SAS and R.\n\n\n\n\n\n\n\n\n\n\n\n\nround half up\nround to even\nround up\nround down\nround towards zero\n\n\n\n\nSAS\nround()\nrounde()\nceil()\nfloor()\nint()\n\n\nR\n\njanitor::round_half_up()\n\n\ntidytlg::roundSAS()\n\n\n\n\nbase::round()\n\n\nbase::ceiling()\n\n\nbase::floor()\n\n\nbase::trunc()\n\n\n\n\nThis table is summarized from links below, where more detailed discussions can be found -\n\nTwo SAS blogs about round-to-even and rounding-up-rounding-down\nR documentation: Base R Round, janitor::round_half_up(), tidytlg::roundSAS()\nCAMIS (Comparing Analysis Method Implementations in Software): A cross-industry initiative to document discrepant results between software. Rounding is one of the comparisons, and there are much more on this page!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#round-half-up-in-r",
    "href": "posts/2023-07-24_rounding/rounding.html#round-half-up-in-r",
    "title": "Rounding",
    "section": "Round half up in R",
    "text": "Round half up in R\nThe motivation for having a ‘round half up’ function is clear: it’s a widely used rounding method, but there are no such options available in base R.\nThere are multiple forums that have discussed this topic, and quite a few functions already available. But which ones to choose? Are they safe options?\nThe first time I needed to round half up in R, I chose the function from a PHUSE paper and applied it to my study. It works fine for a while until I encountered the following precision issue when double programming in R for TLGs made in SAS.\n\nNumerical precision issue\nExample of rounding half up for 2436.845, with 2 decimal places:\n\n# a function that rounds half up\n# exact copy from: https://www.lexjansen.com/phuse-us/2020/ct/CT05.pdf\nut_round &lt;- function(x, n = 0) {\n  # x is the value to be rounded\n  # n is the precision of the rounding\n  scale &lt;- 10^n\n  y &lt;- trunc(x * scale + sign(x) * 0.5) / scale\n  # Return the rounded number\n  return(y)\n}\n# round half up for 2436.845, with 2 decimal places\nut_round(2436.845, 2)\n\n[1] 2436.84\n\n\nThe expected result is 2436.85, but the output rounds it down. Thanks to the community effort, there are already discussions and resolution available in a StackOverflow post -\n\nThere are numerical precision issues, e.g., round2(2436.845, 2) returns 2436.84. Changing z + 0.5 to z + 0.5 + sqrt(.Machine$double.eps) seems to work for me. – Gregor Thomas Jun 24, 2020 at 2:16\n\n\n.Machine$double.eps is a built-in constant in R that represents the smallest positive floating-point number that can be represented on the system (reference: Machine Characteristics)\nThe expression + sqrt(.Machine$double.eps) is used to add a very small value to mitigate floating-point precision issues.\nFor more information about computational precision and floating-point, see the following links -\n\nR: Why doesn’t R think these numbers are equal?\nSAS: Numerical Accuracy in SAS Software\n\n\nAfter the fix:\n\n# revised rounds half up\nut_round1 &lt;- function(x, n = 0) {\n  # x is the value to be rounded\n  # n is the precision of the rounding\n  scale &lt;- 10^n\n  y &lt;- trunc(x * scale + sign(x) * 0.5 + sqrt(.Machine$double.eps)) / scale\n  # Return the rounded number\n  return(y)\n}\n# round half up for 2436.845, with 2 decimal places\nut_round1(2436.845, 2)\n\n[1] 2436.85\n\n\n\n\nWe are not alone\nThe same issue occurred in the following functions/options as well, and has been raised by users:\n\njanitor::round_half_up(): issue was raised and fixed in v2.1.0\nTplyr: options(tplyr.IBMRounding = TRUE), issue was raised\nscrutiny::round_up_from()/round_up(): issue was raised and fixed\n... and many others!\n\n\n\nWhich ones to use?\nThe following functions have the precision issue mentioned above fixed, they all share the same logic from this StackOverflow post.\n\njanitor::round_half_up() version &gt;= 2.1.0\ntidytlg::roundSAS()\n\nthis function has two more arguments that can convert the result to character and allow a character string to indicate missing values\n\nscrutiny::round_up_from()/round_up() version &gt;= 0.2.5\n\nround_up_from() has a threshold argument for rounding up, which adds flexibility for rounding up\nround_up() rounds up from 5, which is a special case of round_up_from()\n\n\n\n\nAre they safe options?\nThose “round half up” functions do not offer the same level of precision and accuracy as the base R round function.\nFor example, let’s consider a value a that is slightly less than 1.5. If we choose round half up approach to round a to 0 decimal places, an output of 1 is expected. However, those functions yield a result of 2 because 1.5 - a is less than sqrt(.Machine$double.eps).\n\na &lt;- 1.5 - 0.5 * sqrt(.Machine$double.eps)\nut_round1(a, 0)\n\n[1] 2\n\njanitor::round_half_up(a, digits = 0)\n\n[1] 2\n\n\nThis behavior aligns the floating point number comparison functions all.equal() and dplyr::near() with default tolerance .Machine$double.eps^0.5, where 1.5 and a are treated as equal.\n\nall.equal(a, 1.5)\n\n[1] TRUE\n\ndplyr::near(a, 1.5)\n\n[1] TRUE\n\n\nWe can get the expected results from base R round as it provides greater accuracy.\n\nround(a)\n\n[1] 1\n\n\nHere is an example when base R round reaches the precision limit:\n\n# b is slightly less than 1.5\nb &lt;- 1.5 - 0.5 * .Machine$double.eps\n# 1 is expected but the result is 2\nround(b)\n\n[1] 2\n\n\nThe precision and accuracy requirements can vary depending on the application. Therefore, it is essential to be aware each function’s performance in your specific context before making a choice."
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#conclusion",
    "href": "posts/2023-07-24_rounding/rounding.html#conclusion",
    "title": "Rounding",
    "section": "Conclusion",
    "text": "Conclusion\n\nWith the differences in default behaviour across languages, you could consider your QC strategy and whether an acceptable level of fuzz in the electronic comparisons could be allowed for cases such as rounding when making comparisons between 2 codes written in different languages as long as this is documented. Alternatively you could document the exact rounding approach to be used in the SAP and then match this regardless of programming language used. - Ross Farrugia\n\nThanks Ross Farrugia, Ben Straub, Edoardo Mancini and Liming for reviewing this blog post and providing valuable feedback!\nIf you spot an issue or have different opinions, please don’t hesitate to raise them through pharmaverse/blog!"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#last-updated",
    "href": "posts/2023-07-24_rounding/rounding.html#last-updated",
    "title": "Rounding",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:16.172725"
  },
  {
    "objectID": "posts/2023-07-24_rounding/rounding.html#details",
    "href": "posts/2023-07-24_rounding/rounding.html#details",
    "title": "Rounding",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html",
    "href": "posts/2023-07-09_falcon/falcon.html",
    "title": "falcon",
    "section": "",
    "text": "The {falcon} initiative is an industry collaborative effort under pharmaverse that unites Boehringer Ingelheim, Moderna, Roche, and Sanofi with the aspiration of building and open-sourcing a comprehensive catalog of harmonized tables, listings, and graphs (TLGs) for clinical study reporting. By leveraging existing open-source R packages, {falcon} aims to simplify the process of output review, comparison, and meta-analyses, while fostering efficient communication among stakeholders in the pharmaceutical industry."
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#what-is-falcon",
    "href": "posts/2023-07-09_falcon/falcon.html#what-is-falcon",
    "title": "falcon",
    "section": "",
    "text": "The {falcon} initiative is an industry collaborative effort under pharmaverse that unites Boehringer Ingelheim, Moderna, Roche, and Sanofi with the aspiration of building and open-sourcing a comprehensive catalog of harmonized tables, listings, and graphs (TLGs) for clinical study reporting. By leveraging existing open-source R packages, {falcon} aims to simplify the process of output review, comparison, and meta-analyses, while fostering efficient communication among stakeholders in the pharmaceutical industry."
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#why-do-we-build-it",
    "href": "posts/2023-07-09_falcon/falcon.html#why-do-we-build-it",
    "title": "falcon",
    "section": "Why do we build it?",
    "text": "Why do we build it?\nThe collaborative effort focuses on improving the clarity, consistency, and accessibility of TLGs by addressing variations and redundancies in their creation and use. This harmonized approach allows for streamlined reporting processes and facilitates effective communication of study results within the industry and to regulatory authorities."
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#what-has-been-done-so-far",
    "href": "posts/2023-07-09_falcon/falcon.html#what-has-been-done-so-far",
    "title": "falcon",
    "section": "What has been done so far?",
    "text": "What has been done so far?\nDrawing inspiration from the FDA Standard Safety Tables and Figures Integrated Guide, the {falcon} initiative has successfully developed and open-sourced 11 templates to date. 4 product owners and 11 developers from 4 companies have collaborated to make these templates available and also published them on the official {falcon} website at https://pharmaverse.github.io/falcon/."
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#next-steps-vision",
    "href": "posts/2023-07-09_falcon/falcon.html#next-steps-vision",
    "title": "falcon",
    "section": "Next steps & vision",
    "text": "Next steps & vision\nFuture plans for {falcon} involve expanding the catalog through continuous collaboration from participating companies and inviting wider industry engagement. The ultimate goal is to promote harmonization of TLGs for clinical reporting across the pharmaceutical industry, leading to greater efficiency, collaboration, and innovation. Even though templates currently come from a published FDA guide, the collaborating companies are open to share and discuss similarities and differences on analysis concepts and output layouts of their own implementations in clinical reporting, for both safety and efficacy analyses.\nIn addition, while currently all templates were built using {rtables}, {tern}, {rlistings} and drew inspiration from the open-sourced TLG-Catalog, moving forward, we plan to support creating the same templates using alternative table engines such as {gt}.\n{falcon} will be presented at the upcoming PHUSE EU (Standards Implementation stream), where we will share the collaboration journey of {falcon} so far, providing more details on the current progress, long-term vision, and strategies for this initiative. Attendees will gain insights into the challenges and opportunities of harmonizing clinical reporting through open-source collaboration and learn about the potential benefits and future direction of {falcon}."
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#last-updated",
    "href": "posts/2023-07-09_falcon/falcon.html#last-updated",
    "title": "falcon",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:12.299567"
  },
  {
    "objectID": "posts/2023-07-09_falcon/falcon.html#details",
    "href": "posts/2023-07-09_falcon/falcon.html#details",
    "title": "falcon",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html",
    "title": "PK Examples",
    "section": "",
    "text": "A new pharmaverse examples website has some exciting new features to explore.\nOne of these is the ability to launch Posit Cloud to explore the example code and make your own modifications. This interactive Posit Cloud environment is preconfigured with all required package installations. Click here: Launch Posit Cloud to explore the examples code.\nThis sample code here is based on the Population PK Analysis Data (ADPPK) model which follows the recently released CDISC Implementation Guide.\nPopulation PK models generally make use of nonlinear mixed effects models that require numeric variables. The data used in the models will include both dosing and concentration records, relative time variables, and numeric covariate variables. For more details see the {admiral} vignette.\n\n\nFirst we will load the packages required for our project. We will use {admiral} for the creation of analysis data. {admiral} requires {dplyr}, {lubridate} and {stringr}. We will use {metacore} and {metatools} to store and manipulate metadata from our specifications. We will use {xportr} to perform checks on the final data and export to a transport file.\nThe source SDTM data will come from the CDISC pilot study data stored in {pharmaversesdtm} and the ADaM ADSL data will come from {pharmaverseadam}.\n\n# Load Packages\nlibrary(admiral)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(metacore)\nlibrary(metatools)\nlibrary(xportr)\nlibrary(readr)\nlibrary(pharmaversesdtm)\nlibrary(pharmaverseadam)\n\n\n\n\nWe have saved our specifications in an Excel file and will load them into {metacore} with the metacore::spec_to_metacore() function.\n\n# ---- Load Specs for Metacore ----\nmetacore &lt;- spec_to_metacore(\"pk_spec.xlsx\") %&gt;%\n  select_dataset(\"ADPPK\")\n\n\n\n\nWe will load our SDTM data from {pharmaversesdtm}. The main components of the Population PK will be exposure data from EX and pharmacokinetic concentration data from PC. Here we will use ADSL from {pharmaverseadam} for baseline characteristics and we will derive additional baselines from vital signs VS and laboratory data LB.\n\n# ---- Load source datasets ----\n# Load PC, EX, VS, LB and ADSL\ndata(\"pc\")\ndata(\"ex\")\ndata(\"vs\")\ndata(\"lb\")\ndata(\"adsl\")\n\nex &lt;- convert_blanks_to_na(ex)\npc &lt;- convert_blanks_to_na(pc)\nvs &lt;- convert_blanks_to_na(vs)\nlb &lt;- convert_blanks_to_na(lb)\n\n\n\n\nIn this step we will create our numeric covariates using the metatools::create_var_from_codelist() function.\n\n#---- Derive Covariates ----\n# Include numeric values for STUDYIDN, USUBJIDN, SEXN, RACEN etc.\n\ncovar &lt;- adsl %&gt;%\n  create_var_from_codelist(metacore, input_var = STUDYID, out_var = STUDYIDN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SEX, out_var = SEXN) %&gt;%\n  create_var_from_codelist(metacore, input_var = RACE, out_var = RACEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ETHNIC, out_var = AETHNIC) %&gt;%\n  create_var_from_codelist(metacore, input_var = AETHNIC, out_var = AETHNICN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORT) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORTC) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYN) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYL) %&gt;%\n  mutate(\n    STUDYIDN = as.numeric(word(USUBJID, 1, sep = fixed(\"-\"))),\n    SITEIDN = as.numeric(word(USUBJID, 2, sep = fixed(\"-\"))),\n    USUBJIDN = as.numeric(word(USUBJID, 3, sep = fixed(\"-\"))),\n    SUBJIDN = as.numeric(SUBJID),\n    ROUTE = unique(ex$EXROUTE),\n    FORM = unique(ex$EXDOSFRM),\n    REGION1 = COUNTRY,\n    REGION1N = COUNTRYN,\n    SUBJTYPC = \"Volunteer\",\n  ) %&gt;%\n  create_var_from_codelist(metacore, input_var = FORM, out_var = FORMN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ROUTE, out_var = ROUTEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SUBJTYPC, out_var = SUBJTYP)\n\n\n\nNext we add additional baselines from vital signs and laboratory data. Several common variables are computed using some of the built in functions in {admiral}.\n\nlabsbl &lt;- lb %&gt;%\n  filter(LBBLFL == \"Y\" & LBTESTCD %in% c(\"CREAT\", \"ALT\", \"AST\", \"BILI\")) %&gt;%\n  mutate(LBTESTCDB = paste0(LBTESTCD, \"BL\")) %&gt;%\n  select(STUDYID, USUBJID, LBTESTCDB, LBSTRESN)\n\ncovar_vslb &lt;- covar %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"HEIGHT\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(HTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"WEIGHT\" & VSBLFL == \"Y\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(WTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_transposed(\n    dataset_merge = labsbl,\n    by_vars = exprs(STUDYID, USUBJID),\n    key_var = LBTESTCDB,\n    value_var = LBSTRESN\n  ) %&gt;%\n  mutate(\n    BMIBL = compute_bmi(height = HTBL, weight = WTBL),\n    BSABL = compute_bsa(\n      height = HTBL,\n      weight = HTBL,\n      method = \"Mosteller\"\n    ),\n    CRCLBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CRCL\"\n    ),\n    EGFRBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CKD-EPI\"\n    )\n  ) %&gt;%\n  rename(TBILBL = BILIBL)\n\nThis covariate section of the code will be combined with the dosing and observation records from EX and PC.\nThe rest of the code can be seen on the pharmaverse examples website or in the Posit Cloud environment.\nHappy exploring!"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#first-load-packages",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#first-load-packages",
    "title": "PK Examples",
    "section": "",
    "text": "First we will load the packages required for our project. We will use {admiral} for the creation of analysis data. {admiral} requires {dplyr}, {lubridate} and {stringr}. We will use {metacore} and {metatools} to store and manipulate metadata from our specifications. We will use {xportr} to perform checks on the final data and export to a transport file.\nThe source SDTM data will come from the CDISC pilot study data stored in {pharmaversesdtm} and the ADaM ADSL data will come from {pharmaverseadam}.\n\n# Load Packages\nlibrary(admiral)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(metacore)\nlibrary(metatools)\nlibrary(xportr)\nlibrary(readr)\nlibrary(pharmaversesdtm)\nlibrary(pharmaverseadam)"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#next-load-specifications-for-metacore",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#next-load-specifications-for-metacore",
    "title": "PK Examples",
    "section": "",
    "text": "We have saved our specifications in an Excel file and will load them into {metacore} with the metacore::spec_to_metacore() function.\n\n# ---- Load Specs for Metacore ----\nmetacore &lt;- spec_to_metacore(\"pk_spec.xlsx\") %&gt;%\n  select_dataset(\"ADPPK\")"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#load-source-datasets",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#load-source-datasets",
    "title": "PK Examples",
    "section": "",
    "text": "We will load our SDTM data from {pharmaversesdtm}. The main components of the Population PK will be exposure data from EX and pharmacokinetic concentration data from PC. Here we will use ADSL from {pharmaverseadam} for baseline characteristics and we will derive additional baselines from vital signs VS and laboratory data LB.\n\n# ---- Load source datasets ----\n# Load PC, EX, VS, LB and ADSL\ndata(\"pc\")\ndata(\"ex\")\ndata(\"vs\")\ndata(\"lb\")\ndata(\"adsl\")\n\nex &lt;- convert_blanks_to_na(ex)\npc &lt;- convert_blanks_to_na(pc)\nvs &lt;- convert_blanks_to_na(vs)\nlb &lt;- convert_blanks_to_na(lb)"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#derive-covariates-using-metatools",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#derive-covariates-using-metatools",
    "title": "PK Examples",
    "section": "",
    "text": "In this step we will create our numeric covariates using the metatools::create_var_from_codelist() function.\n\n#---- Derive Covariates ----\n# Include numeric values for STUDYIDN, USUBJIDN, SEXN, RACEN etc.\n\ncovar &lt;- adsl %&gt;%\n  create_var_from_codelist(metacore, input_var = STUDYID, out_var = STUDYIDN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SEX, out_var = SEXN) %&gt;%\n  create_var_from_codelist(metacore, input_var = RACE, out_var = RACEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ETHNIC, out_var = AETHNIC) %&gt;%\n  create_var_from_codelist(metacore, input_var = AETHNIC, out_var = AETHNICN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORT) %&gt;%\n  create_var_from_codelist(metacore, input_var = ARMCD, out_var = COHORTC) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYN) %&gt;%\n  create_var_from_codelist(metacore, input_var = COUNTRY, out_var = COUNTRYL) %&gt;%\n  mutate(\n    STUDYIDN = as.numeric(word(USUBJID, 1, sep = fixed(\"-\"))),\n    SITEIDN = as.numeric(word(USUBJID, 2, sep = fixed(\"-\"))),\n    USUBJIDN = as.numeric(word(USUBJID, 3, sep = fixed(\"-\"))),\n    SUBJIDN = as.numeric(SUBJID),\n    ROUTE = unique(ex$EXROUTE),\n    FORM = unique(ex$EXDOSFRM),\n    REGION1 = COUNTRY,\n    REGION1N = COUNTRYN,\n    SUBJTYPC = \"Volunteer\",\n  ) %&gt;%\n  create_var_from_codelist(metacore, input_var = FORM, out_var = FORMN) %&gt;%\n  create_var_from_codelist(metacore, input_var = ROUTE, out_var = ROUTEN) %&gt;%\n  create_var_from_codelist(metacore, input_var = SUBJTYPC, out_var = SUBJTYP)\n\n\n\nNext we add additional baselines from vital signs and laboratory data. Several common variables are computed using some of the built in functions in {admiral}.\n\nlabsbl &lt;- lb %&gt;%\n  filter(LBBLFL == \"Y\" & LBTESTCD %in% c(\"CREAT\", \"ALT\", \"AST\", \"BILI\")) %&gt;%\n  mutate(LBTESTCDB = paste0(LBTESTCD, \"BL\")) %&gt;%\n  select(STUDYID, USUBJID, LBTESTCDB, LBSTRESN)\n\ncovar_vslb &lt;- covar %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"HEIGHT\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(HTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_merged(\n    dataset_add = vs,\n    filter_add = VSTESTCD == \"WEIGHT\" & VSBLFL == \"Y\",\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(WTBL = VSSTRESN)\n  ) %&gt;%\n  derive_vars_transposed(\n    dataset_merge = labsbl,\n    by_vars = exprs(STUDYID, USUBJID),\n    key_var = LBTESTCDB,\n    value_var = LBSTRESN\n  ) %&gt;%\n  mutate(\n    BMIBL = compute_bmi(height = HTBL, weight = WTBL),\n    BSABL = compute_bsa(\n      height = HTBL,\n      weight = HTBL,\n      method = \"Mosteller\"\n    ),\n    CRCLBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CRCL\"\n    ),\n    EGFRBL = compute_egfr(\n      creat = CREATBL, creatu = \"SI\", age = AGE, weight = WTBL, sex = SEX,\n      method = \"CKD-EPI\"\n    )\n  ) %&gt;%\n  rename(TBILBL = BILIBL)\n\nThis covariate section of the code will be combined with the dosing and observation records from EX and PC.\nThe rest of the code can be seen on the pharmaverse examples website or in the Posit Cloud environment.\nHappy exploring!"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#last-updated",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#last-updated",
    "title": "PK Examples",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:08.85306"
  },
  {
    "objectID": "posts/2023-12-20_p_k__examples/p_k__examples.html#details",
    "href": "posts/2023-12-20_p_k__examples/p_k__examples.html#details",
    "title": "PK Examples",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html",
    "title": "admiral 1.0.0",
    "section": "",
    "text": "admiral 1.0.0 is out on CRAN. This release brings several new features to your tool set for working with ADaMs in R. admiral 1.0.0 also brings needed stability to users who were wishing to adopt admiral, but were a little worried by the fast deprecation and experimentation for pre-v1.0.0 releases.\nThis blog post will discuss our commitment to stability, walk you through the new features available, discuss some of the bug fixes, a push for common APIs across our functions, and showcase the resources available to help you on-board to admiral."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_extreme_event",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_extreme_event",
    "title": "admiral 1.0.0",
    "section": "derive_vars_extreme_event()",
    "text": "derive_vars_extreme_event()\nThis function takes available records from user-defined events by selecting the extreme observations and appending them as a variable(s) to your dataset.   derive_vars_extreme_event()  works similar to   derive_extreme_event() , but instead of adding observations the function will add variable(s).\nLet’s take a peek with a very simple example where we just use ADSL! The documentation for   derive_vars_extreme_event()  has a much richer example with events from other domains that is more aligned to where you would use this function.\nLet us make some dummy ADSL data and load up our packages. The goal here is to add two new variables LSTALVDT and DTHFL based on a list of objects that are used to specify the following:\n\nthe dataset to look at\na set of conditions\nwhat to set the values for the new variables\n\n\nlibrary(tibble)\nlibrary(admiral)\nlibrary(lubridate)\n\nadsl &lt;- tribble(\n  ~STUDYID, ~USUBJID, ~TRTEDT, ~DTHDT,\n  \"PILOT01\", \"01-1130\", ymd(\"2014-08-16\"), ymd(\"2014-09-13\"),\n  \"PILOT01\", \"01-1133\", ymd(\"2013-04-28\"), ymd(\"\"),\n  \"PILOT01\", \"01-1211\", ymd(\"2013-01-12\"), ymd(\"\"),\n  \"PILOT01\", \"09-1081\", ymd(\"2014-04-27\"), ymd(\"\"),\n  \"PILOT01\", \"09-1088\", ymd(\"2014-10-09\"), ymd(\"2014-11-01\"),\n)\n\nIn this example, we only use ADSL as the source dataset, so it is a bit contrived, but much more compact for us. Note the use of the events that is taking in our list of event objects and the different conditions and values we set to create our LSTALVDT and DTHFL variables.\n\nderive_vars_extreme_event(\n  adsl,\n  by_vars = exprs(STUDYID, USUBJID),\n  events = list(\n    event(\n      dataset_name = \"adsl\",\n      condition = !is.na(DTHDT),\n      set_values_to = exprs(LSTALVDT = DTHDT, DTHFL = \"Y\")\n    ),\n    event(\n      dataset_name = \"adsl\",\n      condition = !is.na(TRTEDT),\n      set_values_to = exprs(LSTALVDT = TRTEDT, DTHFL = \"N\")\n    )\n  ),\n  source_datasets = list(adsl = adsl),\n  order = exprs(LSTALVDT),\n  mode = \"last\",\n  new_vars = exprs(LSTALVDT = LSTALVDT, DTHFL = DTHFL)\n)\n\n# A tibble: 5 × 6\n  STUDYID USUBJID TRTEDT     DTHDT      LSTALVDT   DTHFL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;\n1 PILOT01 01-1130 2014-08-16 2014-09-13 2014-09-13 Y    \n2 PILOT01 01-1133 2013-04-28 NA         2013-04-28 N    \n3 PILOT01 01-1211 2013-01-12 NA         2013-01-12 N    \n4 PILOT01 09-1081 2014-04-27 NA         2014-04-27 N    \n5 PILOT01 09-1088 2014-10-09 2014-11-01 2014-11-01 Y    \n\n\nOkay! We used a very small example to showcase how to find extreme observations and appending this information as new variables to our ADSL dataset. Highly recommend checking out the more detailed example in the function documentation to see its true power!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_var_merged_ef_msrc",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_var_merged_ef_msrc",
    "title": "admiral 1.0.0",
    "section": "derive_var_merged_ef_msrc()",
    "text": "derive_var_merged_ef_msrc()\nThis function has some similarity to   derive_vars_extreme_event() , but now we are only looking at adding a single flag variable based on checking conditions across multiple datasets.\nWe develop some simple dummy data for ADSL, CM and PR. Our goal is to flag patients who have CMCAT = \"ANTI-CANCER\" in the CM dataset or have records in the PR dataset. Any participants who meet these conditions will have our new variable CANCTRFL set as \"Y\".\n\nadsl &lt;- tribble(\n  ~USUBJID,\n  \"1\",\n  \"2\",\n  \"3\",\n  \"4\",\n)\n\ncm &lt;- tribble(\n  ~USUBJID, ~CMCAT,        ~CMSEQ,\n  \"1\",      \"ANTI-CANCER\",      1,\n  \"1\",      \"GENERAL\",          2,\n  \"2\",      \"GENERAL\",          1,\n  \"3\",      \"ANTI-CANCER\",      1\n)\n\npr &lt;- tribble(\n  ~USUBJID, ~PRSEQ,\n  \"2\",      1,\n  \"3\",      1\n)\n\nNow we have the argument flag_events that takes a list of objects where we define the conditions and datasets to check in.\n\nderive_var_merged_ef_msrc(\n  adsl,\n  flag_events = list(\n    flag_event(\n      dataset_name = \"cm\",\n      condition = CMCAT == \"ANTI-CANCER\"\n    ),\n    flag_event(\n      dataset_name = \"pr\"\n    )\n  ),\n  source_datasets = list(cm = cm, pr = pr),\n  by_vars = exprs(USUBJID),\n  new_var = CANCTRFL\n)\n\n# A tibble: 4 × 2\n  USUBJID CANCTRFL\n  &lt;chr&gt;   &lt;chr&gt;   \n1 1       Y       \n2 2       Y       \n3 3       Y       \n4 4       &lt;NA&gt;    \n\n\nLet’s go! We searched over multiple datasets, CM and PR, with multiple conditions and appended a new variable CANCTRFL to ADSL setting to \"Y\" if those conditions were met."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_computed",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#derive_vars_computed",
    "title": "admiral 1.0.0",
    "section": "derive_vars_computed()",
    "text": "derive_vars_computed()\nThis function is very similar to   derive_param_computed() , but instead of adding observations we are going to add variable(s). Very handy when wanting to add some additional variables to ADSL, e.g. baseline variables.\nLet’s make some dummy data for an ADSL and ADVS. Our goal is to derive a BMIBL variable pulled from ADVS and append to ADSL.\n\nadsl &lt;- tribble(\n  ~STUDYID,   ~USUBJID, ~AGE,   ~AGEU,\n  \"PILOT01\", \"01-1302\",   61, \"YEARS\",\n  \"PILOT01\", \"17-1344\",   64, \"YEARS\"\n)\n\nadvs &lt;- tribble(\n  ~STUDYID, ~USUBJID, ~PARAMCD, ~PARAM, ~VISIT, ~AVAL, ~AVALU, ~ABLFL,\n  \"PILOT01\", \"01-1302\", \"HEIGHT\", \"Height (cm)\", \"SCREENING\", 177.8, \"cm\", \"Y\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"SCREENING\", 81.19, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"BASELINE\", 82.1, \"kg\", \"Y\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 2\", 81.19, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 4\", 82.56, \"kg\", \"N\",\n  \"PILOT01\", \"01-1302\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 6\", 80.74, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"HEIGHT\", \"Height (cm)\", \"SCREENING\", 163.5, \"cm\", \"Y\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"SCREENING\", 58.06, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"BASELINE\", 58.06, \"kg\", \"Y\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 2\", 58.97, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 4\", 57.97, \"kg\", \"N\",\n  \"PILOT01\", \"17-1344\", \"WEIGHT\", \"Weight (kg)\", \"WEEK 6\", 58.97, \"kg\", \"N\"\n)\n\nTake a look at how we use new_vars and filter_add. We use a function inside of new_vars to help us calculate the BMI while using the filter_add argument to only look at baseline records for the calculation.\n\nderive_vars_computed(\n  dataset = adsl,\n  dataset_add = advs,\n  by_vars = exprs(STUDYID, USUBJID),\n  parameters = c(\"WEIGHT\"),\n  constant_by_vars = exprs(STUDYID, USUBJID),\n  constant_parameters = c(\"HEIGHT\"),\n  new_vars = exprs(BMIBL = compute_bmi(height = AVAL.HEIGHT, weight = AVAL.WEIGHT)),\n  filter_add = ABLFL == \"Y\"\n)\n\n# A tibble: 2 × 5\n  STUDYID USUBJID   AGE AGEU  BMIBL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 PILOT01 01-1302    61 YEARS  26.0\n2 PILOT01 17-1344    64 YEARS  21.7\n\n\nAlright! Simple enough. We just took records from ADVSto help us calculate the BMI at baseline using this function and appended our new variable to ADSL."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#argument-alignment",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#argument-alignment",
    "title": "admiral 1.0.0",
    "section": "Argument Alignment",
    "text": "Argument Alignment\nA huge push was made for 1.0.0 to help align our arguments across all of &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions. What does this mean? We identified arguments in functions where the argument did the same things but was slightly named differently. For 1.0.0, we really want users to have a solid API for &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions.\nLet’s take a peak at the function   consolidate_metadata()  to even better understand our goal here.\nconsolidate_metadata(\n  datasets,\n  key_vars,\n  source_var = SOURCE,\n  check_vars = \"warning\",\n  check_keys,\n  check_type = \"error\"\n)\nIn previous versions of   {admiral}  the   consolidate_metadata()  function had the argument check_keys, which helps to check uniqueness. Other functions had a similar argument, but were called check_unique. Therefore, to better align our common API for   {admiral}  functions we decided to rename the check_keys argument to check_unique. You can follow the discussion around this renaming effort in this GitHub Issue.\n\n\n\n\n\n\n\n\n\nThe argument has a deprecated tag in the function documentation and will issue a warning to users. There was quite a bit of renaming of arguments for 1.0.0 so there are quite a few of these tags in our documentation. In subsequent releases, these arguments will be removed. Please see the changelog if you would like to explore other functions that had arguments renamed. The issues are linked to each rename so you can follow along with the discussions!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#bug-fixes",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#bug-fixes",
    "title": "admiral 1.0.0",
    "section": "Bug Fixes",
    "text": "Bug Fixes\nWe love fixing bugs and take them incredibly seriously - especially when identified by members from the community.\nIf you find a pesky bug, please fill out a Bug Report on our Issues Tab.\nEach bug fixed by our development team is documented in our changelog with the Issue linked.\n\n\n\n\n\n\n\n\n\nFor example, if you click through the issue for derive_extreme_event() that identified a problem where the condition was ignored if the mode argument was used, you can see the Bug Report along with a reproducible example. You can also see the Pull Request for the exact code changes that are addressing this bug linked in the Issue! Way cool!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiraldiscovery",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiraldiscovery",
    "title": "admiral 1.0.0",
    "section": "admiraldiscovery",
    "text": "admiraldiscovery\nThis is a dedicated website that lists out in a tabular format standard ADaM datasets and their common variables with corresponding &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; functions that could be used to create the variables. Very handy when you just want to get some starter code on deriving EOSDT or TRTSDT!"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiral-cheat-sheet",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#admiral-cheat-sheet",
    "title": "admiral 1.0.0",
    "section": "admiral Cheat Sheet",
    "text": "admiral Cheat Sheet\nInspired by other R package cheat sheets! We try and surface commonly needed functions for doing ADaM derivations with simple tables to show how the data is transforming."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#way-back-machine",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#way-back-machine",
    "title": "admiral 1.0.0",
    "section": "Way Back Machine",
    "text": "Way Back Machine\nStudies can last a long time. Adopting R as your primary analysis language for your study can introduce certain risks around package dependencies. Fixing those dependencies to certain package versions can help mitigate those risks. Unfortunately, as package websites are updated those helpful documents, examples and vignettes can be lost as the version changes. Do not lose heart &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt; users. If you decided to fix to a certain version of &lt;bslib-tooltip placement=\"auto\" bsOptions=\"[]\" data-require-bs-version=\"5\" data-require-bs-caller=\"tooltip()\"&gt;   &lt;template&gt;ADaM in R Asset Library • admiral&lt;/template&gt;   &lt;a href=\"https://pharmaverse.github.io/admiral/\" class=\"r-link-pkg\" target=\"_blank\"&gt;{admiral}&lt;/a&gt; &lt;/bslib-tooltip&gt;, we have you covered with our Way Back Machine that allows you to change the website documentation back to the version you are using."
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#last-updated",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#last-updated",
    "title": "admiral 1.0.0",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:03.852699"
  },
  {
    "objectID": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#details",
    "href": "posts/2023-12-18_admiral_1_0/admiral_1_0.html#details",
    "title": "admiral 1.0.0",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html",
    "title": "Blanks and NAs",
    "section": "",
    "text": "Reading in SAS-based datasets (.sas7bdat or xpt) into R has users calling the R package haven. A typical call might invoke read_sas() or read_xpt() to bring in your source data to construct your ADaMs or SDTMs.\nUnfortunately, while using haven the character blanks (missing data) found in a typical SAS-based dataset are left as blanks. These blanks will typically prove problematic while using functions like is.na in combination with dplyr::filter() to subset data. Check out Bayer’s SAS2R catalog: handling-of-missing-values for more discussion on missing values and NAs.\nIn the admiral package, we have built a simple function called convert_blanks_to_na() to help us quickly remedy this problem. You can supply an entire dataframe to this function and it will convert any character blanks to NA_character_"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#loading-packages-and-making-dummy-data",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#loading-packages-and-making-dummy-data",
    "title": "Blanks and NAs",
    "section": "Loading Packages and Making Dummy Data",
    "text": "Loading Packages and Making Dummy Data\n\nlibrary(admiral)\nlibrary(tibble)\nlibrary(dplyr)\n\ndf &lt;- tribble(\n  ~USUBJID, ~RFICDTC,\n  \"01\", \"2000-01-01\",\n  \"02\", \"2001-01-01\",\n  \"03\", \"\", # Here we have a character blank\n  \"04\", \"2001-01--\",\n  \"05\", \"2001---01\",\n  \"05\", \"\", # Here we have a character blank\n)\n\ndf\n\n# A tibble: 6 × 2\n  USUBJID RFICDTC     \n  &lt;chr&gt;   &lt;chr&gt;       \n1 01      \"2000-01-01\"\n2 02      \"2001-01-01\"\n3 03      \"\"          \n4 04      \"2001-01--\" \n5 05      \"2001---01\" \n6 05      \"\""
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#a-simple-conversion",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#a-simple-conversion",
    "title": "Blanks and NAs",
    "section": "A simple conversion",
    "text": "A simple conversion\n\ndf_na &lt;- convert_blanks_to_na(df)\n\ndf_na\n\n# A tibble: 6 × 2\n  USUBJID RFICDTC   \n  &lt;chr&gt;   &lt;chr&gt;     \n1 01      2000-01-01\n2 02      2001-01-01\n3 03      &lt;NA&gt;      \n4 04      2001-01-- \n5 05      2001---01 \n6 05      &lt;NA&gt;      \n\n\n\ndf_na %&gt;% filter(is.na(RFICDTC))\n\n# A tibble: 2 × 2\n  USUBJID RFICDTC\n  &lt;chr&gt;   &lt;chr&gt;  \n1 03      &lt;NA&gt;   \n2 05      &lt;NA&gt;"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#thats-it",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#thats-it",
    "title": "Blanks and NAs",
    "section": "That’s it!",
    "text": "That’s it!\nA simple call to this function can make your derivation life so much easier while working in R if working with SAS-based datasets. In admiral, we make use of this function at the start of all ADaM templates for common ADaM datasets. You can use the function use_ad_template() to get the full R script for the below ADaMs.\n\nlist_all_templates()\n\nExisting ADaM templates in package 'admiral':\n• ADAE\n• ADCM\n• ADEG\n• ADEX\n• ADLB\n• ADLBHY\n• ADMH\n• ADPC\n• ADPP\n• ADPPK\n• ADSL\n• ADVS"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#last-updated",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#last-updated",
    "title": "Blanks and NAs",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:57.903743"
  },
  {
    "objectID": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#details",
    "href": "posts/2023-07-10_blanks_and_nas/blanks_and_nas.html#details",
    "title": "Blanks and NAs",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "",
    "text": "When creating ADaM Basic Data Structure (BDS) datasets, we often encounter deriving a new parameter based on the analysis values (e.g., AVAL) of other parameters.\nThe admiral function derive_param_computed() adds a parameter computed from the analysis value of other parameters.\nIt works like a calculator to derive new records without worrying about merging and combining datasets, all you need is a derivation formula, which also improves the readability of the code."
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#introduction",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#introduction",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "",
    "text": "When creating ADaM Basic Data Structure (BDS) datasets, we often encounter deriving a new parameter based on the analysis values (e.g., AVAL) of other parameters.\nThe admiral function derive_param_computed() adds a parameter computed from the analysis value of other parameters.\nIt works like a calculator to derive new records without worrying about merging and combining datasets, all you need is a derivation formula, which also improves the readability of the code."
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#example",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#example",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Example",
    "text": "Example\nA value level validation use case, where derive_param_computed() is used to validate a derived parameter - PARAMCD = ADPCYMG (Actual Dose per Cycle) in ADEX dataset.\n\nDerivation\nActual Dose per Cycle is derived from the Total Amount of Dose (PARAMCD = TOTDOSE) / Number of Cycles (PARAMCD = NUMCYC)\nIn this example, ADEX.AVAL when ADEX.PARAMCD = ADPCYMG can be derived as:\n\\[\nAVAL (PARAMCD = ADPCYMG) = \\frac{AVAL (PARAMCD = TOTDOSE)}{AVAL (PARAMCD = NUMCYC)}\n\\]\n\n\nLoading Packages and Creating Example Data\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(diffdf)\nlibrary(admiral)\n\nadex &lt;- tribble(\n  ~USUBJID,  ~PARAMCD,  ~PARAM,                       ~AVAL,\n  \"101\",     \"TOTDOSE\", \"Total Amount of Dose (mg)\",  180,\n  \"101\",     \"NUMCYC\",  \"Number of Cycles\",           3\n)\n\n\n\nDerive New Parameter\n\nadex_admiral &lt;- derive_param_computed(\n  adex,\n  by_vars = exprs(USUBJID),\n  parameters = c(\"TOTDOSE\", \"NUMCYC\"),\n  set_values_to = exprs(\n    PARAMCD = \"ADPCYMG\",\n    PARAM = \"Actual Dose per Cycle (mg)\",\n    AVAL = AVAL.TOTDOSE / AVAL.NUMCYC\n  )\n)\n\n\n\n# A tibble: 3 × 4\n  USUBJID PARAMCD PARAM                       AVAL\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;\n1 101     TOTDOSE Total Amount of Dose (mg)    180\n2 101     NUMCYC  Number of Cycles               3\n3 101     ADPCYMG Actual Dose per Cycle (mg)    60\n\n\n\n\nCompare\nFor validation purpose, the diffdf package is used below to mimic SAS proc compare.\n\nadex_expected &lt;- bind_rows(\n  adex,\n  tribble(\n    ~USUBJID,  ~PARAMCD,  ~PARAM,                       ~AVAL,\n    \"101\",     \"ADPCYMG\", \"Actual Dose per Cycle (mg)\", 60\n  )\n)\n\ndiffdf(adex_expected, adex_admiral, keys = c(\"USUBJID\", \"PARAMCD\"))\n\nNo issues were found!"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#last-updated",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#last-updated",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:53.8988"
  },
  {
    "objectID": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#details",
    "href": "posts/2023-06-27_admiral/valuelevel/derive_param_computed.html#details",
    "title": "Derive a new parameter computed from the value of other parameters",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html",
    "title": "The pharmaverse (hi)story",
    "section": "",
    "text": "The pharmaverse: from motivation to present"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#human-history-and-pharmaverse-context",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#human-history-and-pharmaverse-context",
    "title": "The pharmaverse (hi)story",
    "section": "Human history and pharmaverse context",
    "text": "Human history and pharmaverse context\nSince the Australian Aboriginal, the earliest peoples recorded to have inhabited the Earth and who have been in Australia for at least 65,000 to 80,000 years (Encyclopædia Britannica), human beings live in group. Whether to protect yourself, increase your life expectancy or simply share tasks.\nFor most aspects of life, it doesn’t make sense to think, act or work alone for two main reasons:\n\nYou will spend more energy and time;\nSomeone else may be facing (or have faced) the same situation.\n\nThe English poet John Donne used to say “No man is an island entire of itself; every man is a piece of the continent, a part of the main;”. I can’t disagree with him. And I dare say that Ari Siggaard Knoph (Novo Nordisk), Michael Rimler (GSK), Michael Stackhouse (Atorus), Ross Farrugia (Roche), and Sumesh Kalappurakal (Janssen) can’t disagree with him either. They are the founders of pharmaverse, members of its Council and kindly shared their memories of how independent companies, in mid-2020, worked together in the creation of a set of packages developed to support the clinical reporting pipeline.\nIf you are not familiar with this pipeline, the important thing to know is that, in a nutshell, pharmaceutical companies must follow a bunch of standardized procedures and formats (from Clinical Data Interchange Standards Consortium, CDISC) when submitting clinical results to Health Authorities. The focus is on this: different companies seeking the same standards for outputs.\nParaphrasing Ross Farrugia (Roche) Breaking boundaries through open-source collaboration presentation in R/Pharma 2022 and thinking of the development of a new drug, we are talking about a “post-competitive” scenario: the drug has already been discovered and the companies should “just” produce and deliver standardized results."
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#clinical-reporting-outputs",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#clinical-reporting-outputs",
    "title": "The pharmaverse (hi)story",
    "section": "Clinical reporting outputs",
    "text": "Clinical reporting outputs\nRationally, we can say that companies face the same challenges in these steps of the process. Not so intuitively, we can also say they were working in silos on that before 2018. Just as Isaac Newton and Gottfried W. Leibniz independently developed the theory of infinitesimal calculus, pharmaceutical companies were independently working on R solutions for this pipeline.\nBut on August 16 and 17 of the mentioned year above, they gathered at the first edition of R/Pharma conference to discuss R and open-source tooling for drug development (the reasons why open-source is an advantageous approach can be found in this post written by Stefan Thoma). And according to Isabela Velásquez’s article, Pharmaverse: Packages for clinical reporting workflows, one of the most popular questions in this conference was “Is the package code available or on CRAN?”.\nWell, many of them were. And not necessarily at that date, but just to mention a few: pharmaRTF and Tplyr from Atorus, r2rtf from Merck, rtables from Roche, etc. The thing is that, overall, there were almost 10000 other packages as well (today, almost 20000). And that took to another two questions:\n\nWith this overwhelming number of packages on CRAN, how to find the ones related to solving “clinical reporting problems”?\nOnce the packages were found, how to choose which one to use among those that have the same functional purpose?\n\nSo, again, companies re-started to working in silos to find those answers. But now, in collaborative silos and with common goals: create extremely useful packages to solve pharmaceutical-specific gaps once and solve them well!"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#first-partnerships",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#first-partnerships",
    "title": "The pharmaverse (hi)story",
    "section": "First partnerships",
    "text": "First partnerships\nIn 2020, Michael Stackhouse (Atorus) and Michael Rimler (GSK) talked and formed a partnership between their companies to develop a few more packages, including metacore, to read, store and manipulate metadata for ADaMs/SDTMs in a standardized object; xportr, to create submission compliant SAS transport files and perform pharma specific dataset level validation checks; and logrx (ex-timber), to build log to support reproducibility and traceability of an R script.\nAround the same time, Thomas Neitmann (currently at Denali Therapeutics, then at Roche) and Michael Rimler (GSK) discovered that both were working with ADaM in R, so Thomas Neitmann (currently at Denali Therapeutics, then at Roche), Ross Farrugia (Roche) and Michael Rimler (GSK) saw an opportunity there and GSK started their partnership with Roche to build and release admiral package.\nThe idea of working together, the sense of community, and the appetite from organizations built more and more, with incentive and priority established up into the programming heads council.\nJanssen had a huge effort in building R capabilities going on as well, by releasing tidytlg and envsetup), so eventually Michael Rimler (GSK), Michael Stackhouse (Atorus) and Ross Farrugia (Roche) formalized pharmaverse and formed the council, adding in Sumesh Kalappurakal (Janssen) and Ari Siggaard Knoph (Novo Nordisk) later joined as the fifth council member."
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#release-growth-and-developments",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#release-growth-and-developments",
    "title": "The pharmaverse (hi)story",
    "section": "Release, growth and developments",
    "text": "Release, growth and developments\nAt the end of their presentation “Closing the Gap: Creating an End to End R Package Toolkit for the Clinical Reporting Pipeline.”, in R/Pharma 2021, Ben Straub (GSK) and Eli Miller (Atorus) welcomed the community to the pharmaverse, a curated collection of packages developed by various pharmaceutical companies to support open-source clinical workflows.\nFrom the outset, the name pharmaverse was chosen so that it could be a neutral home, unrelated to any company. Also, it was established as not being a consortium, which means that founders don’t own, fund, or maintain the packages. Some individuals and companies maintain them but often allowing for community contributions and being licensed permissively so that there is always a feeling of community ownership. The focus of pharmaverse early on, and today, is on inter organization cooperation, to build an environment where, if organizations identify that they have a joint problem that they want to solve, this is the right space to work on and release it.\nPharmaverse has grown a lot, at the time of writing this post we have &gt;25 packages recommended in pharmaverse, and this has led to a partnership with PHUSE to get support from their organization and platform, and because they are eager to advance and support pharmaverse mission.\nDespite all its structure, it is impossible to say that we have a single solution for each clinical reporting analysis when it comes to pharmaverse, a single pathway is impractical. Instead, it is necessary to accept viable tools fitting different pathways into pharmaverse to direct and give people options as to what might work for them. After all, even though we live together as a community, we still have our own unique internal problems.\n\n\n\nSample of pharmaverse packages"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#last-updated",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#last-updated",
    "title": "The pharmaverse (hi)story",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:48.182908"
  },
  {
    "objectID": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#details",
    "href": "posts/2023-10-10_pharmaverse_story/pharmaverse_story.html#details",
    "title": "The pharmaverse (hi)story",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "",
    "text": "We are excited to announce that Appsilon and Sanofi will take up the remaining two open council seats on the pharmaverse council!\nAppsilon will on the council be represented by Damian Rodziewicz and Sanofi by Andre Couturier."
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#last-updated",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#last-updated",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:44.539641"
  },
  {
    "objectID": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#details",
    "href": "posts/2024-04-29_appsilon_and__sa.../appsilon_and__sanofi_joins_the_pharmaverse_council!.html#details",
    "title": "Appsilon and Sanofi join the pharmaverse council!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "",
    "text": "Throughout the pharmaverse journey, our focus has always been on achieving equity across pharma, biotech, charity groups and more. We’ve strived to enable any company, no matter how big or small, to prepare clinical trial reporting using cutting edge and freely available solutions in R. The ultimate ambition being that the best treatments have the best chance of reaching patients all over the world. However, in our focus on this mission we took our eyes off other forms of equity – such as the gender diversity of our representatives.\nDiversity is not a new issue in open source – for example there has long been an uneven gender representation across many communities and initiatives in this space. Given the growing influence of pharmaverse we want to strive to use our platform to not further any such imbalance.\nWith respect to gender diversity, the pharmaverse council is currently formed of 7 male members. Our council is and always has been open to any individual put forth by an organization, and anyone who wants to volunteer in the Pharmaverse is welcome. If this perception has differed, then we have to do our part to ensure anyone feels comfortable bringing their contribution forth and engaging in our community.\nRepresentation starts from the top, and as council members complete their 2-year terms, we will advocate within our companies for more diverse candidates to be proactively encouraged to step forward for consideration.\nWe are committed to using our influence to ensure everyone has an equal opportunity to make a difference in our community at all levels, so this discussion has to continue across package development teams and all the various ways people can contribute.\nTo close, we’d like to reflect on the past years by saying that the true strength of what has been built here is not from us on the council, or from the packages or the developers, it really comes from the whole community being built. This is a community of like-minded people that have come together from all walks of life passionate to make a difference and change the siloed trends of our industry. That is what makes the pharmaverse, and we hope that everyone no matter what demographic you belong to feels welcome to join and give your free time to help do more for ALL patients across the world.\nYours faithfully,\npharmaverse council"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#last-updated",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#last-updated",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:40.36483"
  },
  {
    "objectID": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#details",
    "href": "posts/2024-05-31_diversity_&__inc.../diversity_&__inclusion_in_pharmaverse.html#details",
    "title": "Diversity & Inclusion in pharmaverse",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "",
    "text": "The use of R programming in clinical trials has not always been the most popular and obvious in the past. Despite experiencing significant growth in recent years, the adoption of R programming in clinical trials is not as widespread and evident as anticipated. Practical implementation faces obstacles due to various factors, including occasional misunderstandings, particularly in the context of validation, and a notable lack of awareness regarding its capabilities. However, despite these challenges, R is steadily establishing a growing presence within the pharmaceutical industry."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#introduction",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#introduction",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "",
    "text": "The use of R programming in clinical trials has not always been the most popular and obvious in the past. Despite experiencing significant growth in recent years, the adoption of R programming in clinical trials is not as widespread and evident as anticipated. Practical implementation faces obstacles due to various factors, including occasional misunderstandings, particularly in the context of validation, and a notable lack of awareness regarding its capabilities. However, despite these challenges, R is steadily establishing a growing presence within the pharmaceutical industry."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#opportunities-for-r-programming-in-clinical-trials",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#opportunities-for-r-programming-in-clinical-trials",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Opportunities for R Programming in Clinical Trials",
    "text": "Opportunities for R Programming in Clinical Trials\nAlthough R is versatile and applicable in various settings, it is commonly associated with scientific computing and statistics. In the context of clinical trials, where researchers aim to understand and enhance drug development and testing processes, R has become a prominent tool for analyzing the collected data. While SAS® has been a longstanding programming language for clinical trials, the industry has been exploring alternative options. There is a quest for sustainable technology and tools that can effectively address industry challenges.\nTo drive innovation, there is a need to move away from traditional, inefficient processes and tools toward solutions that are efficient, simple, easy to implement, reliable, and cost-effective. Collaboration among industry stakeholders is crucial to develop a robust technology ecosystem and establish consensus on validation and regulatory benchmarks. Equally vital is preparing the workforce with the necessary skillsets to meet future demands."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#current-usage-trends-of-r",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#current-usage-trends-of-r",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Current Usage Trends of R",
    "text": "Current Usage Trends of R\nAnalyzing the current trends of R in the pharmaceutical industry reveals that its usage still has room for growth when related to Pharma Regulatory Submissions. However, R finds extensive use in public health projects, healthcare economics, exploratory and scientific analysis, trend identification, generating plots/graphs, specific statistical analysis, and machine learning. R continues to advance steadily in clinical trials, however lacks widespread usage within the clinical space.\nThis is an area that we see gradually evolving thanks to a number of across-industry efforts such as pharmaverse."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#sas-or-r-programming-which-is-better",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#sas-or-r-programming-which-is-better",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "SAS® or R Programming: Which is Better?",
    "text": "SAS® or R Programming: Which is Better?\n\n\n\nSAS® or R?\n\n\nThe ongoing debate in the programming community revolves around whether to replace SAS® with R, use both, or explore other alternatives like Python. Instead of adopting an either-or scenario, leveraging the strengths of each programming language for specific Data Science problems is recommended, recognizing that one size does not fit all. Despite the challenges early adopters of R have faced in regulatory compliance, there have been notable successes that highlight the benefits and potential of using R in regulated industries. Early adopters of R have faced challenges, with regulatory compliance for R packages being a common issue.\nFor R to be considered for tasks related to regulatory submission, a rigorous risk assessment of R packages, feasibility analysis, and the establishment of processes for R usage through pilot projects with necessary documentation becomes imperative. We see great progress in this area through efforts such as the R Consortium R Submissions WG."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#benefits-of-using-r-programming",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#benefits-of-using-r-programming",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Benefits of Using R Programming",
    "text": "Benefits of Using R Programming\nR, as a language and environment for statistical computing and graphics, possesses characteristics that make it a potentially powerful tool for Data Analysis. With approximately 2 million users worldwide and three decades of legacy, R stands out as open-source software receiving substantial support from the community. Its availability under the GNU General Public License and extensive documentation contribute to its strength. R is versatile, running on various platforms, offering a wide array of statistical and graphical techniques, and its ease of producing publication-quality plots enhances its appeal.\nThe pharmaceutical industry has witnessed the emergence of various R packages tailored for Clinical Trial reporting. Examples include {rtables} for creating tables for reporting clinical trials, {admiral} for CDISC ADaM, {pkglite} to support eSubmission, and many others. Pharmaverse packages cater to different aspects of clinical trial data analysis, showcasing the versatility of R in this domain.\nThis article talks more about use of R in clinical trials and how this will be used by taking advantages of open source of R. The FDA emphasizes the need for fully documenting software packages used for statistical analysis in submissions. The use of R poses specific challenges related to validation, given its free and open-source nature. To address this, the R Validation Hub has released guidance documents focusing in this space.\nGiven that the cost of the R package is non-chargeable, it can also serve as a potential tool for API integration. For instance, in signal detection, R packages can prove to be valuable tools due to the intricate derivation process for EBGM in the Bayesian approach, which aims to mitigate false positive signals resulting from multiple comparisons. The computation adjusts the observed-to-expected reporting ratio for temporal trends and confounding variables such as age and sex. While both methods can estimate this, the accessibility of R as free software enables easy integration into any system as an API or for macro estimation purposes without any copyrights issue. As always though consult the license of any package being used to be sure your usage is in compliance."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#identifying-the-limitations-in-using-r-programming",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#identifying-the-limitations-in-using-r-programming",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Identifying the Limitations in Using R Programming",
    "text": "Identifying the Limitations in Using R Programming\nIt is crucial to note that software cost is essential to any company, including Pharma and Biotechs. While R and RStudio® are free and SAS® requires an annual license, using R instead of SAS® may not always lower costs. The cost of software is only one part of the equation. To be used in a highly regulated industry such as pharmaceuticals, software validation, maintenance, and support are critical, and their costs need to be considered. Although R is free and open source, it comes with a learning curve, and in short term the industry might face a shortage of experienced pharma R programmers compared to those familiar with SAS®."
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#leveraging-the-right-tools-to-capture-value",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#leveraging-the-right-tools-to-capture-value",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Leveraging the Right Tools to Capture Value",
    "text": "Leveraging the Right Tools to Capture Value\nCapturing the value of R programming starts with a clear vision for its use and a systematic approach to identifying and prioritizing the needs in the industry. Clinical Data Science is evolving rapidly, and the industry actively seeks alternative solutions to unlock valuable insights from diverse datasets. Recognizing the need for innovation, collaboration, and efficient tools is crucial. Rather than viewing SAS®, R, and Python as mutually exclusive, leveraging the strengths of each for appropriate Data Science problems provides a nuanced and effective approach.\nEnsuring data quality, scientific integrity, and regulatory compliance through risk assessment frameworks, validation, and documentation are imperative in this dynamic landscape. Pharmaverse is also actively steering the pharmaceutical industry’s path by pioneering connections and advocating for R, thus exemplifying the broader trend of industries acknowledging the value and potential of open-source tools in tackling complex challenges.\n\n\n\nLeveraging the Right Tools"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#last-updated",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#last-updated",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:37.093075"
  },
  {
    "objectID": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#details",
    "href": "posts/2024-04-15_de-_mystifying__.../de-_mystifying__r__programming_in__clinical__trials.html#details",
    "title": "De-Mystifying R Programming in Clinical Trials",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html",
    "title": "Inside the pharmaverse",
    "section": "",
    "text": "Greetings, pharmaverse phriends!\nNow that the pharmaverse has been around for a few years and seems to be making an impact on clinical reporting (yay!), we wanted to take an opportunity to cast a light on some of the inner workings of pharmaverse and its Council (us). Fortunately, some of you (you know who you are) innovated and created this fantastic channel for pharmaverse blogs. We can’t think of a better way to communicate than by the channel that the pharmaverse community itself created."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-inception",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-inception",
    "title": "Inside the pharmaverse",
    "section": "Pharmaverse Inception",
    "text": "Pharmaverse Inception\nThe pharmaverse Council was formed in 2020 as a coming together of 4 like-minded individuals embedded in the industry-wide efforts to increase the use of the R language for clinical trial analysis and reporting. These individuals included Ross Farrugia (Roche), Sumesh Kalappurakal (Johnson & Johnson), Michael Rimler (GSK), and Mike Stackhouse (Atorus). Each had historical experience with R adoption, through their primary roles within their respective organizations.\nCollectively, these 4 individuals saw the value in\n\nreducing the choice set of R packages for users,\nidentifying gaps in available R packages for delivering the clinical data pipeline, and\nencouraging the development of new packages and/or features to close those gaps.\n\nThe result would be a subset of permissively licensed and open-sourced packages that anyone could use (or modify) to suit their specific use cases, thereby reducing the incidence of organizations privately solving typical problems in isolation. For more background, please refer to the pharmaverse Charter.\nThe current pharmaverse Council companies include GSK, Roche, Johnson & Johnson, Atorus, and Novo Nordisk, represented by the original four founders and Ari Siggaard Knoph from Novo Nordisk.\nIn March 2023, the PHUSE Board of Directors approached the pharmaverse Council with a proposal offering support to pharmaverse developers and leadership to advance the pharmaverse mission. The PHUSE proposal aligned with its mission to promote “[s]haring ideas, tools and standards around data, statistical and reporting technologies to advance the future of life sciences.” PHUSE is currently a supporter of the pharmaverse and its objectives, evident through activities such as:\n\nthe provision of access to PHUSE’s communication and file-sharing platforms to package teams,\nthe provision of project management support to package teams (if needed),\nenablement and promotion of pharmaverse innovations and activities, and\nenablement of face-to-face community connections at PHUSE events."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-objective",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#pharmaverse-objective",
    "title": "Inside the pharmaverse",
    "section": "Pharmaverse Objective",
    "text": "Pharmaverse Objective\nPharmaverse aims to promote the collaborative development of curated open-source R packages for clinical reporting in pharma. Indirectly, we also strive to encourage and increase R adoption within the industry and facilitate communication and collaboration among both R users and R developers."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#becoming-a-council-member",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#becoming-a-council-member",
    "title": "Inside the pharmaverse",
    "section": "Becoming a Council Member",
    "text": "Becoming a Council Member\nMembership to the pharmaverse Council is granted to the organization and is open to any type of organization (requirements further described here). In layperson’s terms, the Council member organization must support a representative on the council and demonstrate a commitment to contributions to the pharmaverse codebase. Specifically, “Commitment to at least 2 open source packages under pharmaverse via reviews, hands-on code development, product leads, or other roles which contribute to the design, development, testing, release, and/or maintenance.” If any representative steps away from their council position, the member organization would identify the replacement. Council meeting minutes are published in the pharmaverse GitHub here."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#package-inclusion",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#package-inclusion",
    "title": "Inside the pharmaverse",
    "section": "Package Inclusion",
    "text": "Package Inclusion\nApplications for a package to be included in the pharmaverse may originate from anybody and anywhere. The Council (through its Working Groups) reviews applications and either (i) approves the package into the pharmaverse or (ii) rejects the package with a rationale. When a rejection has been issued, common reasons have been ‘not now’ with feedback to enhance testing, documentation, or other aspects that enrich the package. Working Groups may also steer the applicant to other packages with similar functionality. In this sense, respective package maintainers (product owners) are encouraged to work collaboratively to reduce duplicated features across packages and improve the overall experience of the user base. The Council and Working Groups are tasked with assessing this aspect of the pharmaverse code base, but the decision on implementation ultimately resides with the individual package maintainers.\nPackage reviews are conducted in the open:\n\nExample 1: {riskassessment} from R Validation Hub working group – result successful: https://github.com/pharmaverse/pharmaverse/issues/195\nExample 2: {rhino} from Appsilon – result successful: https://github.com/pharmaverse/pharmaverse/issues/260\nExample 3: {synthetic.cdisc.data} from Roche – result unsuccessful: https://github.com/pharmaverse/pharmaverse/issues/235\n\nAt the time of publication, there were over 250 contributors to pharmaverse packages, across a network of organizations much broader than just the 5 Council member organizations. pharmaverse Council encourages diversity of individual contributors and the organizations they are affiliated with. The decision on product development team rosters is the sole discretion of the packages. Of course, Council member organizations are providing people to support the pharmaverse ecosystem (e.g., developers and product owners). However, both strategic and operational decisions on the development and maintenance of individual packages reside outside the pharmaverse Council."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#user-adoption",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#user-adoption",
    "title": "Inside the pharmaverse",
    "section": "User adoption",
    "text": "User adoption\nPharma companies are free to choose any selection of R packages for clinical reporting and only they can justify their choices. The pharmaverse website states that “[a]nyone is free to choose any selection of pharmaverse recommended software or those from any other source.” In addition, pharmaverse does not seek to engage or get endorsement from any health authority. We are merely trying to provide a public service to help individuals and organizations involved in clinical reporting navigate a vast field of available R packages licensed as open source.\nThe pharmaverse website provides illustrative examples of how to use pharmaverse and other packages to build common deliverables."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#so-what-really-is-pharmaverse",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#so-what-really-is-pharmaverse",
    "title": "Inside the pharmaverse",
    "section": "So, what really is pharmaverse?",
    "text": "So, what really is pharmaverse?\nPharmaverse is essentially two things:\n\nPharmaverse is a list of packages curated by the pharmaverse Council and Working Groups, primarily communicated via the pharmaverse website. Pharmaverse also maintains an R-universe build for ease of use outside GxP settings.\npharmaverse is a community of R users and R developers “working to promote collaborative development of curated open-source R packages for clinical reporting usage in pharma.”\n\nPharmaverse Council provides a Slack workspace to build community amongst all interested parties and serve as a communication platform for individuals and package teams. At present, membership to this workspace exceeds &gt;1200 members.\nPharmaverse Council also provides a GitHub organization for developers to work in, but hosting packages in the pharmaverse GitHub is not required to be part of pharmaverse.\nThese elements are supported on an all-volunteer basis, mostly with community versions of various platforms (e.g., we use the free tier on Slack)."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#thank-you",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#thank-you",
    "title": "Inside the pharmaverse",
    "section": "Thank you",
    "text": "Thank you\nThank you, each of you, for the part that you play. No matter how big or small, you are helping amplify the impact that open collaboration is having on how we deliver new medicines and vaccines to patients around the world. We hope you are finding, and continue to find, pharmaverse a valuable piece of the clinical reporting puzzle.\nAnd, if not – let us know!\nPharmaverse Council – Ari, Michael, Mike, Ross, and Sumesh\nDisclaimer: This blog contains opinions that are of the authors alone and do not necessarily reflect the strategy of their respective organizations."
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#last-updated",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#last-updated",
    "title": "Inside the pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:33.319719"
  },
  {
    "objectID": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#details",
    "href": "posts/2024-03-04_inside_the__phar.../inside_the__pharmaverse.html#details",
    "title": "Inside the pharmaverse",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "",
    "text": "Date and time is collected in SDTM as character values using the extended ISO 8601 format yyyy-dd-mmThh:mm:ss. This universal format allows missing parts date or time - e.g. the string\"2019-10\" represents a date where the day and the time are unknown. In contrast, ADaM timing variables like ADTM (Analysis Datetime) or ADY (Analysis Relative Day) are numeric variables, which can be derived only if the date or datetime is complete.\nMost ADaM programmers have, at one point or another, encountered situations where missing dates, unexpected formats or confusing imputation functions rendered derivations of timing variables frustrating and time consuming. {admiral} aims to mitigate this (where possible!) by providing functions which automatically derive date/datetime variables for you, and fill in missing date or time parts according to well-defined imputation rules.\nIn this article, we first examine the arsenal of functions provided by{admiral} to aid in datetime imputation and timing variable derivation. We then observe everything in action through a number of selected typical examples."
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-a-partial-date-portion",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-a-partial-date-portion",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputing a Partial Date Portion",
    "text": "Imputing a Partial Date Portion\nIt is easy impute dates to the first day/month if they are partial just by using the highest_imputation argument:\n\nlibrary(admiral)\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(dplyr, warn.conflicts = FALSE)\n\ndates &lt;- c(\n  \"2019-07-18T15:25:40\",\n  \"2019-07-18T15:25\",\n  \"2019-07-18T15\",\n  \"2019-07-18\",\n  \"2019-02\",\n  \"2019\",\n  \"2019\",\n  \"2019---07\",\n  \"\"\n)\n\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\"\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-01\"\n[6] \"2019-01-01\" \"2019-01-01\" \"2019-01-01\" NA          \n\n\nA simple modification using date_imputation = \"mid\" or date_imputation = \"last\" or enables the imputation to be made using the middle or last day/month instead:\n\n# Impute to last day/month if date is partial\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\",\n  date_imputation = \"last\",\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-28\"\n[6] \"2019-12-31\" \"2019-12-31\" \"2019-12-31\" NA          \n\n# Impute to mid day/month if date is partial\nimpute_dtc_dt(\n  dtc = dates,\n  highest_imputation = \"M\",\n  date_imputation = \"mid\"\n)\n\n[1] \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-07-18\" \"2019-02-15\"\n[6] \"2019-06-30\" \"2019-06-30\" \"2019-06-30\" NA          \n\n\nBut what if there exist minimum dates that the imputed date cannot exceed? Here, the min_date argument comes to the rescue:\n\nimpute_dtc_dt(\n  \"2020-12\",\n  min_dates = list(\n    ymd(\"2020-12-06\"),\n    ymd(\"2020-11-11\")\n  ),\n  highest_imputation = \"M\"\n)\n\n[1] \"2020-12-06\""
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#computing-date-imputation-flags",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#computing-date-imputation-flags",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Computing Date Imputation Flags",
    "text": "Computing Date Imputation Flags\nWhen it comes to carrying out an imputation, the twin task is to flag the type of imputation that was executed. Here, functions like compute_dtf() make this straightforward. For this function, all that needs to be done is to pass a date character date to the dtc argument, and the resulting imputed date to the dt argument. This will then return the right date imputation flag - see the examples below for some possible behaviors:\n\ncompute_dtf(dtc = \"2019-07\", dt = as.Date(\"2019-07-18\"))\n\n[1] \"D\"\n\ncompute_dtf(dtc = \"2019\", dt = as.Date(\"2019-07-18\"))\n\n[1] \"M\"\n\ncompute_dtf(dtc = \"--06-01T00:00\", dt = as.Date(\"2022-06-01\"))\n\n[1] \"Y\""
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-datetime-and-date-variable-and-imputation-flag-variables",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-datetime-and-date-variable-and-imputation-flag-variables",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Creating an Imputed Datetime and Date Variable and Imputation Flag Variables",
    "text": "Creating an Imputed Datetime and Date Variable and Imputation Flag Variables\nAs described previously, derive_vars_dtm() derives an imputed datetime variable and the corresponding date and time imputation flags. The imputed date variable can then be derived by using derive_vars_dtm_to_dt(). It is not necessary and advisable to perform the imputation for the date variable if it was already done for the datetime variable. CDISC considers the datetime and the date variable as two representations of the same date. Thus the imputation must be the same and the imputation flags are valid for both the datetime and the date variable.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\",\n    time_imputation = \"first\"\n  ) %&gt;%\n  derive_vars_dtm_to_dt(exprs(ASTDTM))\n\n\n\n\n\n\nAESTDTC\nASTDTM\nASTDTF\nASTTMF\nASTDT\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09 12:34:56\nNA\nNA\n2019-08-09\n\n\n2019-04-12\n2019-04-12 00:00:00\nNA\nH\n2019-04-12\n\n\n2010-09\n2010-09-01 00:00:00\nD\nH\n2010-09-01\n\n\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-date-variable-and-imputation-flag-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#creating-an-imputed-date-variable-and-imputation-flag-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Creating an Imputed Date Variable and Imputation Flag Variable",
    "text": "Creating an Imputed Date Variable and Imputation Flag Variable\nIf an imputed date variable without a corresponding datetime variable is required, it can be derived by the derive_vars_dt() function.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dt(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\"\n  )\n\n\n\n\n\n\nAESTDTC\nASTDT\nASTDTF\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09\nNA\n\n\n2019-04-12\n2019-04-12\nNA\n\n\n2010-09\n2010-09-01\nD\n\n\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-time-without-imputing-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputing-time-without-imputing-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputing Time Without Imputing Date",
    "text": "Imputing Time Without Imputing Date\nIf the time should be imputed but not the date, the highest_imputation argument should be set to \"h\". This results in NA if the date is partial. As no date is imputed the date imputation flag is not created.\n\nae &lt;- tribble(\n  ~AESTDTC,\n  \"2019-08-09T12:34:56\",\n  \"2019-04-12\",\n  \"2010-09\",\n  NA_character_\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"h\",\n    time_imputation = \"first\"\n  )\n\n\n\n\n\n\nAESTDTC\nASTDTM\nASTTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-08-09 12:34:56\nNA\n\n\n2019-04-12\n2019-04-12 00:00:00\nH\n\n\n2010-09\nNA\nNA\n\n\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-before-a-particular-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-before-a-particular-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Avoiding Imputed Dates Before a Particular Date",
    "text": "Avoiding Imputed Dates Before a Particular Date\nUsually an adverse event start date is imputed as the earliest date of all possible dates when filling the missing parts. The result may be a date before treatment start date. This is not desirable because the adverse event would not be considered as treatment emergent and excluded from the adverse event summaries. This can be avoided by specifying the treatment start date variable (TRTSDTM) for the min_dates argument.\nImportantly, TRTSDTM is used as imputed date only if the non missing date and time parts of AESTDTC coincide with those of TRTSDTM. Therefore 2019-10 is not imputed as 2019-11-11 12:34:56. This ensures that collected information is not changed by the imputation.\n\nae &lt;- tribble(\n  ~AESTDTC,              ~TRTSDTM,\n  \"2019-08-09T12:34:56\", ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-10\",             ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-11\",             ymd_hms(\"2019-11-11T12:34:56\"),\n  \"2019-12-04\",          ymd_hms(\"2019-11-11T12:34:56\")\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AESTDTC,\n    new_vars_prefix = \"AST\",\n    highest_imputation = \"M\",\n    date_imputation = \"first\",\n    time_imputation = \"first\",\n    min_dates = exprs(TRTSDTM)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAESTDTC\nTRTSDTM\nASTDTM\nASTDTF\nASTTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-11-11 12:34:56\n2019-08-09 12:34:56\nNA\nNA\n\n\n2019-10\n2019-11-11 12:34:56\n2019-10-01 00:00:00\nD\nH\n\n\n2019-11\n2019-11-11 12:34:56\n2019-11-11 12:34:56\nD\nH\n\n\n2019-12-04\n2019-11-11 12:34:56\n2019-12-04 00:00:00\nNA\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-after-a-particular-date",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#avoiding-imputed-dates-after-a-particular-date",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Avoiding Imputed Dates After a Particular Date",
    "text": "Avoiding Imputed Dates After a Particular Date\nIf a date is imputed as the latest date of all possible dates when filling the missing parts, it should not result in dates after data cut off or death. This can be achieved by specifying the dates for the max_dates argument.\nImportantly, non missing date parts are not changed. Thus 2019-12-04 is imputed as 2019-12-04 23:59:59 although it is after the data cut off date. It may make sense to replace it by the data cut off date but this is not part of the imputation. It should be done in a separate data cleaning or data cut off step.\n\nae &lt;- tribble(\n  ~AEENDTC,              ~DTHDT,            ~DCUTDT,\n  \"2019-08-09T12:34:56\", ymd(\"2019-11-11\"), ymd(\"2019-12-02\"),\n  \"2019-11\",             ymd(\"2019-11-11\"), ymd(\"2019-12-02\"),\n  \"2019-12\",             NA,                ymd(\"2019-12-02\"),\n  \"2019-12-04\",          NA,                ymd(\"2019-12-02\")\n) %&gt;%\n  derive_vars_dtm(\n    dtc = AEENDTC,\n    new_vars_prefix = \"AEN\",\n    highest_imputation = \"M\",\n    date_imputation = \"last\",\n    time_imputation = \"last\",\n    max_dates = exprs(DTHDT, DCUTDT)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAEENDTC\nDTHDT\nDCUTDT\nAENDTM\nAENDTF\nAENTMF\n\n\n\n\n2019-08-09T12:34:56\n2019-11-11\n2019-12-02\n2019-08-09 12:34:56\nNA\nNA\n\n\n2019-11\n2019-11-11\n2019-12-02\n2019-11-11 23:59:59\nD\nH\n\n\n2019-12\nNA\n2019-12-02\n2019-12-02 23:59:59\nD\nH\n\n\n2019-12-04\nNA\n2019-12-02\n2019-12-04 23:59:59\nNA\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputation-without-creating-a-new-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#imputation-without-creating-a-new-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Imputation Without Creating a New Variable",
    "text": "Imputation Without Creating a New Variable\nIf imputation is required without creating a new variable the convert_dtc_to_dt() function can be called to obtain a vector of imputed dates. It can be used for example here:\n\nmh &lt;- tribble(\n  ~MHSTDTC,     ~TRTSDT,\n  \"2019-04\",    ymd(\"2019-04-15\"),\n  \"2019-04-01\", ymd(\"2019-04-15\"),\n  \"2019-05\",    ymd(\"2019-04-15\"),\n  \"2019-06-21\", ymd(\"2019-04-15\")\n) %&gt;%\n  filter(\n    convert_dtc_to_dt(\n      MHSTDTC,\n      highest_imputation = \"M\",\n      date_imputation = \"first\"\n    ) &lt; TRTSDT\n  )\n\n\n\n\n\n\nMHSTDTC\nTRTSDT\n\n\n\n\n2019-04\n2019-04-15\n\n\n2019-04-01\n2019-04-15"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#using-more-than-one-imputation-rule-for-a-variable",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#using-more-than-one-imputation-rule-for-a-variable",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Using More Than One Imputation Rule for a Variable",
    "text": "Using More Than One Imputation Rule for a Variable\nUsing different imputation rules depending on the observation can be done by using the higher-order function slice_derivation(), which applies a derivation function differently (by varying its arguments) in different subsections of a dataset. For example, consider this Vital Signs case where pre-dose records require a different treatment to other records:\n\nvs &lt;- tribble(\n  ~VSDTC,                ~VSTPT,\n  \"2019-08-09T12:34:56\", NA,\n  \"2019-10-12\",          \"PRE-DOSE\",\n  \"2019-11-10\",          NA,\n  \"2019-12-04\",          NA\n) %&gt;%\n  slice_derivation(\n    derivation = derive_vars_dtm,\n    args = params(\n      dtc = VSDTC,\n      new_vars_prefix = \"A\"\n    ),\n    derivation_slice(\n      filter = VSTPT == \"PRE-DOSE\",\n      args = params(time_imputation = \"first\")\n    ),\n    derivation_slice(\n      filter = TRUE,\n      args = params(time_imputation = \"last\")\n    )\n  )\n\n\n\n\n\n\nVSDTC\nVSTPT\nADTM\nATMF\n\n\n\n\n2019-08-09T12:34:56\nNA\n2019-08-09 12:34:56\nNA\n\n\n2019-11-10\nNA\n2019-11-10 23:59:59\nH\n\n\n2019-12-04\nNA\n2019-12-04 23:59:59\nH\n\n\n2019-10-12\nPRE-DOSE\n2019-10-12 00:00:00\nH"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#last-updated",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#last-updated",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:29.272767"
  },
  {
    "objectID": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#details",
    "href": "posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html#details",
    "title": "Date/Time Functions and Imputation in {admiral}",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "",
    "text": "Pic 1: ISCR 17th Annual Conference 2024\n\n\nIndian Society for Clinical Research (ISCR), launched in June 2005, is a not-for-profit professional association of all stakeholders in clinical research.\nISCR hosted its 17th Annual Conference 2024 at Hotel Novotel HICC, Hyderabad, INDIA on the theme Transformations in Clinical Research For Better Patient Outcomes, with Pre-Conference Workshops held on February 1, 2024 (Thursday) and two-day main Conference held on February 02 & 03, 2024 (Friday-Saturday), which were attended by over 800 delegates from academic institutions, ethics committees, bio-pharmaceutical industry, government, patient organizations and clinical research organizations."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#iscr-17th-annual-conference-2024",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#iscr-17th-annual-conference-2024",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "",
    "text": "Pic 1: ISCR 17th Annual Conference 2024\n\n\nIndian Society for Clinical Research (ISCR), launched in June 2005, is a not-for-profit professional association of all stakeholders in clinical research.\nISCR hosted its 17th Annual Conference 2024 at Hotel Novotel HICC, Hyderabad, INDIA on the theme Transformations in Clinical Research For Better Patient Outcomes, with Pre-Conference Workshops held on February 1, 2024 (Thursday) and two-day main Conference held on February 02 & 03, 2024 (Friday-Saturday), which were attended by over 800 delegates from academic institutions, ethics committees, bio-pharmaceutical industry, government, patient organizations and clinical research organizations."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#session-recap",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#session-recap",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Session Recap",
    "text": "Session Recap\nI had the privilege to present in front of 100+ delegates across the industry ranging from freshers to seasoned clinical professionals during ISCR 17th Annual Conference 2024 on the topic Travese the PHARMAVERSE: ouR Insights in the Biostatistics and Statistical Programming|02-Feb-2024 track including many more interesting presentations highlighting their experience with R submissions using various open source technologies.\n\n\n\nPic 2: Source - LinkedIn\n\n\nThe session consisted of three presentations, namely:\n\nA real world insight and navigation on bridging FDA submission using R by Soumitra Kar & Mahendran Venkatachalam\nTraverse the ‘PHARMAVERSE’ : ouR insights by Pooja Kumari\nPackage in CRAN : {admiralvaccine} by Divya Kanagaraj and Arjun R\n\nIt was inaugurated with great enthusiast and sharing insights on Opportunities/Challenges of using different technologies like R in regulatory Submissions by the session chair Soumitra Kar. He along with his co-presenter Mahendran Venkatachalam shared their experience of submitting first R-based Submission to FDA. The presentation was a perfect combination of inspiring storytelling, climax and thrill to address FDA review comments and releasing the blockbuster R submission by Novo-Nordisk creating history.\nThis was followed by my presentation on Travese the PHARMAVERSE: ouR Insights, wherein I gave a brief introduction to PHARMAVERSE universe and how we operate. Many R enthusiasts are well versed with the evolution of {admiral} and its propensity to develop ADaMs. However, very few know about other packages such as {metacore}, {metatools}, {xportr} which are developed considering the regulatory agency guidelines and can aid the process of creating ADaM datasets proficiently. I took the opportunity to supercharge the process knowledge of creating submission ready ADaMs covering end-to-end process using these PHARMAVERSE packages along with some to R submission success stories.\n\n\n\nPresentation: Travese the PHARMAVERSE: ouR Insights\n\n\nNext presentation was on {admiralvaccine}, an extension package of {admiral} specific to vaccine studies under the PHARMAVERSE universe by Divya Kanagaraj and Arjun R. They shared their exciting journey of developing the package since inception to final CRAN release from a developer’s perspective. They also talked about the collaborative effort that went into its successful release.\nOverall session was concluded with an interactive Q&A wherein all the presenters and presentations were applauded by the audience as well as the Scientific Committee members. It was an enriching session to witness the growth of R programming leading to R submissions in Clinical Research & Pharmaceutical Industry.\n\n\n\nPic 3: Biostatistics and Statistical Programming | 02-Feb-2024, Session 4, Audience Q&A round, Left to right: Pooja Kumari, GSK; Dhivya Kanagaraj, Pfizer; Arjun Rubalingam, Pfizer; Soumitra Kar, Novo Nordisk; Mahendran Venkatachalam, Novo Nordisk"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#key-takeaways",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#key-takeaways",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe two-day conference was full of great learning and meeting esteemed Clinical Pharmaceutical Industry veterans/newbies discussing on trending topics such as Optimizing Clinical Research through effective collaboration between Statisticians and Statistical Programmers, Can new technologies (AI/ML/IOT) a threat or blessing for Biostatisticians and Statistical Programmers? through Panel discussions.\nThe power of technology coupled with domain expertise can make us deliver quality results faster and serve the world with disease-free healthy life.\n\nWhy should we attend Conferences?\n\nConferences are the best place to Connect, Collaborate and Communicate your thoughts with like-minded tribe.\nBiostatistics and Clinical Statistical Programming industry is growing and adopting open source technologies with great acceptance. As an individual we can contribute to communities like PHARMAVERSE to enhance our end-to-end process knowledge, develop programming skills and contribute to a revolutionary concept.\nIt gives you a platform to strengthen your presentation as well as self-branding skills."
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#gallery",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#gallery",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Gallery",
    "text": "Gallery\n\n\n\n\n\n\n\n\n\nPic 4: GCC GSK Biostatistics-India reps. at ISCR, Left to Right: Pooja Kumari; Abhishek Mishra\n\n\n\n\n\n\n\nPic 5: Keep Calm and Explore PHARMAVERSE"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#last-updated",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#last-updated",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:25.387694"
  },
  {
    "objectID": "posts/2024-02-14_iscr_conference/iscr_conference.html#details",
    "href": "posts/2024-02-14_iscr_conference/iscr_conference.html#details",
    "title": "ISCR 17th Annual Conference 2024",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html",
    "href": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "",
    "text": "Cytel is the first CRO involved in the {admiral} open-source project. Having the ambition to demonstrate our skills in the open-source projects, it was with lots of excitement that we accepted the challenge to enter the {admiral} development team family. We are thus thrilled to share with you this new & challenging experience."
  },
  {
    "objectID": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html#last-updated",
    "href": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html#last-updated",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:21.59896"
  },
  {
    "objectID": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html#details",
    "href": "posts/2024-05-21_our_experience_a.../our_experience_as_new_admiral_developers.html#details",
    "title": "Our experience as new admiral developers, coming from a CRO",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "R/readme.html",
    "href": "R/readme.html",
    "title": "Files in this folder",
    "section": "",
    "text": "Some of these files help in creating/developing blog-posts, others are used by our CICD pipeline.\n\ncreate_blogpost.R: use this script to create a new blogpost based on our blogpost template.\nCICD.R: use this script to spellcheck and stylecheck your blogpost.\n\n\n\n\nhelp_create_blogpost.R: script containing the function(s) used by create_blogpost.R\nswitch.R: Used by CICD spellcheck workflow."
  },
  {
    "objectID": "R/readme.html#development-files",
    "href": "R/readme.html#development-files",
    "title": "Files in this folder",
    "section": "",
    "text": "help_create_blogpost.R: script containing the function(s) used by create_blogpost.R\nswitch.R: Used by CICD spellcheck workflow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the pharmaverse blog!",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDiversity & Inclusion in pharmaverse\n\n\n\n\n\n\ncommunity\n\n\n\nAn update from pharmaverse council regarding diversity and inclusion within our community.\n\n\n\n\n\nMay 31, 2024\n\n\npharmaverse council\n\n\n\n\n\n\n\n\n\n\n\n\nOur experience as new admiral developers, coming from a CRO\n\n\n\n\n\n\nCommunity\n\n\nConferences\n\n\n\nAs the first CRO having joined the {admiral} family, we are very excited to share with you our first steps and thoughts in this new adventure\n\n\n\n\n\nMay 29, 2024\n\n\nFanny Gautier, Lina Patil\n\n\n\n\n\n\n\n\n\n\n\n\nTLG Catalog 🤝 WebR\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nIntroducing WebR to TLG Catalog: A Game Changer for Interactive Learning\n\n\n\n\n\nMay 8, 2024\n\n\nPawel Rucki\n\n\n\n\n\n\n\n\n\n\n\n\nDe-Mystifying R Programming in Clinical Trials\n\n\n\n\n\n\nCommunity\n\n\n\nA blog highlighting the benefits/limitations of using R Programming and using the right tools to create value\n\n\n\n\n\nMay 2, 2024\n\n\nVenkatesan Balu\n\n\n\n\n\n\n\n\n\n\n\n\nAppsilon and Sanofi join the pharmaverse council!\n\n\n\n\n\n\nCommunity\n\n\n\nUpdates to the pharmaverse council\n\n\n\n\n\nApr 29, 2024\n\n\nAri Siggaard Knoph\n\n\n\n\n\n\n\n\n\n\n\n\nteal.modules.clinical v0.9.0 is now on CRAN!\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nThis package release now completes the suite of {teal} family of packages recently released to CRAN.\n\n\n\n\n\nApr 8, 2024\n\n\nLeena Khatri\n\n\n\n\n\n\n\n\n\n\n\n\nxportr 0.4.0\n\n\n\n\n\n\nADaM\n\n\nSDTM\n\n\n\nxportr is a tool tailor-made for clinical programmers, enabling them to generate CDISC-compliant XPT files for SDTM and/or ADaM. It streamlines the preparation on xpt files, ensuring they are primed for submission to clinical data validation applications or regulatory agencies.\n\n\n\n\n\nMar 29, 2024\n\n\nSadchla Mascary\n\n\n\n\n\n\n\n\n\n\n\n\nTips for First Time Contributors\n\n\n\n\n\n\nCommunity\n\n\n\nOpen source can be daunting the first time you dive in - this blog will help you get started!\n\n\n\n\n\nMar 11, 2024\n\n\nRoss Farrugia\n\n\n\n\n\n\n\n\n\n\n\n\nInside the pharmaverse\n\n\n\n\n\n\nCommunity\n\n\n\nA short blog to help the Pharmaverse community understand how Pharmaverse is governed.\n\n\n\n\n\nMar 4, 2024\n\n\nMichael Rimler\n\n\n\n\n\n\n\n\n\n\n\n\nRhino: A Step Forward in Validating Shiny Apps\n\n\n\n\n\n\nSubmission\n\n\nCommunity\n\n\n\nIn this post, we explore the importance of validation in the pharmaceutical industry and how Rhino framework aids in ensuring reliability and accuracy of Shiny applications.\n\n\n\n\n\nMar 1, 2024\n\n\nKamil Żyła, Ege Can Taşlıçukur, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nISCR 17th Annual Conference 2024\n\n\n\n\n\n\nConferences\n\n\n\nThis blog highlights my experience of presenting at Indian Society for Clinical Research (ISCR) 17th Annual Conference 2024.\n\n\n\n\n\nFeb 26, 2024\n\n\nPooja Kumari\n\n\n\n\n\n\n\n\n\n\n\n\nFilter out the noise!\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nA brief exposition of the filter_* functions in {admiral} - what they do and how to use them.\n\n\n\n\n\nFeb 26, 2024\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nteal is now available on CRAN 🎉\n\n\n\n\n\n\nTLG\n\n\nShiny\n\n\n\nAnnouncing the release of teal v0.15.0 on CRAN!\n\n\n\n\n\nFeb 14, 2024\n\n\nDony Unardi\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Containers and WebAssembly in Submissions to the FDA\n\n\n\n\n\n\nSubmission\n\n\n\nIn this post, we dig into the ongoing efforts undertaken to evaluate new technologies for submissions to the Food and Drug Administration (FDA). These transformative approaches, including WebAssembly and containers, hold immense potential to transform the regulatory landscape and streamline the submission process.\n\n\n\n\n\nFeb 1, 2024\n\n\nAndré Veríssimo, Tymoteusz Makowski, Pedro Silva, Vedha Viyash, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nPK Examples\n\n\n\n\n\n\nCommunity\n\n\nADaM\n\n\nMetadata\n\n\n\nExplore PK ADaM Examples on Pharmaverse Examples Page\n\n\n\n\n\nJan 26, 2024\n\n\nJeff Dickinson\n\n\n\n\n\n\n\n\n\n\n\n\nadmiral 1.0.0\n\n\n\n\n\n\nADaM\n\n\n\n1.0.0 brings a commitment to stability, new features, a few bug fixes, argument alignment and onboarding resources!\n\n\n\n\n\nJan 4, 2024\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nEnd of Year Update from the pharmaverse Council\n\n\n\n\n\n\nCommunity\n\n\n\n2023 Was a big year - let’s talk about it!\n\n\n\n\n\nJan 4, 2024\n\n\nMike Stackhouse\n\n\n\n\n\n\n\n\n\n\n\n\nBelieve in a higher order!\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nA brief foray into the higher order functions in the {admiral} package.\n\n\n\n\n\nNov 27, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nFloating point\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nThe untold story of how admiral deals with floating points.\n\n\n\n\n\nOct 30, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing the R Submissions Pilot 2 Shiny Application using Rhino\n\n\n\n\n\n\nSubmission\n\n\nCommunity\n\n\n\nA short blog post about a Rhino pilot submission.\n\n\n\n\n\nOct 10, 2023\n\n\nIsmael Rodriguez, Vedha Viyash, André Veríssimo, APPSILON\n\n\n\n\n\n\n\n\n\n\n\n\nDate/Time Functions and Imputation in {admiral}\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nDates, times and imputation can be a frustrating facet of any programming language. Here’s how {admiral} makes all of this easy!\n\n\n\n\n\nSep 26, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nThe pharmaverse (hi)story\n\n\n\n\n\n\nCommunity\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nNicholas Eugenio\n\n\n\n\n\n\n\n\n\n\n\n\nRounding\n\n\n\n\n\n\nTechnical\n\n\n\nExploration of some commonly used rounding methods and their corresponding functions in SAS and R, with a focus on ‘round half up’ and reliable solutions for numerical precision challenges.\n\n\n\n\n\nAug 22, 2023\n\n\nKangjie Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s all relative? - Calculating Relative Days using admiral\n\n\n\n\n\n\nADaM\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nCross-Industry Open Source Package Development\n\n\n\n\n\n\nCommunity\n\n\nConferences\n\n\nADaM\n\n\n\nThis post is based on a talk given at the regional useR! conference on July 21st 2023 in Basel.\n\n\n\n\n\nJul 25, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Code Sections\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nCode sections - are you using them right?\n\n\n\n\n\nJul 14, 2023\n\n\nEdoardo Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nBlanks and NAs\n\n\n\n\n\n\nADaM\n\n\nTechnical\n\n\n\nReading SAS datasets into R. The data is not always as it seems!\n\n\n\n\n\nJul 10, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nfalcon\n\n\n\n\n\n\nTLG\n\n\n\nThe {falcon} initiative is an industry collaborative effort under pharmaversewith the aspiration of building and open-sourcing a comprehensive catalog of harmonized TLGs for clinical study reporting.\n\n\n\n\n\nJul 9, 2023\n\n\nVincent Shen\n\n\n\n\n\n\n\n\n\n\n\n\nHello pharmaverse\n\n\n\n\n\n\nCommunity\n\n\n\nShort, fun and user-driven content around the pharmaverse.\n\n\n\n\n\nJun 28, 2023\n\n\nBen Straub\n\n\n\n\n\n\n\n\n\n\n\n\nHackathon Feedback Application\n\n\n\n\n\n\nShiny\n\n\nCommunity\n\n\nADaM\n\n\n\nGoing through the process of creating a shiny app for the admiral hackathon. The shiny app allows users to check their solutions autonomously, gives feedback, and rates their results.\n\n\n\n\n\nJun 27, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nAdmiral Hackathon 2023 Revisited\n\n\n\n\n\n\nCommunity\n\n\nADaM\n\n\n\nLet’s have a look at the Admiral Hackathon 2023 together.\n\n\n\n\n\nJun 27, 2023\n\n\nStefan Thoma\n\n\n\n\n\n\n\n\n\n\n\n\nDerive a new parameter computed from the value of other parameters\n\n\n\n\n\n\nADaM\n\n\n\nUse admiral::derive_param_computed() like a calculator to derive new parameters/rows based on existing ones\n\n\n\n\n\nJun 27, 2023\n\n\nKangjie Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html",
    "href": "posts/2023-06-27__hackathon_app/index.html",
    "title": "Hackathon Feedback Application",
    "section": "",
    "text": "We recently created a shiny application for the admiral hackathon in February 2023. The admiral hackathon was an event designed to make statistical programmers from the pharmaceutical industry more comfortable with the admiral R package which allows users to efficiently transform data from one data standard (SDTM) to another (ADaM).\nHackathon participants formed groups of up to five people and were then tasked to create R-scripts that map the SDTM data to ADaM according to specifics defined in the metadata.\nThe purpose of the shiny app was threefold:\nIn this blog post I want to highlight some of the thoughts that went into this application. Please keep in mind that this work was done under tight time restraints.\nThe hackathon application is still online (although data-upload is switched off) and the GitHub repository is publicly available. The application is embedded into this post right after this paragraph. I have also uploaded to GitHub a .zip file of the workspace to which hackathon participants had access via posit cloud. For more context you can watch recordings of the hackathon-meetings."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#permanent-data",
    "href": "posts/2023-06-27__hackathon_app/index.html#permanent-data",
    "title": "Hackathon Feedback Application",
    "section": "Permanent Data",
    "text": "Permanent Data\nThe biggest challenge you have to consider for this app is the permanent data storage. Shiny apps run on a server. Although we can write files on this server, whenever the app restarts, the files are lost. Therefore, a persistent data storage solution is required.\n\nGoogle drive\nI decided to leverage Google drive using the googledrive package package. This allowed me to save structured data (the team registry and the submission scores) as well as unstructured data (their R-script files).\n\n\n\n\n\n\nAuthentication\n\n\n\nTo access Google drive using the googledrive package we need to authenticate. This can be done interactively using the command googledrive::drive_auth() which takes you to the Google login page. After login you receive an authentication token requested by R.\nFor non-interactive authentication this token must be stored locally. In our case where the shiny app must access the token once deployed, the token must be stored on the project level.\nI have included the authentication procedure I followed in the R folder in google_init.R. You can find more extensive documentation of the non-interactive authentication.\n\n\nThe initial concept was: Each team gets their own folder including the most recent submission for each task, and a .csv file containing team information. To keep track of the submissions and the respective scores we wrote a .csv file in the mock-hackathon folder, so one folder above the team folders.\nSaving the team info as a .csv file worked fine as each team received their own file which – once created – was not touched anymore. As each upload for every team should simply add a row to the submissions.csv file, appending the file would be ideal. This was not possible using the googledrive package package. Instead, for each submission, the submissions file was downloaded, appended, and uploaded again. Unfortunately, this lead to a data loss, as the file was continuously overwritten, especially when two teams would submit simultaneously.\n\n\n\n\n\n\nRecover the Lost Data\n\n\n\nWhenever the submissions.csv file was uploaded, the previous version was sent to the Google drive bin. We ended up with over 3000 submissions.csv files containing a lot of redundant information. I had to write the following chunk to first get the unique file IDs of the 3000 submissions.csv files, create an empty submissions data-frame, and then download each file and add its information to the submisisons data-frame. To keep the data-frame as light as possible, after each append I deleted all duplicate submissions.\n\n# get all task_info.csv ID's\n# each row identifies one file in the trash\ntask_info_master &lt;- drive_find(\n  pattern = \"task_info.csv\",\n  trashed = TRUE\n)\n\n\n# set up empty df to store all submissions\norigin &lt;- tibble(\n  score = numeric(),\n  task = character(),\n  team = character(),\n  email = character(),\n  time = character()\n)\n\n# downloads, reads, and returns one csv file given a file id\nget_file &lt;- function(row) {\n  tf &lt;- tempfile()\n  row %&gt;%\n    as_id() %&gt;%\n    drive_download(path = tf)\n  new &lt;- read_csv(tf) %&gt;%\n    select(score, task, team) %&gt;%\n    distinct()\n}\n\n\n# quick and dirty for loop to subsequently download each file, extract information\n#  merge with previous information and squash it (using distinct()).\nfor (i in 1:nrow(task_info_master)) {\n  origin &lt;- rbind(origin, get_file(row = task_info_master[i, ])) %&gt;%\n    distinct()\n\n  # save progress in a separate file after every 100 downloaded and merged sheets\n  if (i %% 100 == 0) {\n    print(i)\n    write_csv(origin, paste(\"prog_data/task_info_prog_\", i, \".csv\", sep = \"\"))\n    # update on progress\n    message(i / nrow(task_info_master) * 100)\n  }\n}\n\nWhen doing such a time-intensive task, make sure to try it first with only a couple of files to see whether any errors are produced. I am not quite sure how long this took but when I returned from my lunch break everything had finished.\n\n\nIf you want to stay in the Google framework, I recommend using the googlesheets4 package for structured data. googlesheets4 allows appending new information to an already existing sheet without the need to download the file first. As both packages follow the same style, going from one to the other is really simple. googlesheets4 requires authentication as well. However, you can reuse the cached token from the googledrive package authentication by setting gs4_auth(token = drive_token()).\n\n\nSecurity Concerns\nConnecting a public shiny app to your Google account introduces a security vulnerability in general. Especially so because we implemented the upload of files to Google drive. And even more problematic: We run a user generated script and display some of its output. A malicious party might be able to extract the authentication token of our Google account or could upload malware to the drive.\nTo reduce the risk, I simply created an un-associated Google account to host the drive. There are certainly better options available, but this seemed a reasonable solution achieved with very little effort."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#register-team",
    "href": "posts/2023-06-27__hackathon_app/index.html#register-team",
    "title": "Hackathon Feedback Application",
    "section": "Register Team",
    "text": "Register Team\nWe wanted to allow users to sign up as teams using the shiny app. The app provides a simple interface where users could input a team name and the number of members. This in turn would open two fields for each user to input their name and email address.\nWe do simple checks to make sure at least one valid email address is supplied, and that the group name is acceptable. The group name cannot be empty, already taken, or contain vulgar words.\nThe team registration itself was adding the team information to the Google sheets file event_info into the sheet teams and to create a team folder in which to store the uploaded R files.\nThe checks and registration is implemented in the register_team() function stored in interact_with_google.R.\n\n\n\nScreenshot of the register team interface\n\n\nThe challenge here was to adapt the number of input fields depending on the number of team members. This means that the team name and email interface must be rendered: First, we check how many team members are part of the group, this is stored in the input$n_members input variable. Then we create a tagList with as many elements as team members. Each element contains two columns, one for the email, one for the member name. This tagList is then returned and displayed to the user.\n\n# render email input UI of the register tab\noutput$name_email &lt;- shiny::renderUI({\n  # create field names\n  N &lt;- input$n_members\n  NAME &lt;- sapply(1:N, function(i) {\n    paste0(\"name\", i)\n  })\n  EMAIL &lt;- sapply(1:N, function(i) {\n    paste0(\"email\", i)\n  })\n\n  output &lt;- tagList()\n\n\n  firstsecondthird &lt;- c(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\")\n  for (i in 1:N) {\n    output[[i]] &lt;- tagList()\n    output[[i]] &lt;- fluidRow(\n      shiny::h4(paste(firstsecondthird[i], \" Member\")),\n      column(6,\n        textInput(NAME[i], \"Name\"),\n        value = \" \" # displayed default value\n      ),\n      column(6,\n        textInput(EMAIL[i], \"Email\"),\n        value = \" \"\n      )\n    )\n  }\n  output\n})\n\nThe team information is then uploaded to Google drive. Because some teams have more members than others, we have to create the respective data-frame with the number of team members in mind.\nThe following chunk creates the registration data. Noteworthy here the creation of the NAME and EMAIL variables which depend on the number of members in this team. Further, the user input of these fields is extracted via input[[paste0(NAME[i])]] within a for-loop.\nWe also make the data-creation dependent on the press of the Register Group button and cache some variables.\n\n## registration\nregistrationData &lt;-\n  reactive({\n    N &lt;- input$n_members\n    NAME &lt;- sapply(1:N, function(i) {\n      paste0(\"name\", i)\n    })\n    EMAIL &lt;- sapply(1:N, function(i) {\n      paste0(\"email\", i)\n    })\n    names &lt;- character(0)\n    emails &lt;- character(0)\n\n    for (i in 1:N) {\n      names[i] &lt;- input[[paste0(NAME[i])]]\n      emails[i] &lt;- input[[paste0(EMAIL[i])]]\n    }\n    # create df\n    dplyr::tibble(\n      team_name = input$team_name,\n      n_members = N,\n      member_name = names,\n      member_email = emails\n    )\n  }) %&gt;%\n  bindCache(input$team_name, input$n_members, input$name1, input$email1) %&gt;%\n  bindEvent(input$register) # wait for button press"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#upload-source-script",
    "href": "posts/2023-06-27__hackathon_app/index.html#upload-source-script",
    "title": "Hackathon Feedback Application",
    "section": "Upload & Source Script",
    "text": "Upload & Source Script\nTo upload a script, participants had to select their team first. The input options were based on the existing folders on the Google-drive in the mock_hackathon folder. To upload a particular script participants had to also select the task to be solved. The uploaded script is then uploaded to the team folder following a standardised script naming convention.\nThere are different aspects to be aware of when sourcing scripts on a shiny server. For example, you have to anticipate the packages users will include in their uploaded scripts, as their scripts will load but not install packages. Further, you should keep the global environment of your shiny app separate from the environment in which the script is sourced. This is possible by supplying an environment to the source() function, e.g: source(path_to_script, local = new.env())\nAnother thing we had to consider was to replicate the exact folder-structure on the shiny server that participants were working with when creating the scripts, as they were required to source some scripts and to save their file into a specific folder. This was relatively straight forward as we provided participants with a folder structure in the posit cloud instance they were using. They had access to the sdtm folder in which the data was stored, and the adam folder into which they saved their solutions. The structure also included a folder with metadata which was also available on the shiny server.\nFor some tasks, participants required some ADaM-datasets stored in the adam folder, essentially the output from previous tasks. This was achieved by first creating a list mapping tasks to the required ADaM datasets:\n\ndepends_list &lt;- list(\n  \"ADADAS\" = c(\"ADSL\"),\n  \"ADAE\" = c(\"ADSL\"),\n  \"ADLBC\" = c(\"ADSL\"),\n  \"ADLBH\" = c(\"ADSL\"),\n  \"ADLBHY\" = c(\"ADSL\"),\n  \"ADSL\" = NULL,\n  \"ADTTE\" = c(\"ADSL\", \"ADAE\"),\n  \"ADVS\" = c(\"ADSL\")\n)\n\nThis list is sourced from the R/parameters.R file when initiating the application. We then call the get_depends() function sourced from R/get_depends.R which copies the required files from the key folder (where our solutions to the tasks were stored) to the adam folder. After sourcing the uploaded script the content in the adam folder is deleted."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#compare-to-solution-file",
    "href": "posts/2023-06-27__hackathon_app/index.html#compare-to-solution-file",
    "title": "Hackathon Feedback Application",
    "section": "Compare to Solution File",
    "text": "Compare to Solution File\nWe want to compare the file created by participants with our solution (key) file stored in the key folder. The diffdf::diffdf() function allows for easy comparison of two data-frames and directly provides extensive feedback for the user:\n\nlibrary(dplyr)\ndf1 &lt;- tibble(\n  numbers = 1:10,\n  letters = LETTERS[1:10]\n)\ndf2 &lt;- tibble(\n  numbers = 1:10,\n  letters = letters[1:10]\n)\n\ndiffdf::diffdf(df1, df2)\n\nWarning in diffdf::diffdf(df1, df2): \nNot all Values Compared Equal\n\n\nDifferences found between the objects!\n\nA summary is given below.\n\nNot all Values Compared Equal\nAll rows are shown in table below\n\n  =============================\n   Variable  No of Differences \n  -----------------------------\n   letters          10         \n  -----------------------------\n\n\nAll rows are shown in table below\n\n  ========================================\n   VARIABLE  ..ROWNUMBER..  BASE  COMPARE \n  ----------------------------------------\n   letters         1         A       a    \n   letters         2         B       b    \n   letters         3         C       c    \n   letters         4         D       d    \n   letters         5         E       e    \n   letters         6         F       f    \n   letters         7         G       g    \n   letters         8         H       h    \n   letters         9         I       i    \n   letters        10         J       j    \n  ----------------------------------------"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#score",
    "href": "posts/2023-06-27__hackathon_app/index.html#score",
    "title": "Hackathon Feedback Application",
    "section": "Score",
    "text": "Score\nTo compare submissions between participants we implemented a simple scoring function (score_f()) based on the table comparison by diffdf(). The function can be found in the compare_dfs.R file:\n\nscore_f &lt;- function(df_user, df_key, keys) {\n  score &lt;- 10\n  diff &lt;- diffdf::diffdf(df_user, df_key, keys = keys)\n  if (!diffdf::diffdf_has_issues(diff)) {\n    return(score)\n  }\n\n  # check if there are any differences if the comparison is not strict:\n  if (!diffdf::diffdf_has_issues(diffdf::diffdf(df_user,\n    df_key,\n    keys = keys,\n    strict_numeric = FALSE,\n    strict_factor = FALSE\n  ))) {\n    # if differences are not strict, return score - 1\n    return(score - 1)\n  }\n\n  return(round(min(max(score - length(diff) / 3, 1), 9), 2))\n}\n\nEvery comparison starts with a score of 10. We then subtract the length of the comparison object divided by a factor of 3. The length of the comparison object is a simplified way to represent the difference between the two data-frames by one value. Finally, the score is bounded by 1 using max(score, 1).\nThe score is not a perfect capture of the quality of the script uploaded but: 1. helped participants get an idea of how close their data-frame is to the solution file 2. allowed us to raffle prizes based on the merit of submitted r-scripts"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#reactiveness",
    "href": "posts/2023-06-27__hackathon_app/index.html#reactiveness",
    "title": "Hackathon Feedback Application",
    "section": "Reactiveness",
    "text": "Reactiveness\nSome of the app functions can take quite some time to execute, e.g. running the uploaded script. Other tasks, e.g. registering a team, do not intrinsically generate user facing outputs. This would make the app using really frustrating, as users would not know whether the app is correctly working or whether it froze.\nWe implemented two small features that made the app more responsive. One is simple loading icons that integrate into the user interface and show that output is being computed – that something is working. The other is a pop up window which communicates whether team registration was successful, and if not, why not.\nWe further aimed to forward errors generated by the uploaded scripts to the user interface, but errors generated by the application itself should be concealed."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#conclusion",
    "href": "posts/2023-06-27__hackathon_app/index.html#conclusion",
    "title": "Hackathon Feedback Application",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough the application was continuously improved during the hackathon it proved to be a useful resource for participants from day one as it allowed groups to set their own pace. It further allowed admiral developers to gain insights on package usage of a relatively large sample of potential end users. From our perspective, the application provided a great added value to the hackathon and eased the workload of guiding the participants through all the tasks."
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#last-updated",
    "href": "posts/2023-06-27__hackathon_app/index.html#last-updated",
    "title": "Hackathon Feedback Application",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:23.411611"
  },
  {
    "objectID": "posts/2023-06-27__hackathon_app/index.html#details",
    "href": "posts/2023-06-27__hackathon_app/index.html#details",
    "title": "Hackathon Feedback Application",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html",
    "href": "posts/2023-06-28_welcome/index.html",
    "title": "Hello pharmaverse",
    "section": "",
    "text": "The communications working group (CWG) seeks to promote and showcase how R can be used in the Clinical Reporting pipeline through short and informative blog posts. These posts will be hosted on this pharmaverse blog and promoted on the pharmaverse slack channels as well as on LinkedIn.\nAs the CWG is a small team, we hope to make the blog development process easy enough that pharmaverse community members will be able to easily write blog posts with guidance from the CWG team."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#purpose",
    "href": "posts/2023-06-28_welcome/index.html#purpose",
    "title": "Hello pharmaverse",
    "section": "",
    "text": "The communications working group (CWG) seeks to promote and showcase how R can be used in the Clinical Reporting pipeline through short and informative blog posts. These posts will be hosted on this pharmaverse blog and promoted on the pharmaverse slack channels as well as on LinkedIn.\nAs the CWG is a small team, we hope to make the blog development process easy enough that pharmaverse community members will be able to easily write blog posts with guidance from the CWG team."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#spirit-of-a-blog-post",
    "href": "posts/2023-06-28_welcome/index.html#spirit-of-a-blog-post",
    "title": "Hello pharmaverse",
    "section": "Spirit of a Blog Post",
    "text": "Spirit of a Blog Post\nThe CWG believes that the following 4 points will help guide the creation of Blog Posts.\n\nShort\nPersonalized\nReproducible\nReadable\n\nShort: Posts should aim to be under a 10 minute read. We encourage longer posts to be broken up into multiple posts.\nPersonalized: Posts should have a personality! For example, a person wishing to post on a function in a package needs to differentiate the post from the documentation for function, i.e. we don’t want to just recycle the documentation. How can you add your voice and experience? A bit of cheeky language is also encouraged.\nReproducible: Posts should work with minimal dependencies with data, packages and outside sources. Every dependency introduced in a post adds some risk to the post longevity. As package dependencies change, posts should be built in a way that they can be updated to stay relevant.\nReadable: The CWG sees this site as more of introductory site rather advanced user site. Therefore, the CWG feels that code should be introduced in a way that promotes readability over complexity."
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#what-types-of-posts-are-allowed-on-this-site",
    "href": "posts/2023-06-28_welcome/index.html#what-types-of-posts-are-allowed-on-this-site",
    "title": "Hello pharmaverse",
    "section": "What types of posts are allowed on this site?",
    "text": "What types of posts are allowed on this site?\nOverall, we want to stay focus on the Clinical Reporting Pipeline, which we see as the following topics:\n\nPackages in the Clinical Reporting Pipeline\nFunctions from packages in the Clinical Reporting Pipeline\nWider experiences of using R in the Clinical Reporting Pipeline\nConference experiences and the Clinical Reporting Pipeline\n\nHowever, it never hurts to ask if you topic might fit into this medium!\n\nMinimum Post Requirements\n\nA unique image to help showcase the post.\nWorking Code\nSelf-contained data or package data.\nDocumentation of package versions\n\nThat is it! After that you can go wild, but we do ask that it is kept short!"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#how-can-i-make-a-blog-post",
    "href": "posts/2023-06-28_welcome/index.html#how-can-i-make-a-blog-post",
    "title": "Hello pharmaverse",
    "section": "How can I make a Blog Post",
    "text": "How can I make a Blog Post\nStep 1: Reach out to us through pharmaverse/slack or make an issue on our GitHub.\nStep 2: Branch off main\nStep 3: Review the Spirit of the Blog Post in the Pull Request Template\nStep 4: Poke us to do a review!"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#last-updated",
    "href": "posts/2023-06-28_welcome/index.html#last-updated",
    "title": "Hello pharmaverse",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:27.049951"
  },
  {
    "objectID": "posts/2023-06-28_welcome/index.html#details",
    "href": "posts/2023-06-28_welcome/index.html#details",
    "title": "Hello pharmaverse",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html",
    "href": "posts/2023-08-08_study_day/study_day.html",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "",
    "text": "Creating --DY variables for your ADaMs is super easy using derive_vars_dy() from the admiral package.\nLet’s build some dummy data with 4 subjects, a start date/time for treatment (TRTSDTM), an analysis start date/time variable (ASTDTM) and an analysis end date variable (AENDT).\nlibrary(admiral)\nlibrary(lubridate)\nlibrary(dplyr)\n\nadam &lt;- tribble(\n  ~USUBJID, ~TRTSDTM, ~ASTDTM, ~AENDT,\n  \"001\", \"2014-01-17T23:59:59\", \"2014-01-18T13:09:O9\", \"2014-01-20\",\n  \"002\", \"2014-02-25T23:59:59\", \"2014-03-18T14:09:O9\", \"2014-03-24\",\n  \"003\", \"2014-02-12T23:59:59\", \"2014-02-18T11:03:O9\", \"2014-04-17\",\n  \"004\", \"2014-03-17T23:59:59\", \"2014-03-19T13:09:O9\", \"2014-05-04\"\n) %&gt;%\n  mutate(\n    TRTSDTM = as_datetime(TRTSDTM),\n    ASTDTM = as_datetime(ASTDTM),\n    AENDT = ymd(AENDT)\n  )\nOkay! Next we run our dataset through derive_vars_dy(), specifying:\nderive_vars_dy(\n  adam,\n  reference_date = TRTSDTM,\n  source_vars = exprs(ASTDTM, AENDT)\n)\n\n# A tibble: 4 × 6\n  USUBJID TRTSDTM             ASTDTM              AENDT      ASTDY AENDY\n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 001     2014-01-17 23:59:59 2014-01-18 13:09:09 2014-01-20     2     4\n2 002     2014-02-25 23:59:59 2014-03-18 14:09:09 2014-03-24    22    28\n3 003     2014-02-12 23:59:59 2014-02-18 11:03:09 2014-04-17     7    65\n4 004     2014-03-17 23:59:59 2014-03-19 13:09:09 2014-05-04     3    49\nThat’s it! We got both our ASTDY and AENDY variables in only a few short lines of code!\nWhat if I want my variables to have a different naming convention?\nEasy! In the source_vars argument if you want your variables to be called DEMOADY and DEMOEDY just do DEMOADY = ASTDTM and DEMOEDY = AENDT and derive_vars_dy() will do the rest!\nderive_vars_dy(\n  adam,\n  reference_date = TRTSDTM,\n  source_vars = exprs(DEMOADY = ASTDTM, DEMOEDY = AENDT)\n)\n\n# A tibble: 4 × 6\n  USUBJID TRTSDTM             ASTDTM              AENDT      DEMOADY DEMOEDY\n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 001     2014-01-17 23:59:59 2014-01-18 13:09:09 2014-01-20       2       4\n2 002     2014-02-25 23:59:59 2014-03-18 14:09:09 2014-03-24      22      28\n3 003     2014-02-12 23:59:59 2014-02-18 11:03:09 2014-04-17       7      65\n4 004     2014-03-17 23:59:59 2014-03-19 13:09:09 2014-05-04       3      49\nIf you want to get --DT or --DTM variables using admiral then check out derive_vars_dt() and derive_vars_dtm(). If things are messy in your data, e.g. partial dates, both functions have great imputation abilities, which we will cover in an upcoming blog post!"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html#last-updated",
    "href": "posts/2023-08-08_study_day/study_day.html#last-updated",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:31.609803"
  },
  {
    "objectID": "posts/2023-08-08_study_day/study_day.html#details",
    "href": "posts/2023-08-08_study_day/study_day.html#details",
    "title": "It’s all relative? - Calculating Relative Days using admiral",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html",
    "title": "xportr 0.4.0",
    "section": "",
    "text": "In the pharmaceuticals and healthcare industries, it is crucial to maintain a standard structure for data exchange and regulatory submissions, enter xpt datasets! xpt datasets are binary files that are typically created by SAS software, they contain structured data, including variables, labels, and metadata. In order to develop xpt formatted files in R, let’s introducing you to xportr."
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#last-updated",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#last-updated",
    "title": "xportr 0.4.0",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:35.251047"
  },
  {
    "objectID": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#details",
    "href": "posts/2024-03-29_xportr_0_4_0/xportr_0_4_0.html#details",
    "title": "xportr 0.4.0",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html",
    "title": "teal is now available on CRAN 🎉",
    "section": "",
    "text": "We’re thrilled to announce that teal v0.15.0 has been released on CRAN!\nThis marks a significant milestone in our journey, and we’re incredibly excited about the possibilities teal brings to the R community, particularly within clinical trial settings.\nOne of the most notable changes in this release is the introduction of teal_data class. This addition enhances how data is handled within the teal framework, paving the way for custom data modules tailored to the needs of our R users, both inside and outside the clinical trial space. With teal_data, users can expect improved efficiency and flexibility in managing their data, opening doors to innovative approaches in data analysis and visualization.\nWhile we’re enthusiastic about the advancements teal v0.15.0 brings, we have to introduce breaking changes to this version.\nBut worry not, we’ve got you covered!\nTo ease the transition, we’ve provided comprehensive guidance on migrating your applications from version 0.14.0 to 0.15.0. Check out our migration guide here, and feel free to ask any questions you may have in the discussion thread.\nAs we roll out teal v0.15.0, we’re also working diligently on releasing teal modules packages to CRAN to fully support this version. While we’re still in the process, we encourage you to dive into the latest teal release and start exploring its capabilities. To get started, make sure to install the development versions of teal.transform, teal.reporter, and any other modules you’re using.\nTo simplify the process, you can execute the following code to verify that you have the correct teal and teal modules versions:\nRest assured, we’re committed to completing the release of the teal modules as swiftly as possible to provide users with an uninterrupted experience.\nAs always, thank you for your continued support and enthusiasm for teal. We can’t wait to see the incredible ways in which teal empowers you to revolutionize your data exploration in R.\nFor further details about the release, please refer to this link.\nFeel free to explore the teal website here to learn more about the latest features."
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#last-updated",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#last-updated",
    "title": "teal is now available on CRAN 🎉",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:38.745844"
  },
  {
    "objectID": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#details",
    "href": "posts/2024-02-13_teal_on_cran/teal_on_cran.html#details",
    "title": "teal is now available on CRAN 🎉",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html",
    "href": "posts/2023-10-30_floating_point/floating_point.html",
    "title": "Floating point",
    "section": "",
    "text": "{admiral} recently ran into some trouble when dealing with floating point values, captured by this thread on GitHub. This post gives a brief overview on floating point values, recaps the discussion on GitHub, and explains how {admiral} deals with floating point values."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#floating-point-values",
    "href": "posts/2023-10-30_floating_point/floating_point.html#floating-point-values",
    "title": "Floating point",
    "section": "Floating point values",
    "text": "Floating point values\nFloating point values are numeric objects representing numbers between integers, e.g. 0.5, 2.3, 3.1415, etc. However, floating point numbers are not stored like integers, and most floating point numbers are approximations to the number they represent. To see what value a floating point number is actually stored as, we can use the format() function where we can increase the number of digits shown:\n\nformat(1.4, digits = 22)\n\n[1] \"1.399999999999999911182\"\n\n\nThese very small numerical differences impact the result of mathematical operations:\n\n0.1 + 0.2 == 0.3\n\n[1] FALSE\n\n\nIf we look at the actually stored values, this makes sense:\n\n0.1 %&gt;% format(digits = 22)\n\n[1] \"0.1000000000000000055511\"\n\n0.2 %&gt;% format(digits = 22)\n\n[1] \"0.2000000000000000111022\"\n\n(0.1 + 0.2) %&gt;% format(digits = 22)\n\n[1] \"0.3000000000000000444089\"\n\n0.3 %&gt;% format(digits = 22)\n\n[1] \"0.2999999999999999888978\"\n\n\nThe bottom line is: Avoid using exact comparators such as == and &gt;= when comparing floating point values.\n\n\n\n\n\n\nExact floating point values\n\n\n\nFloating point values are stored in binary format. While most floating point values are approximations, there are some exceptions which can be exactly represented, namely if they can be written down as \\(\\frac{x}{2^y}\\), where x and y are integers. For example, 0.5 is stored as \\(\\frac{1}{2}\\), 0.25 is stored as \\(\\frac{1}{4}\\), 0.125 is stored as \\(\\frac{1}{8}\\), etc.\n\n# simple examples\n0.5 %&gt;% format(digits = 22)\n\n[1] \"0.5\"\n\n0.25 %&gt;% format(digits = 22)\n\n[1] \"0.25\"\n\n0.125 %&gt;% format(digits = 22)\n\n[1] \"0.125\"\n\n0.0625 %&gt;% format(digits = 22)\n\n[1] \"0.0625\"\n\n# some weird values for x and y\n(1121 / (2^9)) %&gt;% format(digits = 22)\n\n[1] \"2.189453125\"\n\n\nAll floating point values are stored as \\(\\frac{x}{2^y}\\), where the outcome may be a very close approximation to the value they represent*.\nhttps://en.wikipedia.org/wiki/Floating-point_arithmetic#Representable_numbers,_conversion_and_rounding If you would like to learn more about representable floating point values please read the wikipedia article on floating point values, especially section Representable numbers, conversion and rounding.\n* Based on a recollection of the course associated with this GitHub Repository by Martin Mächler."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#issues-arising",
    "href": "posts/2023-10-30_floating_point/floating_point.html#issues-arising",
    "title": "Floating point",
    "section": "Issues arising",
    "text": "Issues arising\nGordon Miller came across this issue when he was creating DAIDS criteria for adverse events in cancer therapy when using case_when statements to implement the grade.\nWe can have a glimpse here:\n\natoxgr_criteria_daids %&gt;%\n  filter(TERM %in% c(\"Amylase, High\", \"Lipase, High\")) %&gt;%\n  select(TERM, GRADE_CRITERIA_CODE) %&gt;%\n  reactable(defaultPageSize = 4, highlight = TRUE, bordered = TRUE, striped = TRUE, resizable = TRUE)\n\n\n\n\n\nAs you can see, the data-frame contains the column GRADE_CRITERIA_CODE which contains comparisons of floating point values. And there was a discrepancy of what Gordon expected to see, and how R actually computed the comparison initially:\n\nThe test is AVAL &gt;= 1.1*ANRHI should give a value of “1” where AVAL = 110 and ANRHI = 100.\nI tried it separately and I also got 1.1*ANRHI not equal to 110 where ANRHI = 100.\n\nWhere ANRHI is the analysis range upper limit and AVAL is an analysis value.\nWhat happened here? Gordon Miller wanted to compute the analysis range upper limit plus 10% and compare it to the analysis value. He expected the comparison to yield TRUE (or 1 if converted to numeric) as AVAL (110) should be exactly 1.1 * 100. However, he multiplied an integer (100) with a floating point value (1.1). And the result was not exactly 110, as 1.1 is not exactly represented as a floating point value.\n\n(1.1 * 100) %&gt;% format(digits = 22)\n\n[1] \"110.0000000000000142109\"\n\n1.1 * 100 == 110\n\n[1] FALSE\n\n\nOn my machine, the result is actually larger than 110, while on Gordon Miller’s machine the result was smaller than 110. In {admiral}, we strive towards removing platform specific and unexpected behavior, so we had to find a way to solve the floating point issue."
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#potential-solutions",
    "href": "posts/2023-10-30_floating_point/floating_point.html#potential-solutions",
    "title": "Floating point",
    "section": "Potential solutions",
    "text": "Potential solutions\nA very crude option would be to round the result of the multiplication to the nearest integer.\n\nround(1.1 * 100) %&gt;% format(digits = 22)\n\n[1] \"110\"\n\n\nHowever, this does not work when the result is not an integer, i.e. the upper limit was 101 instead. We should then compare the analysis value to 101 * 1.1, which should be exactly 111.1. We could try to round to the nearest decimal place, but that value would again be stored as a floating point value:\n\n(101 * 1.1) %&gt;%\n  round(digits = 1) %&gt;%\n  format(digits = 22)\n\n[1] \"111.0999999999999943157\"\n\n\nA workaround would be to multiply both sides of the equation with 10, and then round to the next integer:\n\n(101 * 1.1 * 10) %&gt;%\n  round() %&gt;%\n  format(digits = 22)\n\n[1] \"1111\"\n\n(111.1 * 10) %&gt;%\n  round() %&gt;%\n  format(digits = 22)\n\n[1] \"1111\"\n\n\nThis is very awkward, as you don’t know by how much you need to multiply each time, a very clunky solution.\nAlternatively, we can compare the absolute value of the difference between the analysis value and the upper limit plus 10% to a very small number, e.g. 0.0000001:\n\nAVAL &lt;- 111.1\nANRHI &lt;- 101\n\nabs(AVAL - ANRHI * 1.1) &lt; 0.0000001\n\n[1] TRUE\n\n\nComparing to a very small value is also how the all.equal() function works, which compares two numeric values and returns TRUE if they are equal within a tolerance. By default the tolerance is around \\(1.5 * 10^{-8}\\) but you can set it yourself to a lower value, e.g. machine tolerance .Machine$double.eps - (one of**) the smallest positive floating-point number x such that 1 + x != 1.\n\n1 + .Machine$double.eps == 1\n\n[1] FALSE\n\n# but:\n1 + .Machine$double.eps / 2 == 1\n\n[1] TRUE\n\n# so we can use:\nall.equal(AVAL, ANRHI * 1, 1, tolerance = .Machine$double.eps)\n\n[1] \"Mean absolute difference: 10.1\"\n\n\nThis would still be a little clunky for greater than or equal to comparisons:\n\nall.equal(AVAL, ANRHI * 1.1) | AVAL &gt; ANRHI * 1.1\n\n[1] TRUE\n\n# unfortunately, the all.equal() function does not return a FALSE if they are not the same:\nall.equal(AVAL, ANRHI * 1.1 + 1)\n\n[1] \"Mean relative difference: 0.0090009\"\n\n\nFor some reason, the value it returns is also not correct.\nThere is also a dplyr function called near() which does essentially the same thing as all.equal():\n\nANRHI &lt;- 100\nAVAL &lt;- 110\n(ANRHI * 1.1) %&gt;% format(digits = 22)\n\n[1] \"110.0000000000000142109\"\n\nAVAL &gt; ANRHI * 1.1 | near(AVAL, ANRHI * 1.1)\n\n[1] TRUE\n\n\nGordon Miller suggested to replace the standard comparators with the following functions across {admiral}\n\n\n\n{base}\nimproved\n\n\n\n\nA &gt;= B\nA &gt; B | near(A, B)\n\n\nA &lt;= B\nA &lt; B | near(A, B)\n\n\nA == B\nnear(A, B)\n\n\nA != B\n!near(A, B)\n\n\nA &gt; B\nA &gt; B & !near(A, B)\n\n\nA &lt; B\nA &lt; B & !near(A, B)\n\n\n\nThis would work perfectly fine, but especially for case_when() statements, it would add a lot of code-bloat.\nAlthough a minor issue, it looks like the near() function tests for absolute differences, while the all.equal() function tests for relative differences, as discussed in this thread:\n\n# Very large values:\n# When checking for absolute differences\nnear(\n  ANRHI * 1.1 * 10^6,\n  AVAL * 10^6\n)\n\n[1] FALSE\n\n# When checking for relative differences\nall.equal(\n  ANRHI * 1.1 * 10^6,\n  AVAL * 10^6\n)\n\n[1] TRUE\n\n# As:\n(ANRHI * 1.1 * 10^6) %&gt;% format(digits = 22)\n\n[1] \"110000000.0000000149012\"\n\n(AVAL * 10^6) %&gt;% format(digits = 22)\n\n[1] \"1.1e+08\"\n\n\n\n\n\n{base}\n{fpCompare}\n\n\n\n\nA &gt;= B\nA %&gt;=% B\n\n\nA &lt;= B\nA %&lt;=% B\n\n\nA == B\nA %==% B\n\n\nA != B\nA %!=% B\n\n\nA &gt; B\nA %&gt;&gt;% B\n\n\nA &lt; B\nA %&lt;&lt;% B\n\n\n\nAs an example to how this is implemented, we can have a look at the {fpCompare} source code for one of the operators:\n\n`%&lt;=%` &lt;- function(x, y) {\n  (x &lt; y + getOption(\"fpCompare.tolerance\"))\n}\n\nEven if y is ever so slightly smaller than x, adding the tolerance to y will make the result larger than x, and the comparison will return TRUE.\n\n# we need to set the fpCompare.tolerance first, because we did not load the package:\noptions(fpCompare.tolerance = 1e-8)\n\n(ANRHI * 1.1) %&lt;=% AVAL\n\n[1] TRUE\n\n\nAs long as {admiral} remains open source and free to use, using this package, or even reusing the code itself would be fine. Although this was my preferred option, we did not end up implementing it. Instead, we made use of the signif() function, which rounds a number to a specified number of significant digits. This way, we could use the regular infix operators and simply provide the number of significant digits we want to compare to:\n\nsignif_dig &lt;- 15\n\nsignif(AVAL, signif_dig) == signif(ANRHI * 1.1, signif_dig)\n\n[1] TRUE\n\n# as:\n(ANRHI * 1.1) %&gt;%\n  signif(signif_dig) %&gt;%\n  format(digits = 22)\n\n[1] \"110\"\n\n# and although when printed, the number still looks off:\nANRHI &lt;- 101\n((ANRHI * 1.1) %&gt;% signif(signif_dig)) %&gt;% format(digits = 22)\n\n[1] \"111.0999999999999943157\"\n\n# the comparison works now:\n((ANRHI * 1.1) %&gt;% signif(signif_dig)) == 111.1\n\n[1] TRUE\n\n\nThis is now implemented throughout atoxgr_criteria_daids, atoxgr_criteria_ctcv4, and atoxgr_criteria_ctcv5, and we are working on an issue for the 1.0.0 release of {admiral} to implement this for derive_var_anrind as well.\n\natoxgr_criteria_daids %&gt;%\n  select(TERM, GRADE_CRITERIA_CODE) %&gt;%\n  reactable(defaultPageSize = 4, highlight = TRUE, bordered = TRUE, striped = TRUE, resizable = TRUE)"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#conclusion",
    "href": "posts/2023-10-30_floating_point/floating_point.html#conclusion",
    "title": "Floating point",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe recent challenges faced by {admiral} in dealing with floating point values shed light on the complexities and nuances of working with these numerical representations. Floating point values, as we’ve seen, are approximations of real numbers and can lead to unexpected issues in mathematical operations, especially when using exact comparators like == and &gt;=. The differences between how these values are stored and computed can result in platform-specific discrepancies and unexpected behavior.\nSeveral potential solutions were explored to address this issue, including rounding, using near() or all.equal() functions, or implementing custom infix operators as seen in the fpCompare package. However, the most elegant and practical solution adopted in {admiral} was to use the signif() function to round values to a specified number of significant digits. This approach allows for reliable and consistent comparisons without adding unnecessary complexity to the code base.\nReaders and developers should be vigilant when working with floating point values in their own code or when utilizing {admiral} for their projects. Keep in mind that some floating point values can look like integers at first glance as in the above example of 1.1*100. The experience with floating point issues in {admiral} serves as a valuable reminder of the potential pitfalls associated with numerical precision in programming. It’s crucial to exercise caution when performing comparisons with floating point numbers as small discrepancies can have significant downstream implications. When writing your own comparisons consider the following best practices:\n\nAvoid Exact Comparisons: As highlighted earlier, using exact comparators like == or &gt;= when dealing with floating point values can lead to unexpected results. Instead, opt for methods that take into account a tolerance or margin of error, such as the near() function or the signif() approach discussed in this context.\nPlatform Independence: Be aware that floating point representations may differ across various platforms or environments. Always test your code on multiple platforms to ensure consistency in results.\nDocumentation and Comments: When writing code that potentially involves floating point comparisons, it’s advisable to include clear documentation and comments that explain the reasoning behind your approach. This will help others understand and maintain the code effectively.\nTesting and Validation: Implement thorough testing and validation procedures to verify the correctness of your code, particularly when it relies on floating point comparisons. This should include specific tests that would flag floating point issues on any machine or platform.\n\nBy heeding these precautions and understanding the intricacies of floating point representations, you can mitigate the risk of encountering unexpected behavior in your code. Whether you’re working with {admiral} or any other software, a cautious and informed approach to handling floating point values is essential for maintaining code accuracy and reliability.\n** This is a number of the smallest magnitude for which a difference is still detected. I.e. .Machine$double.eps / 1.8 is still detectable, while .Machine$double.eps / 2 is not detectable any longer (at least on my machine):\n\n# eps / 1.8 is still detectable:\n.Machine$double.eps / 1.8 + 1 == 1\n\n[1] FALSE\n\n.Machine$double.eps / 2 + 1 == 1\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#last-updated",
    "href": "posts/2023-10-30_floating_point/floating_point.html#last-updated",
    "title": "Floating point",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:42.423196"
  },
  {
    "objectID": "posts/2023-10-30_floating_point/floating_point.html#details",
    "href": "posts/2023-10-30_floating_point/floating_point.html#details",
    "title": "Floating point",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "",
    "text": "There is significant momentum in driving the adoption of R packages in the life sciences industries, in particular, the R Consortium Submissions Working Group is dedicated to promoting the use of R for regulatory submissions to the FDA.\nThe R Consortium Submissions Working Group successfully completed an R-based submission in November 2021 through the eCTD portal (R Submissions Pilot 1). This Pilot was completed on March 10, 2022 after a successful statistical review and evaluation by the FDA staff.\nMoving forward, the Pilot 2 aimed to include a Shiny application that the FDA staff could deploy on their own servers. The R Consortium recently announced that on September 27, 2023, the R Submissions Working Group successfully completed the follow-up to the Pilot 2 R Shiny-based submission and received a response letter from FDA CDER. This marks the first publicly available submission package that includes a Shiny component. The full FDA response letter can be found here.\nThe Shiny application that was sent for the Pilot 2 had the goal to display the 4 Tables, Listings and Figures (TLFs) that were sent for the Pilot 1 with basic filtering functionality.\nThe submission package adhered to the eCTD folder structure and contained 5 deliverables. Among the deliverables was the proprietary R package {pilot2wrappers}, which enables the execution of the Shiny application.\nThe FDA staff were expected to receive the electronic submission packages in the eCTD format, install and load open-source R packages used as dependencies in the included Shiny application, reconstruct and load the submitted Shiny application, and communicate potential improvements in writing.\nIn the following stage, the R Consortium’s R Submission Working Group launched Pilot 4, aiming to investigate innovative technologies like Linux containers and web assembly. These technologies are being explored to package a Shiny application into a self-contained unit, streamlining the transfer and execution processes for the application.\nIn this post, our aim is to outline how we used the Rhino framework to reproduce the Shiny application that was successfully submitted to the FDA for the Pilot 2 project. Additionally, we detail the challenges identified during the process and how we were able to successfully address them by using an open-source package."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#reproducing-the-r-submission-pilot-2-shiny-app-using-rhino",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#reproducing-the-r-submission-pilot-2-shiny-app-using-rhino",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Reproducing the R Submission Pilot 2 Shiny App using Rhino",
    "text": "Reproducing the R Submission Pilot 2 Shiny App using Rhino\nWhile the original Shiny application submitted to the FDA was wrapped using {Golem}, we replicated the application using our in-house developed framework Rhino. The main motivation was to provide an example of an R Submission that is not an R package and to identify and solve any issues that may arise from this approach.\nOur demo application (FDA-pilot-app) is accessible on our website, alongside other Shiny and Rhinoverse demonstration apps.\n\nThe code for FDA-pilot-app is open-source. You can create your own Rhino-based application by following our tutorial and viewing our workshop, which is available on YouTube."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#brief-introduction-to-rhino",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#brief-introduction-to-rhino",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Brief Introduction to Rhino",
    "text": "Brief Introduction to Rhino\n\nThe Rhino framework was developed by Appsilon to create enterprise-level Shiny applications, consistently and efficiently. This framework allows developers to apply the best software engineering practices, modularize code, test it thoroughly, enhance UI aesthetics, and monitor user adoption effectively.\nRhino provides support in 3 main areas:\n\nClear Code: scalable architecture, modularization based on Box and Shiny modules.\nQuality: comprehensive testing such as unit tests, E2E tests with Cypress and Shinytest2, logging, monitoring and linting.\nAutomation: project startup, CI with GitHub Actions, dependencies management with {renv}, configuration management with config, Sass and JavaScript bundling with ES6 support via Node.js.\n\nRhino is an ideal fit for highly regulated environments such as regulatory submissions or other drug development processes.\n\nFDA-pilot-app structure\nThe structure of this application is available on the github repository. The structure of this Shiny app is the following.\n\nClick here to expand the FDA-pilot-app structure\n\n.\n├── app\n│   ├── view\n│   │   └── demographic_table.R\n|   |   └── km_plot.R\n|   |   └── primary_table.R\n|   |   └── efficacy_table.R\n|   |   └── completion_table.R\n│   ├── logic\n│   │   └── adam_data.R\n│   │   └── eff_modles.R\n│   │   └── formatters.R\n│   │   └── helpers.R\n│   │   └── kmplot_helpers.R\n│   │   └── Tplyr_helpers.R\n│   ├── data\n│   │   └── adam\n│   │       └── adadas.xpt\n│   │       └── adlbc.xpt\n│   │       └── adsl.xpt\n│   │       └── adtte.xpt\n│   ├── docs\n│   │   └── about.md\n│   ├── js\n│   │   └── index.js\n│   ├── static\n│   │   └── favicon.ico\n│   ├── styles\n│   │   └── main.scss\n│   └── app.R\n├── tests\n│   ├── cypress\n│   │   └── integration\n│   │       └── app.spec.js\n│   ├── testthat\n│   │\n│   └── cypress.json\n├── app.R\n├── rhino_submission.Rproj\n├── dependencies.R\n├── renv.lock\n├── rhino.yml\n└── README.md"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#efficient-submissions-to-the-fda",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#efficient-submissions-to-the-fda",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Efficient Submissions to the FDA",
    "text": "Efficient Submissions to the FDA\n\n\n\n\n\nTo comply with the Electronic Submission File Formats and Specifications for the eCTD submission, the programming code should carry a “.txt” extension. In the R Submissions Pilot 3 the group did not use {pkglite} as the FDA clarified that “.zip” and “.r” files are acceptable for submission. In this case, we utilized the {pkglite} R package to efficiently pack and unpack the FDA-pilot-app. This approach would facilitate the FDA reviewers in setting up the submission on their systems.\nThis package allows packing R packages to “.txt” files, which are supported for the submission of proprietary packages to the FDA via the eCTD gateway. \n\nPacking the FDA-pilot-app into a .txt file\nThe code below can be used to pack the Shiny application into a .txt file:\n\napp_name &lt;- \"rhinosubmission\"\nrenv_spec &lt;- pkglite::file_spec(\n  \"renv\",\n  pattern = \"^settings\\\\.dcf$|^activate\\\\.R$\",\n  format = \"text\", recursive = FALSE\n)\ntests_spec &lt;- pkglite::file_tests()\napp_spec &lt;- pkglite::file_auto(\"app\")\nroot_spec &lt;- pkglite::file_spec(\n  path = \".\",\n  pattern = \"^\\\\.Rprofile$|^rhino\\\\.yml$|^renv\\\\.lock$|^dependencies\\\\.R$|^config\\\\.yml$|^app\\\\.R$|^README\\\\.md$|\\\\.Rproj$\",\n  all_files = TRUE,\n  recursive = FALSE\n)\nwrite(paste0(\"Package: \", app_name), \"DESCRIPTION\", append = TRUE)\npkglite::collate(\n  getwd(),\n  renv_spec,\n  tests_spec,\n  app_spec,\n  root_spec\n) |&gt; pkglite::pack()\nfile.remove(\"DESCRIPTION\")\n\n\n\nUnpacking the FDA-pilot-app\nThe packed “.txt” file can be unpacked into a Shiny app by using {pkglite} as follows:\n\npkglite::unpack(\"rhinosubmission.txt\")"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#lessons-learned",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#lessons-learned",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nOur initial objective was to prove that it would be possible to submit a Shiny application using Rhino through the eCTD gateway. During the rewriting process we identified that this could be done by integrating the open-source {pkglite} package. By following this approach, we concluded that it would be possible to submit a Shiny application through the eCTD gateway. This was also achieved through the successful submission of a package that included a Shiny component in Pilot 2.\nHaving rewritten the R Submissions Pilot 2 Shiny application using Rhino holds major implications for the adoption of our framework within the life sciences. Apart from being a strong, opinionated framework that improves reproducibility and reliability for Shiny development, using Rhino for regulatory submissions could improve the flexibility and speed in the clinical reporting pipeline. This would accelerate the adoption of R/Shiny for submissions to the FDA or other regulatory agencies."
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#last-updated",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#last-updated",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:46.218996"
  },
  {
    "objectID": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#details",
    "href": "posts/2023-08-14_rhino_submission_2/rhino_submission_2.html#details",
    "title": "Reproducing the R Submissions Pilot 2 Shiny Application using Rhino",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html",
    "href": "posts/2023-06-27_hackathon_writeup/index.html",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "",
    "text": "This January and February (2023), the admiral development team and the CDISC Open Source Alliance jointly hosted the admiral hackathon. The idea was to build a community of admiral users, and help participants familiarize themselves with R and admiral. This whole effort was led by Thomas Neitmann and was supported by Zelos Zhu, Sadchla Mascary, and me – Stefan Thoma.\nThe hackathon event was structured in two parts. First, we offered an Introduction to R for SAS programmers, a three hour workshop for R beginners to get them up to speed. Here we covered practical R basics, talking about how the R-workflow differs from a SAS workflow, and discussed common R functions - mostly from the tidyverse. This ensured that hackathon participants were familiar with core R concepts. The workshop recording and the course materials are available online.\nThe main hackathon consisted of several ADAM data generating tasks based on a specs file and synthetic data. Participants were able to solve these tasks in groups at their own pace thanks to a online tool where participants could upload their task specific R scripts and they would get automatic feedback for the data-set produced by their script. Script upload through the feedback application was available all through February, and we offered three additional online meetings throughout the month to discuss challenges and give some tips. If you are interested in learning more about the thoughts that went into the feedback application, you can read about it in this blogpost or check out my public GitHub repository for such an application."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#introduction-to-r-workshop",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#introduction-to-r-workshop",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Introduction to R workshop",
    "text": "Introduction to R workshop\nWe were really excited to see over 500 people from around 40 countries joining our Introduction to R workshop in January! To get to know prospective users and hackathon participants better, we conducted some polls during the meetings. Below you can see that representatives of many different sorts of organisations joined our Introduction to R workshop:\n\n\n\n\n\n\n\n\n\n216 out of 402 confirmed that their company is already using R for clinical trial data analysis, the remaining 131 did not answer this question.\nThe target audience for this workshop was programmers who are very familiar with SAS, but not so familiar with R, our polls confirmed this.\n\n\n\n\n\n\n\n\n\nOverall, we were very happy with how the workshop turned out, and participants overall agreed with this sentiment (although there may be a slight survivorship bias…)."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#admiral-hackathon",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#admiral-hackathon",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "admiral Hackathon",
    "text": "admiral Hackathon\nFollowing the kick-off meeting, 371 participants joined the posit (rStudio) workspace that was made available to all participants at no costs by the posit company. About half the participants planned to spend one to two hours per week on the admiral tasks, the other half planned to allocate even more. 15 participants even planned to spend eight hours or more!\nWe were really happy to see an overwhelming amount of activity on the slack channel we set up with over 250 members. Not only were people engaging with the materials, but we saw how a community was formed where people were encouraged to ask questions and where community members went out of their way to help each other. Shout-out to our community hero: Jagadish Katam without whom most issues related to the task programming raised by the community would not have been addressed as quickly as they were. Huge thanks from the organizers!\nIn the end, a total of 44 teams spanning 87 statistical programmers took part in the admiral hackathon and uploaded solution scripts to the hackathon application solving at least one of the 8 tasks available (ADSL, ADAE, ADLBC, ADVS, ADTTE, ADADAS, ADLBH & ADLBHY). Participants’ scripts were then run on the shiny server and the output data-frame were compared to the solutions we provided. At the read-out there was a live draft of teams to win one-on-one admiral consulting with one of the admiral core developers. Winning probabilities were weighted by the number of points each group received for the quality of their output data-frames and for the number of tasks solved.\nCongratulations to the winners:\n\nViiV Team_GSK\nteamspoRt\nTatianaPXL\nDivyasneelam\nAdaMTeamIndia\nSanofi_BP\nJagadish (our community hero)\nAZ_WAWA\n\nAlthough this was uncertain during the hackathon we were excited to provide a Certificate of Completion to all participants who uploaded a script to the Web Application.\nA recording of the hackathon readout can be found in the CDISC Open Source Alliance Quarterly Spotlight."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#conclusion",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#conclusion",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, we are very happy with how the hackathon turned out. We were not only positively surprised with the huge audience for the Intro to R workshop (CDISC record breaking) and for the admiral hackathon, but even more so with the engagement of all the participants.\nAgain, we would like to thank all the organizers, participants, and sponsors for their time and resources and hope to have provided a useful glimpse into our solution for ADAM creation within the end-to-end clinical data analysis open source R framework that the pharmaverse aims to provide.\nAs always, we are very happy to hear more feedback on the hackathon as well as on admiral in general. Simply submit an issue on the admiral GitHub repository. You would like to join the admiral core developers? Please reach out to Edoardo Mancini (product owner) or Ben Straub (technical lead)."
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#last-updated",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#last-updated",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:52.03688"
  },
  {
    "objectID": "posts/2023-06-27_hackathon_writeup/index.html#details",
    "href": "posts/2023-06-27_hackathon_writeup/index.html#details",
    "title": "Admiral Hackathon 2023 Revisited",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html",
    "title": "Cross-Industry Open Source Package Development",
    "section": "",
    "text": "This post is based on a talk given at the regional useR! conference on July 21st 2023 in Basel. I took the opportunity to present my personal perspective on the current cross-industry package development efforts with a particular focus on the transformation of the job description of statistical programmers. As I have only recently started my position at Roche, my personal perspective is the perspective of a newcomer. I have a background in Psychology and Statistics and joined Roche in November 2022 as an intern switching to a permanent position as a statistical programmer – what they call analytical data scientist now – in April 2023. I spend about 20% of my time in such a cross-industry package development project, which was a major reason for applying for this position. In this post I would like to explain how we work in this project, and why this had such an impact on my decision to join Roche."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#context",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#context",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Context",
    "text": "Context\nMy decision was influenced by two current industry trends:\nFirst, the switch to a more language agnostic and open source approach for clinical reporting and data analysis. At the moment, R seems to be the best fitting tool for the job, but the systems used here are language agnostic in general. This is related, but does not necessarily lead to the second trend: The move toward cross-industry collaboration when developing clinical reporting software.\nAs the industry moves toward new (to them) programming languages, fit-for-purpose tools need(ed) to be developed. The realization that siloed solutions – in an area where competitiveness does not benefit patients – are simply resource hungry ways to solve the same problem in parallel provided a great argument for a shift toward collaborations.\nSuch cross-industry collaborations gave rise to the pharmaverse, a curated collection of R packages designed to solve clinical reporting in R. {admiral}, the project that I work on, is part of the pharmaverse and covers the creation of ADaM data sets (CDISC standard data). These data sets are subsequently used to produce tables, listings, and graphs and are usually part of the submission package for regulators."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#insights",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#insights",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Insights",
    "text": "Insights\nWhen creating an open source package in an industry where currently there is a lot of traction you have to move fast. No, I don’t mean: Move fast and break things. I mean: Communicate! Get people on board! We aimed {admiral} to be the package for ADaM creation even before deciding to create the package together with GSK. By being transparent about our endeavor, e.g. Thomas Neitmann (then at Roche) posting on LinkedIn, we managed to connect with Michael Rimler from GSK and soon realized that we were dealing with the exact same challenge at both companies, and that a collaborative effort would improve the final product while reducing individual efforts. A working prototype of {admiral} was to be created by GSK and Roche within six months, and would then be open sourced. In our effort to communicate openly, we informed statistical programmers from over 20 companies about the {admiral} project and invited them to try it out and provide feedback once released. In the end, we received over 500 comments from over 50 programmers.\n\n# check out the latest admiral release from CRAN:\ninstall.packages(\"admiral\")\n\nlibrary(admiral)\n\nThis was instrumental in creating a product that was optimised for general usage in the clinical reporting field and ensured that other companies would not unknowingly invest into their own solution to this challenge. Open sourcing early is particularly beneficial because this ensures from the get-go that code created is aimed at a general audience, and not company specific (perhaps by accident).\n{admiral} was created with the long term goal of having a stable and flexible solution for the clinical reporting pipeline. In that spirit, its permissive apache 2.0 licence (jointly owned by Roche and GSK) further strengthens trust into the project, namely for three reasons:\n\nJointly owned means that efforts to monetize the code-base by one company could be vetoed by the other. The permissive licence ensures that in such an unlikely case, the code-base that has been published would stay available and could always be used and improved upon by others.\nHaving this package backed by Roche and GSK ensures (as far as this is ever possible) funding for properly maintaining the packages. This is crucial, as a package is rarely finished.\nUp to now, experts from many more companies have joined {admiral} or one of its therapeutic area specific package-extensions, inspiring even more trust into its reliability.\n\nIf you would like to learn more about licenses for open source projects in the clinical reporting world please check out the recent PHUSE E2E Guidance on open source license"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#development-workflow",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#development-workflow",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Development workflow",
    "text": "Development workflow\nEvery improvement, task, or feature we want to implement on {admiral} starts as an issue on our GitHub repository. It is the centerpiece of our development workflow, along with our developer guides which describe in detail the strategies, conventions, and workflows used in development. The guides help us keep the {admiral} package internally consistent (e.g. naming conventions, function logic) but also ensure that {admiral} adjacent packages follow the same conventions and share the user interface. This is further helped by the implemented CICD pipeline which ensures styling convention and spelling (and much more).\nThe core package developers team meets once a week (twice a week before a release) to discuss progress and priorities. Here, the role of product lead (currently Edoardo Mancini at Roche) and technical lead (currently Ben Straub at GSK) is to set priorities and track the release schedule. These stand-up meetings are centered around the project-board which gives a complete overview of activities and progress. Issues are mostly self-assigned so developers can really chose what they want to work on.\n\n\n\nGitHub project board\n\n\nBy design, {admiral} is community built. Most developers working on the project are statistical programmers working on clinical reporting themselves. As an open source project, community input is highly valued, and anyone using {admiral} is encouraged to submit issues or take on issues as part of the development team. We also do occasional events to bring the statistical programmers community and the developers closer together. Just last February we organised the {admiral} hackathon which had up to 500 participants."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#impact",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#impact",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Impact",
    "text": "Impact\nFor Roche, cross-industry package development work out in their favor: They get access to software created by specialists and users from across the industry but paying only a fraction of the developmental costs. Of course, they don’t have total developmental control but they do get a seat at the table. Any gaps between the open source {admiral} package and the proprietary Roche workflow were bridged by the internal {admiralroche} package.\nThe switch towards a more language agnostic platform, and open source languages specifically, opens the door to a broad population of university graduates with diverse backgrounds. I personally would not have considered this position five years ago due to a misalignment of skills and job requirements. Working towards an industry standard open source solution will also ensure that skills learned at one company are more easily transferable to external positions, further making the position much more attractive. Access to such a broad pool of potential candidates is clearly beneficial for recruitment at Roche, but also facilitates diversity in teams which makes for a more interesting and effective work place.\nOpen source development comes with much more transparency by definition. Recognition of contributions are built in - anyone can see who did what. This recognition escapes the confines of your company as it is visible to anyone looking at the repository. Anyone can not only see at any time what is being worked on, what discussions are happening and which direction is being taken, but can also participate and contribute. Transparency also applies to errors in the code and how the team is dealing with them. In such an environment it is practically impossible to hide or cover up errors and corrections. Instead, they have to be dealt with publicly and in the open. This openness about errors also helps seeing errors as a natural occurrence that needs to be dealt with. Space for errors encourages learning and is really beneficial for growing both skills and integrity.\nAs you work on a team that spans multiple companies, traditional corporate hierarchies do not apply. Of course, there will always be a sort of hierarchy of experience or skills, but these work in your favor: You will know who to ask for help, and teams are generally very happy for contributors of any skill level. Contributions also need not be in code: Inputs into discussions and domain knowledge contributions are highly valued as well. The flip-side of working in a team without your manager oversight: They may not be directly aware of the work you do. That’s why you have to write blog posts :)\nThe possibility for statistical programmers to pivot towards developing software or writing blog-posts such as this really transforms and broadens their job description. It is this transformation that is reflected by the choice of Roche to re-brand statistical programmers as analytical data scientists. The fact that cross-industry development is being advocated for really lets programmers expand their network outside of their company.\nThe {admiral} project serves as a testament to the power of collaborative open-source development and the potential it holds for the future of work in this industry."
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#last-updated",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#last-updated",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:55.716773"
  },
  {
    "objectID": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#details",
    "href": "posts/2023-07-20_cross_company_dev/cross_industry_dev.html#details",
    "title": "Cross-Industry Open Source Package Development",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "",
    "text": "The R Consortium Submission Working Group has now successfully made two pilot submissions to the FDA. All the submissions done by the group are focused on improving practices for R-based clinical trial regulatory submissions. Now, the R submission Working Groups, in collaboration with Appsilon and Posit, are exploring new technologies such as Containers and WebAssembly. In this article, we dive into the details of this exploration."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#how-everything-started",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#how-everything-started",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "How Everything Started",
    "text": "How Everything Started\n\nPilot 1\nThis pilot was initially submitted on November 22, 2021. This submission was the first publicly available R-based submission to the FDA. This was a test submission that aimed to explore the submission of an R package to the FDA following the eCTD specifications. The submission included an R package, R scripts for analysis, R-based analysis data reviewed guide (ADRG), and other important components. The final response letter from the FDA was received on March 14, 2022."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-2",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-2",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 2",
    "text": "Pilot 2\nThis was one of the first submission packages containing a Shiny application. The main goal of this pilot was to test the submission of an R-based Shiny application bundled into a submission package and transfer it successfully to FDA reviewers. The submitted application was built using the datasets and analyses that were used for the R Submission Pilot 1. The deployed version of this application is available on this site. Alternatively, a Rhino-based version of the application can be found here.\nThe final response letter from the FDA was reviewed on September 27, 2023.\n\n\n\n\n\nIn this submission, there were many open-source R packages that were used to create and execute the Shiny application. A very well-known shiny-based interactive exploration framework {teal} was used mainly for analyzing the clinical trial data; this package is included in the pharmaverse package repository. The full list of open-source and proprietary R analysis packages is available on this Analysis Data Reviewer’s Guide prepared by the R Consortium R Submissions Working Group for the Pilot 2."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-3",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-3",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 3",
    "text": "Pilot 3\nThis pilot was successfully submitted to the FDA on Aug 28, 2023. This was the first publicly available R submission that included R scripts to produce ADaM datasets and TLFs. Both the ADaMs (SDTM .xpt sources from the CDISC Pilot study) and the TLFs (ADaMs .xpt sourced from the ADaMs generated in R by the Pilot 3 team) were created using R.  The next step for this pilot is to await FDA’s review and approval, which may take several months to complete."
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-4",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#pilot-4",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Pilot 4",
    "text": "Pilot 4\nThis pilot aims to explore using technologies such as containers and WebAssembly software to package a Shiny application into a self-contained unit, streamlining the transfer and execution process for enhanced efficiency.\nThis pilot is expected to be divided into two parallel submissions:\n(a) will investigate WebAssembly and\n(b) will investigate containers.\n\nThe Journey with WebAssembly and Containers\nOur team at Appsilon teamed up with the dynamic Pilot 4 crew to explore WebAssembly technology and containers. George Stagg and Winston Chang also joined the working group to discuss the web-assembly portion of Pilot 4. This partnership brought together our engineering prowess to contribute to these tools, injecting fresh perspectives into the ongoing pilot project.\nSome of the outcomes of the collaboration:\n\nWe were able to set up a robust container environment for this pilot project. \nWe aided the progress made on the use of both experimental technologies: containers and WebAssembly.\nWe developed a working prototype submission using Podman container technology.\nWe developed a working early-stage prototype for wrapping a small Shiny application using WebAssembly.\n\n\nWebAssembly\n\nWebAssembly allows languages like R to be executed at near-native speed directly within web browsers, providing users with the ability to run R code without having R installed locally. WebR is essentially the R programming language adapted to run in a web browser environment using WebAssembly. This project is under active development. \n\n\n\nThe Pilot 4 Shiny App Up and Running on webR!\n\n\n\n\n\nThe deployed example of the Shiny app running on webR is available here. Check out the video of the application running below.\n\nDuring this pilot, engineers at Appsilon developed a prototype of a Shiny application running on webR. The application reuses most of the code from the previous pilot apps with some tweaks and a couple of hacks/changes to get around non CRAN dependencies, specially for data loading, WebR compatibilities, and shimming some of the functionality from {teal} and other packages that are (for now) not available on CRAN.\n\nwebR Shiny App\nDuring the second iteration, which was recently held, Pedro Silva shared the process of developing this Shiny app running on webR.\n\n\nThe Process\n\nLeverage the last 2 iterations of the application\n\nReuse as much code as possible\nAvoid touching the logic part\n\nRestrict the number of dependencies to packages on CRAN\n\nReplace/shim functionality that was lost from removing dependencies\n\n\nHere is the list of dependencies to packages on CRAN; those that worked are colored green, and those that were removed are marked in orange. We ended up with just 3 problematic dependencies (bold).\n\n\n\n\n\nIssues with library(cowplot):\n\nSome issues with low-level dependencies when deployed\n\nSolution:\n\nReplace functionality with HTML\n\nIssues with library(teal):\n\nUses {shiny.widgets} (not working for webR)\n\nSolution:\n\nRedo the UI\nLoad modules directly\nRecreate filter functionality\n\nIssues with library(teal.data):\n\nUse rds exports\n\nSolution:\n\nShim functionality, load data directly\n\n\nLeverage shinylive and httpuv to export and serve the application\n\nShinylive can help streamline the export process\n\nProblems\n\nshiny.live won’t let us have non-R files in the application directory - this is an outstanding bug that George asked us to raise an issue for.\nWe wouldn’t be able to run the application as a traditional shiny app.\n\nSolution:\n\nCustom build script\n\n\n{httpuv} can help serve the application\n\n{httpuv} would run natively on a machine to serve the Shiny app\n\n\n\n\n\nApplication Structure\nThe figure below shows an overview of what we ended with:\n\nSome of the issues and solutions found at the very beginning:\n\nThe previous applications were built using golem and another one in Rhino; the support for these frameworks is not great in webR up to now.\n\nSolution\n\n{box} works out of the box (reuse the rhino version modules)\nSimplify the structure and use a simple shiny modular structure\n\n\nShinylive does not like non-R files when generating the bundle\n\nSolution\n\nKeep the app folder as clean as possible for now (www folder only)\n\n\n{teal} and {teal.data} are not on CRAN\n\nSolution\n\nShim and used functionality\nUse a simple tab system for the UI structure\n\n\n\nThe FDA was previously told that the shiny application being prepared for the Pilot 4 submission would not be a 1 to 1 mapping from the previous one submitted for the Pilot 2 due to certain constraints such as {teal} not being on CRAN; however, this didn’t represent a problem for them since they would mainly like to test the technology.\nPedro Silva, one of the engineers working on the development of this app, mentioned “While WebR is still in development, it shows tremendous promise! The loading is definitely still a pain point (over 100mb to set up the environment!) but it will only get better moving forward.”\n\n\nContainers\n\n\n\n\n\nContainerization, particularly through technologies like Docker, Podman or Singularity, offers several advantages for deploying Shiny apps.\n\nChoosing the Right Container\nChoosing the right container was a question that arose in this project. Although Docker is the most popular, we decided to move forward with Podman. \nIn our exploration of containerization tools for deploying Shiny applications, we’ve identified key distinctions between Docker and Podman that influenced our choice. \nPodman stands out for its daemonless architecture, enhancing security by eliminating the need for a central daemon process. Unlike Docker, Podman supports running containers as non-root users, a critical feature for meeting FDA reviewer requirements. Developed by Red Hat and maintained as an open-source project, Podman prioritizes security with its rootless container support, offering a robust solution for security-conscious users. \n\n\nGoals\nA Container-based method to deploy Pilot 2 Shiny App.\n\n\nWhat we did\n\nConfigurable Podman Dockerfile / docker-compose.yml\n\nR version\nRegistry / organization name / image name (differences between docker.io and ghcr.io)\n\nDocumentation on creating the container\nCI: Automated build on amd64 and arm64 platforms\n\n\n\nPodman short-demo\n\nBelow is the dockerfile (recipe) for the container:"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#last-updated",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#last-updated",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:40:59.624662"
  },
  {
    "objectID": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#details",
    "href": "posts/2024-02-01_containers_webassembly_submission/containers_and_webassembly_submissions.html#details",
    "title": "Testing Containers and WebAssembly in Submissions to the FDA",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html",
    "title": "TLG Catalog 🤝 WebR",
    "section": "",
    "text": "TLG Catalog website"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#what-is-webr",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#what-is-webr",
    "title": "TLG Catalog 🤝 WebR",
    "section": "What is WebR?",
    "text": "What is WebR?\n\nWebR makes it possible to run R code in the browser without the need for an R server to execute the code: the R interpreter runs directly on the user’s machine.\n\nSource: WebR documentation\nIn short, WebR is a project that aims to port R into WebAssembly (WASM) which then allows to run compiled code in the website. A special thanks to George Stagg from Posit for making this integration possible. While WebR is still in active development, a significant progress had been made recently increasing its robustness and efficiency.\nHowever, it’s important to note a limitation: not all packages are compatible with WebR. A package must be compiled for WebAssembly to be used with WebR. Fortunately, there’s a dedicated WebR binary R package repository hosting close to 20,000 packages. For packages not yet available, you can utilize a dedicated GitHub Actions workflow to build them yourself, or use r-universe platform that will build it for you."
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#implementation-details",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#implementation-details",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Implementation Details",
    "text": "Implementation Details\nThe integration of WebR into TLG Catalog was made possible through a dedicated quarto-webr Quarto extension, which simplifies the integration process. The main challenge was to ensure a DRY (Don’t Repeat Yourself) approach with respect to the existing codebase. This was achieved through leveraging lesser-known knitr features, including knitr::knit_code$get() to reuse code chunks as well as results = \"asis\" to create code chunk from within another (parent) code chunk. The source code for this is open-source and available on GitHub."
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#interactive-teal-applications-via-shinylive",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#interactive-teal-applications-via-shinylive",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Interactive teal Applications via shinylive",
    "text": "Interactive teal Applications via shinylive\nThe benefits of WebR extend beyond TLG outputs. It also enhances all existing teal applications. Users can now interact with applications and even live-edit their source code! Everything is inside the website itself without any additional application hosting service. This was made possible through the shinylive Quarto extension leveraging Shinylive under the hood. A huge thank you to the Shiny team for their contributions!"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#summary",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#summary",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Summary",
    "text": "Summary\nThe addition of interactivity via WebR marks a significant milestone for TLG Catalog. This update unlocks a myriad of possibilities previously unavailable, such as live code editing, step-by-step code execution, access to function documentation, and dynamic data exploration. This advancement brings R closer to users, especially those new to the language, fostering a more engaging and effective learning experience.\nHappy learning!"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#last-updated",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#last-updated",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:05.813679"
  },
  {
    "objectID": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#details",
    "href": "posts/2024-05-08_tlg_catalog_webr/tlg_catalog_webr.html#details",
    "title": "TLG Catalog 🤝 WebR",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#introduction",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#introduction",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Introduction",
    "text": "Introduction\nI am thrilled to share the exciting news of the release of {teal.modules.clinical} 0.9.0 on CRAN. This significant achievement marks a major milestone for the NEST team in our open-source efforts that will make a profound impact on the entire open-source community.\nThis package release now completes the suite of {teal} family of packages recently released to CRAN (see our other blog post here!). teal is a shiny-based interactive dashboard framework for analyzing data and aims to quickly and easily allow users to create dynamic visualizations. We invite you to delve deeper into the teal family of packages, including {teal.modules.clinical} by visiting our teal website."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#accelerating-clinical-insights",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#accelerating-clinical-insights",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Accelerating clinical insights",
    "text": "Accelerating clinical insights\nDesigned to enable faster insights generation under a clinical data context, the {teal.modules.clinical} package contains a set of standard teal modules to be used with CDISC data to generate many of the common analysis displays used in clinical trial reporting. By leveraging {teal.modules.clinical}, data scientists can visualize, interact, and analyze their data effectively."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#installation",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#installation",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Installation",
    "text": "Installation\nGetting started with {teal.modules.clinical} is incredibly easy. Simply run the command install.packages(\"teal.modules.clinical\") and you’ll be able to install the package directly into your local R studio environment from CRAN. For further information about this release, and information on important breaking changes, please visit the tmc site."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#explore-the-teal-gallery-and-tlg-catalog",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#explore-the-teal-gallery-and-tlg-catalog",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Explore the Teal Gallery and TLG Catalog",
    "text": "Explore the Teal Gallery and TLG Catalog\nTo get a glimpse of the capabilities and potential applications of {teal.modules.clinical}, we encourage you to explore the Teal Gallery and TLG-Catalog. These resources showcase a huge range of examples of interactive visualizations using modules from this package, which can be reused and inspire you when building your teal-shiny app."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#acknowledgments",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#acknowledgments",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe would like to give a huge thanks to the hard work and dedication of the many developers (past and present) for making this release possible. And not to forget our wonderful users for your continued support and enthusiasm.\n\n\n\nExample {teal.modules.clinical} interactive KM-plot created by tm_g_km() function. Read more about this module in the function documentation."
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#last-updated",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#last-updated",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:10.689883"
  },
  {
    "objectID": "posts/2024-04-08_tmc_cran/tmc_cran.html#details",
    "href": "posts/2024-04-08_tmc_cran/tmc_cran.html#details",
    "title": "teal.modules.clinical v0.9.0 is now on CRAN!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html",
    "href": "posts/2023-07-14_code_sections/code_sections.html",
    "title": "How to use Code Sections",
    "section": "",
    "text": "The admiral package embraces a modular style of programming, where blocks of code are pieced together in sequence to create an ADaM dataset. However, with the well-documented advantages of the modular approach comes the recognition that scripts will on average be longer. As such, astute programmers working in RStudio are constantly on the lookout for quick ways to effectively navigate their scripts. Enter code sections!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#introduction",
    "href": "posts/2023-07-14_code_sections/code_sections.html#introduction",
    "title": "How to use Code Sections",
    "section": "",
    "text": "The admiral package embraces a modular style of programming, where blocks of code are pieced together in sequence to create an ADaM dataset. However, with the well-documented advantages of the modular approach comes the recognition that scripts will on average be longer. As such, astute programmers working in RStudio are constantly on the lookout for quick ways to effectively navigate their scripts. Enter code sections!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#so-what-are-code-sections-and-why-are-they-useful",
    "href": "posts/2023-07-14_code_sections/code_sections.html#so-what-are-code-sections-and-why-are-they-useful",
    "title": "How to use Code Sections",
    "section": "So, what are code sections and why are they useful?",
    "text": "So, what are code sections and why are they useful?\nCode Sections are separators for long R scripts or functions in RStudio. They can be set up by inserting a comment line followed by four or more dashes in between portions of code, like so:\n\n# First code section ----\n\na &lt;- 1\n\n# Second code section ----\n\nb &lt;- 2\n\n# Third code section ----\n\nc &lt;- 3\n\nRStudio then recognizes the code sections automatically, and enables you to:\n\nCollapse and expand them using the arrow displayed next to the line number, or with the handy shortcuts Alt+L/Shift+Alt+L on Windows or Cmd+Option+L/Cmd+Shift+Option+L on Mac.\nTravel in between them using the navigator at the bottom of the code pane, or by pressing Shift+Alt+J on Windows or Cmd+Shift+Option+J on Mac.\nView an outline of the file using the “Outline” button at the top right of the pane and/or the orange hashtag “Section Navigator” button at the bottom left of the pane.\n\n\n\n\n\n\nCollapsed sections, outline view and the section navigator for the example above.\n\n\n\n\nIt is also possible to create subsections by using two hashtags at the start of a comment line:\n\n# First code section ----\na &lt;- 1\n\n## A code subsection ----\nb &lt;- 2\n\n# Second code section ----\nc &lt;- 3\n\n\n\n\n\n\nCode subsections for the example above.\n\n\n\n\nFor a complete list of Code Sections shortcuts, and for further information, see here."
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#conclusion",
    "href": "posts/2023-07-14_code_sections/code_sections.html#conclusion",
    "title": "How to use Code Sections",
    "section": "Conclusion",
    "text": "Conclusion\nCode sections are an easy way to navigate long scripts and foster good commenting practices. They are used extensively in the admiral package, but there is no reason that you cannot start using them yourself in your day-to-day R programming!"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#last-updated",
    "href": "posts/2023-07-14_code_sections/code_sections.html#last-updated",
    "title": "How to use Code Sections",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:14.153131"
  },
  {
    "objectID": "posts/2023-07-14_code_sections/code_sections.html#details",
    "href": "posts/2023-07-14_code_sections/code_sections.html#details",
    "title": "How to use Code Sections",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html",
    "title": "Tips for First Time Contributors",
    "section": "",
    "text": "Who remembers their first day at school?\nWhat about if you ever had to move to join a new school mid-year?\nI had that experience as a child, and there was nothing more intimidating than stepping into a classroom of kids that had already made long-standing bonds and cliques, and here’s me joining as an outsider - the new kid! I had a crippling shyness that meant it took many years until I allowed the same level of friendships to grow as I had in my old school.\nMemories like this stay with us, and they can have a habit of holding us back in later life. It never feels comfortable being the “new kid” and it takes a brave step to jump in and be open to the new connections and relationships that might come from this.\nGetting involved with open source can feel similarly daunting. You’ll likely get hit with impostor syndrome making you doubt whether your code is worthy of sharing or you may fear being judged publicly by others. Even more fundamentally, you might lack certain skills or experience (such as git) which could make an initial barrier.\nOne of the key tenets when we started pharmaverse was for this to be driven by a passionate community of people, many of whom that actually use the packages, as who better to empathize with the challenge at hand than the users themselves? So firstly, please erase from your mind the thought that package development is only for the elite and you are “not worthy”. Nothing could be further from the truth!\nTo share a Coretta Scott King quote: “The greatness of a community is most accurately measured by the compassionate actions of its members”. Now, in no way do I mean here to compare our community to her amazing work for the civil rights movement, but the quote is inspiring in many contexts. We are building a pharma global community of people from all walks of life that come together to help build shared solutions to our industry clinical reporting challenges. The best way we can do this is by having compassion with one another, and you often will find exactly that when you first join a package development team - most people are willing to invest in helping you along your learning journey."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-one-overcoming-the-fear",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-one-overcoming-the-fear",
    "title": "Tips for First Time Contributors",
    "section": "",
    "text": "Who remembers their first day at school?\nWhat about if you ever had to move to join a new school mid-year?\nI had that experience as a child, and there was nothing more intimidating than stepping into a classroom of kids that had already made long-standing bonds and cliques, and here’s me joining as an outsider - the new kid! I had a crippling shyness that meant it took many years until I allowed the same level of friendships to grow as I had in my old school.\nMemories like this stay with us, and they can have a habit of holding us back in later life. It never feels comfortable being the “new kid” and it takes a brave step to jump in and be open to the new connections and relationships that might come from this.\nGetting involved with open source can feel similarly daunting. You’ll likely get hit with impostor syndrome making you doubt whether your code is worthy of sharing or you may fear being judged publicly by others. Even more fundamentally, you might lack certain skills or experience (such as git) which could make an initial barrier.\nOne of the key tenets when we started pharmaverse was for this to be driven by a passionate community of people, many of whom that actually use the packages, as who better to empathize with the challenge at hand than the users themselves? So firstly, please erase from your mind the thought that package development is only for the elite and you are “not worthy”. Nothing could be further from the truth!\nTo share a Coretta Scott King quote: “The greatness of a community is most accurately measured by the compassionate actions of its members”. Now, in no way do I mean here to compare our community to her amazing work for the civil rights movement, but the quote is inspiring in many contexts. We are building a pharma global community of people from all walks of life that come together to help build shared solutions to our industry clinical reporting challenges. The best way we can do this is by having compassion with one another, and you often will find exactly that when you first join a package development team - most people are willing to invest in helping you along your learning journey."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-two-where-to-start",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-two-where-to-start",
    "title": "Tips for First Time Contributors",
    "section": "Step Two: Where to Start",
    "text": "Step Two: Where to Start\nOur pharmaverse website has an Individual Contributor page to help you first get started as a contributor, including many tips and some useful free online training resources for topics such as R package development, Git and GitHub.\nHere are some of the recommendations:\n\nStart small - it doesn’t even need to be a code contribution to begin with! We are equally grateful for anyone raising GitHub issues to report bugs or new feature ideas.\nWhen it comes to you feeling ready to start to contribute code, then choose a package that covers an area of clinical reporting that you already feel confident with. Reach out to the team via our Pharmaverse Slack or on GitHub to express interest, and they may even be able to support you onboarding.\nBe kind to yourself and take on one of the easier issues to build confidence - these are often labeled “good first issue” and might include activities like updating a unit test or some documentation.\nExpect to have review findings for your early Pull Requests and that’s absolutely OK as the learning comes from experience. I’ve been there! I share here without shame my first ever code contribution to admiral and 4 rounds of review later we finally got it merged! Over time I’m glad to say things improved, although maybe that depends who you ask… :D"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-three-still-having-doubt",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-three-still-having-doubt",
    "title": "Tips for First Time Contributors",
    "section": "Step Three: Still Having Doubt?",
    "text": "Step Three: Still Having Doubt?\nI wanted to include a testimonial from another member of the community who has more recently been through all of the above steps.\nHere’s Celine Piraux’s story in her own words:\nLast year, I began my involvement with the xportr R package, and I’d like to share my journey towards my first pull request (PR). As a statistical programmer mainly using SAS, with some knowledge of R, I initially had no experience with R package development. Nevertheless, I was eager to contribute to an open-source R package.\nThe first step in getting involved with an R package is to be aware that volunteers are needed. In my case, I noticed a call for volunteers posted on Slack for the xportr package. Since this package aligns with my expertise in metadata, I eagerly jumped at the opportunity.\nWith no prior experience in R package development and apprehensions about potentially breaking the code, I initially adopted an observational approach. I began by installing the package, running its functions, and then exploring the code to understand their implementation. I tested the functions to evaluate if they worked as expected. If I encountered any issues, I documented them in a GitHub issue.\nAfter creating the GitHub issues with my findings, I began to follow the implementation process by another developer to become more familiar with GitHub. This involved observing how branches are created, commits are made, and comparisons between commits and branches are performed. I also learned about the functioning of comments, pull requests, and the review process.\nOnce his PR was completed, it was time for the review phase, during which I ran the new code to understand the changes and experiment with it. This step also involved two technical requirements: firstly, linking my GitHub account with RStudio to access the code in the branch, and secondly, understanding how to execute code within the branch. Since installing packages using install.packages() wouldn’t function for development code, it was essential to know about the function devtools::load_all() to execute the code.\nAs I became familiar with both the package and GitHub, I felt more confident to start my first implementation. I read the book ‘R Packages’ to gain a better understanding of R package development. The first chapter, entitled ‘The Whole Game’, provided a good overview of R package development and introduced useful functions such as load_all(), document(), test(), and check().\nWith this first implementation, I dived into the development process of the package, involving tasks such as creating a new branch, committing changes, writing a descriptive commit message (still not sure of the best approach), updating and adding new tests to cover the modifications, resolving merge conflicts, updating documentation, handling failing checks in CI/CD. Having a checklist in the PR template was very helpful to keep all the necessary steps in mind.\nThe PR was submitted, and the reviewers provided useful feedback and valuable advice on my code. I implemented the comments, and the PR was approved and merged into the main branch.\nAfter successfully merging my first PR, I feel more confident in contributing to open-source R packages. It’s now time to select a new issue to tackle."
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-four-so-what-are-you-waiting-for",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#step-four-so-what-are-you-waiting-for",
    "title": "Tips for First Time Contributors",
    "section": "Step Four: So, what are YOU waiting for…",
    "text": "Step Four: So, what are YOU waiting for…\nTo end, my advice to my 8 year old self from the start of this story would be “Take a risk and dive in - what have you got to lose?!” - and I only hope some part of this message resonates with others reading this blog and considering joining the open source community.\nHonestly, you won’t regret it!\n\n\n\nPicture taken from the pharmaverse website: showing the network graph of the 261 pharmaverse contributors (at the time of writing this blog)"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#last-updated",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#last-updated",
    "title": "Tips for First Time Contributors",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:18.085677"
  },
  {
    "objectID": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#details",
    "href": "posts/2024-03-11_tips_for__first_.../tips_for__first__time__contributors.html#details",
    "title": "Tips for First Time Contributors",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html",
    "href": "posts/2023-11-27_higher_order/higher_order.html",
    "title": "Believe in a higher order!",
    "section": "",
    "text": "Picture the following scenario:\nYou, a budding {admiral} programmer, are finding your groove chaining together modular code blocks to derive variables and parameters in a drive to construct your favorite ADaM dataset, ADAE. Suddenly you notice that one of the flags you are deriving should only use records on or after study day 1. In a moment of mild annoyance, you get to work modifying what was originally a simple call to derive_var_extreme_flag() by first subsetting ADAE to records where AESTDY &gt; 1, then deriving the flag only for the subsetted ADAE, and finally binding the two portions of ADAE back together before continuing on with your program. Miffed by this interruption, you think to yourself: “I wish there was a neater, faster way to do this in stride, that didn’t break my code modularity…”\nIf the above could never be you, then you’ll probably be alright never reading this blog post. However, if you want to learn more about the tools that {admiral} provides to make your life easier in cases like this one, then you are in the right place, since this blog post will highlight how higher order functions can solve such issues.\nA higher order function is a function that takes another function as input. By introducing these higher order functions, {admiral} intends to give the user greater power over derivations, whilst trying to negate the need for both adding additional {admiral} functions/arguments, and the user needing many separate steps.\nThe functions covered in this post are:\n\nrestrict_derivation(): Allows the user to execute a single derivation on a subset of the input dataset.\ncall_derivation(): Allows the user to call a single derivation multiple times with some arguments being fixed across iterations and others varying.\nslice_derivation(): Allows the user to split the input dataset into slices (subsets) and for each slice a single derivation is called separately. Some or all arguments of the derivation may vary depending on the slice.\n\n\n\nThe examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nFor example purpose, the ADSL dataset - which is included in {admiral} - and the SDTM datasets from {pharmaversesdtm} are used.\n\ndata(\"admiral_adsl\")\ndata(\"ae\")\ndata(\"vs\")\nadsl &lt;- admiral_adsl\nae &lt;- convert_blanks_to_na(ae)\nvs &lt;- convert_blanks_to_na(vs)\n\nThe following code creates a minimally viable ADAE dataset to be used where needed in the following examples.\n\nadae &lt;- ae %&gt;%\n  left_join(adsl, by = c(\"STUDYID\", \"USUBJID\")) %&gt;%\n  derive_vars_dt(\n    new_vars_prefix = \"AST\",\n    dtc = AESTDTC,\n    highest_imputation = \"M\"\n  ) %&gt;%\n  mutate(\n    TRTEMFL = if_else(ASTDT &gt;= TRTSDT, \"Y\", NA_character_),\n    TEMP_AESEVN = as.integer(factor(AESEV, levels = c(\"SEVERE\", \"MODERATE\", \"MILD\")))\n  )"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#required-packages",
    "href": "posts/2023-11-27_higher_order/higher_order.html#required-packages",
    "title": "Believe in a higher order!",
    "section": "",
    "text": "The examples in this blog post require the following packages.\n\nlibrary(admiral)\nlibrary(pharmaversesdtm)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nFor example purpose, the ADSL dataset - which is included in {admiral} - and the SDTM datasets from {pharmaversesdtm} are used.\n\ndata(\"admiral_adsl\")\ndata(\"ae\")\ndata(\"vs\")\nadsl &lt;- admiral_adsl\nae &lt;- convert_blanks_to_na(ae)\nvs &lt;- convert_blanks_to_na(vs)\n\nThe following code creates a minimally viable ADAE dataset to be used where needed in the following examples.\n\nadae &lt;- ae %&gt;%\n  left_join(adsl, by = c(\"STUDYID\", \"USUBJID\")) %&gt;%\n  derive_vars_dt(\n    new_vars_prefix = \"AST\",\n    dtc = AESTDTC,\n    highest_imputation = \"M\"\n  ) %&gt;%\n  mutate(\n    TRTEMFL = if_else(ASTDT &gt;= TRTSDT, \"Y\", NA_character_),\n    TEMP_AESEVN = as.integer(factor(AESEV, levels = c(\"SEVERE\", \"MODERATE\", \"MILD\")))\n  )"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#last-updated",
    "href": "posts/2023-11-27_higher_order/higher_order.html#last-updated",
    "title": "Believe in a higher order!",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:22.551922"
  },
  {
    "objectID": "posts/2023-11-27_higher_order/higher_order.html#details",
    "href": "posts/2023-11-27_higher_order/higher_order.html#details",
    "title": "Believe in a higher order!",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "",
    "text": "The importance of reliable and accurate software in the pharmaceutical industry cannot be overstated. As you grapple with the complexities of developing Shiny applications that not only meet requirements but also remain free of defects, the conversation inevitably turns to the topic of validation. This is where the Rhino framework comes into play. It provides a validation environment through a set of tools that not only ensure the thorough testing of your Shiny Application but also enforce good software practices for producing quality code."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#what-is-validation",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#what-is-validation",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "What is Validation?",
    "text": "What is Validation?\nAt its core, validation is about generating objective evidence that the software consistently fulfills the users’ needs and requirements. It is closely intertwined with concepts such as verification, testing, quality assurance, and quality control. However, it is not a one-time activity. Validation is an ongoing aspect of the entire development process for assessing whether an application meets requirements and is free of defects. (phuse, 2021)\nThe foundational aspect of validation lies in the clarity and precision of the requirements set upfront. Projects often encounter challenges not due to technical constraints but unclear requirements. This clear definition of needs is the first and most critical step in the validation process, setting the stage for effective and meaningful software evaluation.\nTesting in some form is indispensable in any software engineering project. However, writing automated tests costs time and effort. Therefore, the approach to testing can vary widely depending on the project’s scope, complexity, and purpose.\nFor instance, manual testing, which involves checking the software by hand for bugs or other issues, might suffice for a Proof of Concept (PoC) application. On the other hand, in applications performing critical functions, such as those involved in clinical trial data analysis, the cost of failure could be extremely high, necessitating more rigorous and comprehensive automated testing.\nThe key is to strike a balance, ensuring the software is thoroughly validated without exhausting resources."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#how-rhino-helps-in-validating-r-shiny-apps",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#how-rhino-helps-in-validating-r-shiny-apps",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "How Rhino Helps in Validating R Shiny Apps",
    "text": "How Rhino Helps in Validating R Shiny Apps\nRhino automates the steps for creating the validation environment so you can spend your time and effort on writing the actual tests.\n\n\n\nValidation with Rhino\n\n\nWhen you first run rhino::init() inside your project’s root directory, Rhino introduces a suite of tools that will help you validate your Shiny application and a robust file structure to modularize your code into meaningful parts. With Rhino initialized, validating your Shiny application becomes an integral and seamless part of your development workflow.\n\nTesting\nRhino comes with testthat installed and introduces the tests/ directory in your project. As part of the validation process, you want to test your requirements. With Rhino, all you need to do is create your test-*.R files in tests/testthat directory and run rhino::test_r().\nAlthough shinytest2 tests can also be added with minimal effort (How to use shinytest2), one of the distinct features of Rhino is the ability to write end-to-end tests with Cypress out of the box. To learn more about how to write Cypress tests via Rhino, you can read more on Write end-to-end tests with Cypress tutorial in the documentation.\n\nContinuous Integration via GitHub\nAs discussed in the “What is validation?” section, validation is a continuous process. To accomplish this, Rhino uses a GitHub Actions workflow. This file enables the automatic running of tests and linting checks every time the code is pushed to the Github repository.\nYou can further configure your repository to require these checks to pass before a pull request can be merged, ensuring only validated code makes it into your project.\n\n\n\nStandardization of Good Practices\nRhino also provides tools for good software development practices. These standardization tools don’t directly address requirements or defects, but they raise the overall code quality.\n\nrhino::lint_* Functions\nRhino provides linters for R (rhino::lint_r), Javascript (rhino::lint_js), and Sass (rhino::lint_sass) code. These linters ensure that the code style is consistent. This consistency might not be visible to the end-users, but it significantly enhances code readability and maintainability, which are crucial for reducing bugs and errors in the application. The cleaner and more readable the code, the easier it is for you to spot and rectify issues.\n\n\nModularization with box\nModularization allows developers to compartmentalize different aspects of the application, making each part easier to understand, develop, and validate individually.\nRhino leverages the box package for effective modularization. This approach to structuring the app makes the codebase more manageable and navigable and significantly enhances the ease of testing and maintenance.\n\n\nReproducibility with renv\nrenv is currently the best tool for ensuring reproducibility and consistency across different development environments. Each Rhino project comes with renv activated. It further automatizes managing dependencies with rhino::pkg_install() and rhino::pkg_remove() functions to install, update or remove a package with just one call. Check out this explanation on Renv configuration to learn more!"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#wider-picture",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#wider-picture",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Wider picture",
    "text": "Wider picture\n\nBare Bones Approach\nA minimal Shiny app can be as simple as a single file. This ‘bare bones’ approach is the most straightforward, requiring minimal setup. However, it places the workload on the developer to establish everything from scratch.\nThis method may be suitable for quick prototypes or small-scale projects but it lacks the robustness needed for complex, large-scale applications, especially in a field as critical as pharmaceuticals.\n\n\nOther Shiny Frameworks\nFrameworks like Golem and Leprechaun adopt R package structure for Shiny apps. This method allows developers to leverage standard R package testing tools like R CMD check.\nWhile this approach brings the benefits of R’s package development ecosystem, it does not offer extra state-of-the-art tools such as linters, Cypress tests, and GitHub Actions that Rhino provides out of the box. Leaving the burden of manually setting them up on the developers.\nRead our {rhino} vs {golem} vs {leprechaun}: Which R/Shiny Library is Right for You? blog post to learn more about the differences.\n\n\nValidation Theater\nTest coverage is often cited as a key metric for assessing the reliability of an application. While high test coverage is beneficial, it is not a definitive guarantee of software quality. The quality of the tests themselves matters significantly more. A common pitfall is the overreliance on achieving 100% test coverage, which may give a false sense of security about the application’s robustness.\nAdditionally, R CMD checks provide reassurance with their green check marks. However, many of these automated validations are not directly applicable or relevant to Shiny apps. This is one of the reasons why Rhino does not use these checks. While they are useful, they do not encompass the full spectrum of what needs to be tested in a Shiny application.\nThe key takeaway is that no automated check can substitute for a thorough and well-thought-out development process. The quality of a Shiny application hinges not just on the tests it passes but on the entirety of its development lifecycle, from initial design to final deployment."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#summing-up-rhino-for-r-shiny-app-validation",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#summing-up-rhino-for-r-shiny-app-validation",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Summing up Rhino for R Shiny App Validation",
    "text": "Summing up Rhino for R Shiny App Validation\nRhino adopts a software engineering perspective, focusing primarily on the technical facets of validation. It provides a comprehensive suite of tools that streamline the process of creating Shiny applications that are not only functionally sound but also robust and maintainable.\nHowever, it’s crucial to recognize that Rhino’s contribution is a piece of a much larger puzzle. Validation, as emphasized throughout this post, is not a standalone activity but a continuous process that intertwines with every phase of software development.\nFrom the initial requirement gathering to the final deployment, each step plays a vital role in shaping the overall quality and effectiveness of the application.\nIf you’d like to learn more about Rhino, especially within the context of building Shiny apps for regulatory submissions you can check out this blogpost, Reproducible and Reliable Shiny Apps for Regulatory Submissions or visit Appsilon’s website."
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#references",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#references",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "References",
    "text": "References\nphuse. (2021). R Package Validation Framework. R Package Validation Framework. https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Data+Visualisation+%26+Open+Source+Technology/WP059.pdf\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#last-updated",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#last-updated",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Last updated",
    "text": "Last updated\n\n2024-05-31 12:41:27.029003"
  },
  {
    "objectID": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#details",
    "href": "posts/2024-03-01_rhino_shiny_app_validation/rhino_shiny_app_validation.html#details",
    "title": "Rhino: A Step Forward in Validating Shiny Apps",
    "section": "Details",
    "text": "Details\n\nSource, Session info"
  }
]